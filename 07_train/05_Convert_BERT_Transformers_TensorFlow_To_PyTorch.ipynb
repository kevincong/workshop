{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TensorFlow-Trained BERT Model to a PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n",
      "\r\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n",
      "\r\n",
      "spyder 4.1.2 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\r\n",
      "spyder 4.1.2 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\r\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\r\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\r\n",
      "jupyterlab 1.2.6 requires jupyterlab_server~=1.0.0, but you'll have jupyterlab-server 1.1.0 which is incompatible.\r\n",
      "awswrangler 1.2.0 requires numpy~=1.18.0, but you'll have numpy 1.19.1 which is incompatible.\r\n",
      "awscli 1.18.119 requires rsa<=4.5.0,>=3.1.2, but you'll have rsa 4.6 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow==2.1.0 --upgrade --ignore-installed \n",
    "!pip install -q transformers==2.8.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _It's OK if you see ERRORs ^^ above ^^.  Please ignore._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PyTorch and TorchServe\n",
    "\n",
    "TorchServe is a flexible and easy to use tool for serving PyTorch models: \n",
    "* https://github.com/pytorch/serve\n",
    "* https://github.com/pytorch/serve/blob/master/docs/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n",
      "\r\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n",
      "\r\n",
      "awswrangler 1.2.0 requires numpy~=1.18.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==1.5.0 --upgrade --ignore-installed\n",
    "!pip install -q torchserve==0.1.1\n",
    "!pip install -q torch-model-archiver==0.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _It's OK if you see ERRORs ^^ above ^^.  Please ignore._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-REQUISITE: \n",
    "\n",
    "## You need to have succesfully run the previous notebooks in the `TRAINING` before you continue with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++\n",
      "SUCCESS. You can continue.\n",
      "+++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    training_job_name\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('SUCCESS. You can continue.')\n",
    "    print(\"+++++++++++++++++++++++++++++++\")\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('STOP: Please run the previous notebooks in the TRAIN section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous training_job_name: tensorflow-training-2020-08-22-19-35-37-636\n"
     ]
    }
   ],
   "source": [
    "print('Previous training_job_name: {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the TensorFlow-Trained Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-250107111215/tensorflow-training-2020-08-22-19-35-37-636/output/model.tar.gz to models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the model and output artifacts from AWS S3\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz $models_dir/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "#!ls -al ./models\n",
    "\n",
    "tar = tarfile.open('{}/model.tar.gz'.format(models_dir))\n",
    "tar.extractall(path=models_dir)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 487052\r\n",
      "drwxrwxr-x  7 ec2-user ec2-user      4096 Aug 22 19:57 .\r\n",
      "drwxrwxr-x 11 ec2-user ec2-user      4096 Aug 22 19:57 ..\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Aug 22 19:44 code\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Aug 22 19:44 metrics\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 498712368 Aug 22 19:45 model.tar.gz\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Aug 22 19:38 tensorboard\r\n",
      "drwxr-xr-x  3 ec2-user ec2-user      4096 Aug 22 19:38 tensorflow\r\n",
      "drwxr-xr-x  3 ec2-user ec2-user      4096 Aug 22 19:38 transformers\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261692\r\n",
      "drwxr-xr-x 2 ec2-user ec2-user      4096 Aug 22 19:43 .\r\n",
      "drwxr-xr-x 3 ec2-user ec2-user      4096 Aug 22 19:38 ..\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user      1358 Aug 22 19:43 config.json\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 267959068 Aug 22 19:43 tf_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "transformer_model_dir = '{}/transformers/fine-tuned/'.format(models_dir)\n",
    "\n",
    "!ls -al $transformer_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"_num_labels\": 5,\r\n",
      "  \"activation\": \"gelu\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"DistilBertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bad_words_ids\": null,\r\n",
      "  \"bos_token_id\": null,\r\n",
      "  \"decoder_start_token_id\": null,\r\n",
      "  \"dim\": 768,\r\n",
      "  \"do_sample\": false,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": false,\r\n",
      "  \"eos_token_id\": null,\r\n",
      "  \"finetuning_task\": null,\r\n",
      "  \"hidden_dim\": 3072,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\",\r\n",
      "    \"3\": \"LABEL_3\",\r\n",
      "    \"4\": \"LABEL_4\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"is_decoder\": false,\r\n",
      "  \"is_encoder_decoder\": false,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2,\r\n",
      "    \"LABEL_3\": 3,\r\n",
      "    \"LABEL_4\": 4\r\n",
      "  },\r\n",
      "  \"length_penalty\": 1.0,\r\n",
      "  \"max_length\": 20,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"min_length\": 0,\r\n",
      "  \"model_type\": \"distilbert\",\r\n",
      "  \"n_heads\": 12,\r\n",
      "  \"n_layers\": 6,\r\n",
      "  \"no_repeat_ngram_size\": 0,\r\n",
      "  \"num_beams\": 1,\r\n",
      "  \"num_return_sequences\": 1,\r\n",
      "  \"output_attentions\": false,\r\n",
      "  \"output_hidden_states\": false,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"prefix\": null,\r\n",
      "  \"pruned_heads\": {},\r\n",
      "  \"qa_dropout\": 0.1,\r\n",
      "  \"repetition_penalty\": 1.0,\r\n",
      "  \"seq_classif_dropout\": 0.2,\r\n",
      "  \"sinusoidal_pos_embds\": false,\r\n",
      "  \"task_specific_params\": null,\r\n",
      "  \"temperature\": 1.0,\r\n",
      "  \"tie_weights_\": true,\r\n",
      "  \"top_k\": 50,\r\n",
      "  \"top_p\": 1.0,\r\n",
      "  \"torchscript\": false,\r\n",
      "  \"use_bfloat16\": false,\r\n",
      "  \"vocab_size\": 30522\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "cat $transformer_model_dir/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TensorFlow Model to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification # PyTorch version\n",
    "\n",
    "loaded_pytorch_model = DistilBertForSequenceClassification.from_pretrained(transformer_model_dir,\n",
    "                                                                     id2label={\n",
    "                                                                       0: 1,\n",
    "                                                                       1: 2,\n",
    "                                                                       2: 3,\n",
    "                                                                       3: 4,\n",
    "                                                                       4: 5\n",
    "                                                                     },\n",
    "                                                                     label2id={\n",
    "                                                                       1: 0,\n",
    "                                                                       2: 1,\n",
    "                                                                       3: 2,\n",
    "                                                                       4: 3,\n",
    "                                                                       5: 4\n",
    "                                                                     },\n",
    "                                                                  from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_distilbert.DistilBertForSequenceClassification'>\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(type(loaded_pytorch_model))\n",
    "print(loaded_pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_device -1\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "inference_device = -1 # CPU: -1, GPU: 0\n",
    "print('inference_device {}'.format(inference_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_pytorch_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='pt',\n",
    "                                                device=inference_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it!  I will recommend this to everyone. [{'label': 1, 'score': 0.229841}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"I loved it!  I will recommend this to everyone.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Really bad.  I hope they don't make this anymore. [{'label': 2, 'score': 0.22936572}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Transformer/PyTorch Model with `.save_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_models_dir = './models/transformers/pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $pytorch_models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pytorch_model.save_pretrained(pytorch_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261588\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user      4096 Aug 22 19:57 .\r\n",
      "drwxr-xr-x 4 ec2-user ec2-user      4096 Aug 22 19:57 ..\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      1302 Aug 22 19:57 config.json\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 267852933 Aug 22 19:57 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $pytorch_models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = DistilBertForSequenceClassification.from_pretrained(pytorch_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "inference_pipeline = TextClassificationPipeline(model=pytorch_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='pt',\n",
    "                                                device=inference_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it!  I will recommend this to everyone. [{'label': 1, 'score': 0.229841}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"I loved it!  I will recommend this to everyone.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Transformer/PyTorch Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pytorch_model_name = 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-250107111215/models/transformer-pytorch/\n"
     ]
    }
   ],
   "source": [
    "transformer_pytorch_model_s3_uri = 's3://{}/models/transformer-pytorch/'.format(bucket)\n",
    "print(transformer_pytorch_model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: models/transformers/pytorch/config.json to s3://sagemaker-us-west-2-250107111215/models/transformer-pytorch/config.json\n",
      "upload: models/transformers/pytorch/pytorch_model.bin to s3://sagemaker-us-west-2-250107111215/models/transformer-pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $pytorch_models_dir $transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 19:57:18       1302 config.json\r\n",
      "2020-08-22 19:57:18  267852933 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'transformer_pytorch_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store transformer_pytorch_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'transformer_pytorch_model_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "autopilot_endpoint_name                          -> 'automl-dm-ep-22-16-47-12'\n",
      "balance_dataset                                  -> True\n",
      "comprehend_endpoint_arn                          -> 'arn:aws:comprehend:us-west-2:250107111215:documen\n",
      "df_dataset_metrics                               ->         entity                   instance         \n",
      "experiment_name                                  -> 'Amazon-Customer-Reviews-BERT-Experiment-159812492\n",
      "header_train_s3_uri                              -> 's3://sagemaker-us-west-2-250107111215/data/amazon\n",
      "max_seq_length                                   -> 64\n",
      "noheader_train_s3_uri                            -> 's3://sagemaker-us-west-2-250107111215/data/amazon\n",
      "prepare_trial_component_name                     -> 'TrialComponent-2020-08-22-193529-qhjz'\n",
      "processed_test_data_s3_uri                       -> 's3://sagemaker-us-west-2-250107111215/sagemaker-s\n",
      "processed_train_data_s3_uri                      -> 's3://sagemaker-us-west-2-250107111215/sagemaker-s\n",
      "processed_validation_data_s3_uri                 -> 's3://sagemaker-us-west-2-250107111215/sagemaker-s\n",
      "raw_input_data_s3_uri                            -> 's3://sagemaker-us-west-2-250107111215/amazon-revi\n",
      "s3_private_path_tsv                              -> 's3://sagemaker-us-west-2-250107111215/amazon-revi\n",
      "test_split_percentage                            -> 0.05\n",
      "train_split_percentage                           -> 0.9\n",
      "training_job_debugger_artifacts_path             -> 's3://sagemaker-us-west-2-250107111215/tensorflow-\n",
      "training_job_name                                -> 'tensorflow-training-2020-08-22-19-35-37-636'\n",
      "transformer_pytorch_model_name                   -> 'pytorch_model.bin'\n",
      "transformer_pytorch_model_s3_uri                 -> 's3://sagemaker-us-west-2-250107111215/models/tran\n",
      "trial_name                                       -> 'trial-1598124929'\n",
      "validation_split_percentage                      -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
