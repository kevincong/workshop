{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Apache Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Apache Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Apache Spark in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 17:41:31   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-08-22 17:41:44   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job\n",
    "\n",
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset processing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mopenjdk:8-jre-slim\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install py4j \u001b[31mpsutil\u001b[39;49;00m==\u001b[34m5\u001b[39;49;00m.6.5 \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.17.4\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get clean\r\n",
      "\u001b[34mRUN\u001b[39;49;00m rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "\u001b[37m# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONHASHSEED \u001b[34m0\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONIOENCODING UTF-8\r\n",
      "\u001b[34mENV\u001b[39;49;00m PIP_DISABLE_PIP_VERSION_CHECK \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Hadoop\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_VERSION \u001b[34m3\u001b[39;49;00m.2.1\r\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_HOME /usr/hadoop-\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHADOOP_CONF_DIR\u001b[39;49;00m=\u001b[31m$HADOOP_HOME\u001b[39;49;00m/etc/hadoop\r\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[31m$HADOOP_HOME\u001b[39;49;00m/bin\r\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m10\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttp://archive.apache.org/dist/hadoop/common/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | tar -x -C /usr/ \u001b[33m\\\u001b[39;49;00m\r\n",
      " && rm -rf \u001b[31m$HADOOP_HOME\u001b[39;49;00m/share/doc \u001b[33m\\\u001b[39;49;00m\r\n",
      " && chown -R root:root \u001b[31m$HADOOP_HOME\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Spark\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_VERSION \u001b[34m2\u001b[39;49;00m.4.6\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_PACKAGE spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m-bin-without-hadoop\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_HOME /usr/spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mSPARK_DIST_CLASSPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/etc/hadoop/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/tools/lib/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_HOME\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/bin\r\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m3\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://archive.apache.org/dist/spark/spark-\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_PACKAGE\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.tgz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | tar x -C /usr/ \u001b[33m\\\u001b[39;49;00m\r\n",
      " && mv /usr/\u001b[31m$SPARK_PACKAGE\u001b[39;49;00m \u001b[31m$SPARK_HOME\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      " && chown -R root:root \u001b[31m$SPARK_HOME\u001b[39;49;00m\r\n",
      " \r\n",
      "\u001b[37m# Point Spark at proper python binary\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYSPARK_PYTHON\u001b[39;49;00m=/usr/bin/python3\r\n",
      "\r\n",
      "\u001b[37m# Setup Spark/Yarn/HDFS user as root\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m/usr/bin:/opt/program:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mPATH\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_RESOURCEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_NODEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_NAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_DATANODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_SECONDARYNAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Set up bootstrapping program and Spark configuration\u001b[39;49;00m\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m program /opt/program\r\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /opt/program/submit\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m hadoop-config /opt/hadoop-config\r\n",
      "\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m jars /usr/jars\r\n",
      "\r\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $SPARK_HOME\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Transformers and TensorFlow\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install -q pip --upgrade\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install -q wrapt --upgrade --ignore-installed\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install -q \u001b[31mtransformers\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.8.0\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install -q \u001b[31mtensorflow\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.1.0 --upgrade --ignore-installed\r\n",
      "\r\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"/opt/program/submit\"\u001b[39;49;00m]\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/37 : FROM openjdk:8-jre-slim\n",
      " ---> d2f9f3c77c25\n",
      "Step 2/37 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> cf302be67c70\n",
      "Step 3/37 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> 62d56031da94\n",
      "Step 4/37 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 381148b87d0e\n",
      "Step 5/37 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> f0ec1306ff8a\n",
      "Step 6/37 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> dba6330dec66\n",
      "Step 7/37 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 4a8cdcf14e77\n",
      "Step 8/37 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 911008bea772\n",
      "Step 9/37 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> ede4fd31963b\n",
      "Step 10/37 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> f844be4ebfd9\n",
      "Step 11/37 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> d36f503d5444\n",
      "Step 12/37 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 2529e926dddc\n",
      "Step 13/37 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 6b52f8636bb3\n",
      "Step 14/37 : RUN curl -sL --retry 10   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Running in 79acb7c0b32e\n",
      "Removing intermediate container 79acb7c0b32e\n",
      " ---> 205af4b98dc8\n",
      "Step 15/37 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Running in e92248ad0ae3\n",
      "Removing intermediate container e92248ad0ae3\n",
      " ---> aa8381acf148\n",
      "Step 16/37 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Running in e87c9407cef6\n",
      "Removing intermediate container e87c9407cef6\n",
      " ---> f8245b5f2ade\n",
      "Step 17/37 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Running in 3dc2b3aa858b\n",
      "Removing intermediate container 3dc2b3aa858b\n",
      " ---> 3b626bbe7b49\n",
      "Step 18/37 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Running in 5dd6dff36044\n",
      "Removing intermediate container 5dd6dff36044\n",
      " ---> 71d7e7be1b1a\n",
      "Step 19/37 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Running in 4a94f79f25e7\n",
      "Removing intermediate container 4a94f79f25e7\n",
      " ---> 88a6305b35ad\n",
      "Step 20/37 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Running in eaaff7f907d2\n",
      "Removing intermediate container eaaff7f907d2\n",
      " ---> ee58b5405c5b\n",
      "Step 21/37 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Running in 82dcb22249bd\n",
      "Removing intermediate container 82dcb22249bd\n",
      " ---> e4263067929b\n",
      "Step 22/37 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Running in 323c975782af\n",
      "Removing intermediate container 323c975782af\n",
      " ---> e3f1abbe6cdc\n",
      "Step 23/37 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Running in 8df6d2a30138\n",
      "Removing intermediate container 8df6d2a30138\n",
      " ---> d5cfe9333929\n",
      "Step 24/37 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Running in c9e6c1063ca4\n",
      "Removing intermediate container c9e6c1063ca4\n",
      " ---> 94822279e130\n",
      "Step 25/37 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Running in 233914c923fd\n",
      "Removing intermediate container 233914c923fd\n",
      " ---> 064690f28b6b\n",
      "Step 26/37 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Running in ad606a996c7c\n",
      "Removing intermediate container ad606a996c7c\n",
      " ---> 87abd3427842\n",
      "Step 27/37 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Running in 60c274f34872\n",
      "Removing intermediate container 60c274f34872\n",
      " ---> 9a2179984205\n",
      "Step 28/37 : COPY program /opt/program\n",
      " ---> a9159f40b39a\n",
      "Step 29/37 : RUN chmod +x /opt/program/submit\n",
      " ---> Running in b8a7d9c26826\n",
      "Removing intermediate container b8a7d9c26826\n",
      " ---> 48e75c299b3e\n",
      "Step 30/37 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> 5e763116fbd9\n",
      "Step 31/37 : COPY jars /usr/jars\n",
      " ---> 510b0efdfdf2\n",
      "Step 32/37 : WORKDIR $SPARK_HOME\n",
      " ---> Running in c4850695c1a4\n",
      "Removing intermediate container c4850695c1a4\n",
      " ---> e0eafbf627ac\n",
      "Step 33/37 : RUN pip3 install -q pip --upgrade\n",
      " ---> Running in 2ed0d0b12c6e\n",
      "Removing intermediate container 2ed0d0b12c6e\n",
      " ---> 4dd2aaba510a\n",
      "Step 34/37 : RUN pip3 install -q wrapt --upgrade --ignore-installed\n",
      " ---> Running in 02abd4b53dff\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container 02abd4b53dff\n",
      " ---> ecb7f597b605\n",
      "Step 35/37 : RUN pip3 install -q transformers==2.8.0\n",
      " ---> Running in 7b657720f0ff\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container 7b657720f0ff\n",
      " ---> 41302aca0a10\n",
      "Step 36/37 : RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
      " ---> Running in d47dfcf37a0b\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container d47dfcf37a0b\n",
      " ---> e09915b6efb9\n",
      "Step 37/37 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in db21ac77b9af\n",
      "Removing intermediate container db21ac77b9af\n",
      " ---> 8671534dc1ce\n",
      "Successfully built 8671534dc1ce\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Docker Image\n",
    "If the image did not build properly, re-run the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"Id\": \"sha256:8671534dc1ce8fffdfd87980dc56e9b65a113b55ee8c8b154980c826c6667258\",\r\n",
      "        \"RepoTags\": [\r\n",
      "            \"amazon-reviews-spark-processor:latest\"\r\n",
      "        ],\r\n",
      "        \"RepoDigests\": [],\r\n",
      "        \"Parent\": \"sha256:e09915b6efb9e4238d028b7fce0ff09e800b5fc9a9f5e7424155a796c788ab3a\",\r\n",
      "        \"Comment\": \"\",\r\n",
      "        \"Created\": \"2020-08-22T18:44:30.675788897Z\",\r\n",
      "        \"Container\": \"db21ac77b9afa3852e86075baa270e72acc7d64e3b67cbcf996e4b9cf1f1bf73\",\r\n",
      "        \"ContainerConfig\": {\r\n",
      "            \"Hostname\": \"db21ac77b9af\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": [\r\n",
      "                \"PATH=/usr/bin:/opt/program:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/hadoop-3.2.1/bin:/usr/spark-2.4.6/bin\",\r\n",
      "                \"LANG=C.UTF-8\",\r\n",
      "                \"JAVA_HOME=/usr/local/openjdk-8\",\r\n",
      "                \"JAVA_VERSION=8u265\",\r\n",
      "                \"PYTHONHASHSEED=0\",\r\n",
      "                \"PYTHONIOENCODING=UTF-8\",\r\n",
      "                \"PIP_DISABLE_PIP_VERSION_CHECK=1\",\r\n",
      "                \"HADOOP_VERSION=3.2.1\",\r\n",
      "                \"HADOOP_HOME=/usr/hadoop-3.2.1\",\r\n",
      "                \"HADOOP_CONF_DIR=/usr/hadoop-3.2.1/etc/hadoop\",\r\n",
      "                \"SPARK_VERSION=2.4.6\",\r\n",
      "                \"SPARK_PACKAGE=spark-2.4.6-bin-without-hadoop\",\r\n",
      "                \"SPARK_HOME=/usr/spark-2.4.6\",\r\n",
      "                \"SPARK_DIST_CLASSPATH=/usr/hadoop-3.2.1/etc/hadoop/*:/usr/hadoop-3.2.1/share/hadoop/common/lib/*:/usr/hadoop-3.2.1/share/hadoop/common/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/*:/usr/hadoop-3.2.1/share/hadoop/yarn/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/*:/usr/hadoop-3.2.1/share/hadoop/tools/lib/*\",\r\n",
      "                \"PYSPARK_PYTHON=/usr/bin/python3\",\r\n",
      "                \"YARN_RESOURCEMANAGER_USER=root\",\r\n",
      "                \"YARN_NODEMANAGER_USER=root\",\r\n",
      "                \"HDFS_NAMENODE_USER=root\",\r\n",
      "                \"HDFS_DATANODE_USER=root\",\r\n",
      "                \"HDFS_SECONDARYNAMENODE_USER=root\"\r\n",
      "            ],\r\n",
      "            \"Cmd\": [\r\n",
      "                \"/bin/sh\",\r\n",
      "                \"-c\",\r\n",
      "                \"#(nop) \",\r\n",
      "                \"ENTRYPOINT [\\\"/opt/program/submit\\\"]\"\r\n",
      "            ],\r\n",
      "            \"Image\": \"sha256:e09915b6efb9e4238d028b7fce0ff09e800b5fc9a9f5e7424155a796c788ab3a\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"/usr/spark-2.4.6\",\r\n",
      "            \"Entrypoint\": [\r\n",
      "                \"/opt/program/submit\"\r\n",
      "            ],\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": {}\r\n",
      "        },\r\n",
      "        \"DockerVersion\": \"19.03.6-ce\",\r\n",
      "        \"Author\": \"\",\r\n",
      "        \"Config\": {\r\n",
      "            \"Hostname\": \"\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": [\r\n",
      "                \"PATH=/usr/bin:/opt/program:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/hadoop-3.2.1/bin:/usr/spark-2.4.6/bin\",\r\n",
      "                \"LANG=C.UTF-8\",\r\n",
      "                \"JAVA_HOME=/usr/local/openjdk-8\",\r\n",
      "                \"JAVA_VERSION=8u265\",\r\n",
      "                \"PYTHONHASHSEED=0\",\r\n",
      "                \"PYTHONIOENCODING=UTF-8\",\r\n",
      "                \"PIP_DISABLE_PIP_VERSION_CHECK=1\",\r\n",
      "                \"HADOOP_VERSION=3.2.1\",\r\n",
      "                \"HADOOP_HOME=/usr/hadoop-3.2.1\",\r\n",
      "                \"HADOOP_CONF_DIR=/usr/hadoop-3.2.1/etc/hadoop\",\r\n",
      "                \"SPARK_VERSION=2.4.6\",\r\n",
      "                \"SPARK_PACKAGE=spark-2.4.6-bin-without-hadoop\",\r\n",
      "                \"SPARK_HOME=/usr/spark-2.4.6\",\r\n",
      "                \"SPARK_DIST_CLASSPATH=/usr/hadoop-3.2.1/etc/hadoop/*:/usr/hadoop-3.2.1/share/hadoop/common/lib/*:/usr/hadoop-3.2.1/share/hadoop/common/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/*:/usr/hadoop-3.2.1/share/hadoop/yarn/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/*:/usr/hadoop-3.2.1/share/hadoop/tools/lib/*\",\r\n",
      "                \"PYSPARK_PYTHON=/usr/bin/python3\",\r\n",
      "                \"YARN_RESOURCEMANAGER_USER=root\",\r\n",
      "                \"YARN_NODEMANAGER_USER=root\",\r\n",
      "                \"HDFS_NAMENODE_USER=root\",\r\n",
      "                \"HDFS_DATANODE_USER=root\",\r\n",
      "                \"HDFS_SECONDARYNAMENODE_USER=root\"\r\n",
      "            ],\r\n",
      "            \"Cmd\": null,\r\n",
      "            \"Image\": \"sha256:e09915b6efb9e4238d028b7fce0ff09e800b5fc9a9f5e7424155a796c788ab3a\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"/usr/spark-2.4.6\",\r\n",
      "            \"Entrypoint\": [\r\n",
      "                \"/opt/program/submit\"\r\n",
      "            ],\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": null\r\n",
      "        },\r\n",
      "        \"Architecture\": \"amd64\",\r\n",
      "        \"Os\": \"linux\",\r\n",
      "        \"Size\": 3529118623,\r\n",
      "        \"VirtualSize\": 3529118623,\r\n",
      "        \"GraphDriver\": {\r\n",
      "            \"Data\": {\r\n",
      "                \"LowerDir\": \"/var/lib/docker/overlay2/fe07360d3d0968a3ae81c4b76192b1c8f7e36a4aeb376240c3ef496c22206db0/diff:/var/lib/docker/overlay2/e8097bdf893494f3e12166a0f8b8a123a6dc175085f63f8a52b244b55c1294a2/diff:/var/lib/docker/overlay2/fa37c9e94bc5483e521cc231d01dece78bc67d5a08bc5b8a7ff4cbad0fbdb7af/diff:/var/lib/docker/overlay2/210a79d3430ad7647d1810aac43f23024e59cdba1d62ad9af11d4a25566be282/diff:/var/lib/docker/overlay2/bb5564f78e248441dfeba3f0674a595c867b5b6d9858e4613fda97643f63acb0/diff:/var/lib/docker/overlay2/94bda8da3dd430762b51dbebe72a3211d9baaa22bd5baae9266ee47a41addf1c/diff:/var/lib/docker/overlay2/36642c35199c102be238aa651bfeaf49d3d99c7b7440835c11f7a28c4253e0cb/diff:/var/lib/docker/overlay2/cb5e7497e5e6555165fd53ae4db1e15634855b0dcff3eb40fd3220693e9776f0/diff:/var/lib/docker/overlay2/894505b621a39fd248c1a3a1bf89094ea76bd85b34de9fbe4a2c55fbd723265c/diff:/var/lib/docker/overlay2/4892563a50cc2aa953c3c1336c0fc982ea8560be156780e107abd5f2beac2721/diff:/var/lib/docker/overlay2/daa131b055a81993cf5e32c0035dfa888bba2aefefcd3ae43f8484ea4e910f97/diff:/var/lib/docker/overlay2/d076aa8722dcfbd9ae09882991e14f2e7d2a4f5b251fb361dced3e854d95b39b/diff:/var/lib/docker/overlay2/e06b8eef919046502cc04e92385571e318fb8f1ef7feff06dc37ff9f011a6609/diff:/var/lib/docker/overlay2/9347119e169aef2c66d941d98b92f5bdaba84e2eaedcd20b32d56bfd23492ea8/diff:/var/lib/docker/overlay2/d0c30e4ab86c579b34c97d6464073714d16358473f891a03cc59a24b3e8e65c1/diff:/var/lib/docker/overlay2/9022dbfadf9d14f36c40ad196fe464584f738b8279875a1fca7623f23a9efb0e/diff:/var/lib/docker/overlay2/9a050f6f812c069a040a7144c13a7f1c7e902231110dc94887887b3f6970063e/diff:/var/lib/docker/overlay2/4c685b6d2b4e7b610179d8f234a8d5472dfbb732a7057d02a9320ec75cb3e640/diff\",\r\n",
      "                \"MergedDir\": \"/var/lib/docker/overlay2/7bc83b0c85badc943930ea4275b3c2e6c168f7f944d4f558e1ddf1547ef9b71f/merged\",\r\n",
      "                \"UpperDir\": \"/var/lib/docker/overlay2/7bc83b0c85badc943930ea4275b3c2e6c168f7f944d4f558e1ddf1547ef9b71f/diff\",\r\n",
      "                \"WorkDir\": \"/var/lib/docker/overlay2/7bc83b0c85badc943930ea4275b3c2e6c168f7f944d4f558e1ddf1547ef9b71f/work\"\r\n",
      "            },\r\n",
      "            \"Name\": \"overlay2\"\r\n",
      "        },\r\n",
      "        \"RootFS\": {\r\n",
      "            \"Type\": \"layers\",\r\n",
      "            \"Layers\": [\r\n",
      "                \"sha256:d0f104dc0a1f9c744b65b23b3fd4d4d3236b4656e67f776fe13f8ad8423b955c\",\r\n",
      "                \"sha256:95c20fa5728d237b86ad3a7f2997e3ad71ec472165297872c119108de68e7114\",\r\n",
      "                \"sha256:ed7152ed4cbd0de85948780e8b33ffe66728f56b864bb8db4b520089b48c4883\",\r\n",
      "                \"sha256:43ee8102614ddbd62242c06c4a2f7dc50fd81a9dbd72daf75467c65264d95d4f\",\r\n",
      "                \"sha256:3ccbffc3afeac065230de307a96b5af1b39b12e3005f66eaa7cc5da42a32fad7\",\r\n",
      "                \"sha256:aaae35ba2eea36f7980d7274c2f93188f658dea064ce085a4d60a8f1984e7582\",\r\n",
      "                \"sha256:597b99d2e6753650c11b5ab26d936219c5d6ac75a9139adef136eb49b536f058\",\r\n",
      "                \"sha256:16e96348f10328085fe9b6746f83ccb19ab27256b46392c1ff328f4418eb03ad\",\r\n",
      "                \"sha256:6a07aa6518ef987ff72e2a0d36cab867ae653dc9f2c958b984d62e0f5c8e83ed\",\r\n",
      "                \"sha256:1116c0d853761738d609b96ffdd37931c91b5fffd165d6e645dc3f637dc8b693\",\r\n",
      "                \"sha256:66305a4e7378fb16d3af49bda25f88c1fcb0868938ac7e55ada447e40d13e676\",\r\n",
      "                \"sha256:add6a8d94da81f09255530dd70aa128f9e4e99731b03a6768fd3c91e5b0b712a\",\r\n",
      "                \"sha256:e3bd1bfb9160b60b27ddff1a512ddeeed510b4a3e2053d8d280db887b3fe2744\",\r\n",
      "                \"sha256:ecfeb293986c510a277a5df8fbff84fedfc0e1476699858eb20930d88abf66e3\",\r\n",
      "                \"sha256:e1ad8fe2ab9bca0b057763857c321a6d71c9c72fd856692f138287d711c342a7\",\r\n",
      "                \"sha256:fe6737afa9f51aa21cd5082aa3cfcfbc64196b31bed2df01bba78792d2d42eef\",\r\n",
      "                \"sha256:ee174f1898a067a6f781629420741a172b5f91f4f1d4333e9b3a614dc14a8379\",\r\n",
      "                \"sha256:6b91b8554a60cf41a576987a2a6cf68534aa5535ee9ee981b67dde7caa68bbac\",\r\n",
      "                \"sha256:caeb27d0bf817f37a607d8670c033c860cad6982fbcd4387e66e86aa8a9d109b\"\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"Metadata\": {\r\n",
      "            \"LastTagTime\": \"2020-08-22T18:44:30.710909858Z\"\r\n",
      "        }\r\n",
      "    }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!docker inspect $docker_repo:$docker_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Image to a Private Docker Repo (Amazon ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore the `RepositoryNotFoundException` Error Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-processor' does not exist in the registry with id '250107111215'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:250107111215:repository/amazon-reviews-spark-processor\",\n",
      "        \"registryId\": \"250107111215\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-processor\",\n",
      "        \"repositoryUri\": \"250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor\",\n",
      "        \"createdAt\": 1598121877.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        },\n",
      "        \"encryptionConfiguration\": {\n",
      "            \"encryptionType\": \"AES256\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1B27d0bf81: Preparing \n",
      "\u001b[1Bb8554a60: Preparing \n",
      "\u001b[1B4f1898a0: Preparing \n",
      "\u001b[1B37afa9f5: Preparing \n",
      "\u001b[1B8fe2ab9b: Preparing \n",
      "\u001b[1Bb293986c: Preparing \n",
      "\u001b[1B1bfb9160: Preparing \n",
      "\u001b[1Ba8d94da8: Preparing \n",
      "\u001b[1B5a4e7378: Preparing \n",
      "\u001b[1Bc0d85376: Preparing \n",
      "\u001b[1Baa6518ef: Preparing \n",
      "\u001b[1B6348f103: Preparing \n",
      "\u001b[1B99d2e675: Preparing \n",
      "\u001b[1B35ba2eea: Preparing \n",
      "\u001b[1Bffc3afea: Preparing \n",
      "\u001b[1B8102614d: Preparing \n",
      "\u001b[1B52ed4cbd: Preparing \n",
      "\u001b[1B0fa5728d: Preparing \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19B7d0bf81: Pushing  1.909GB/2.012GB\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[16A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2KPushing  209.8MB/481.8MB\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2KPushing   53.2MB/69.22MB\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19B7d0bf81: Pushed   2.022GB/2.012GB\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2Klatest: digest: sha256:e1f613fc0d007d743bc451ce0d76bb3ca5e2d5ce66550e4fa0c784afbde9403a size: 4318\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Job using a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a Spark ML script for processing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pip', '--upgrade'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wrapt', '--upgrade', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlinalg\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DenseVector\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m udf, col\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# We set sequences to be at most 128 tokens long.\u001b[39;49;00m\r\n",
      "MAX_SEQ_LENGTH = \u001b[34m64\u001b[39;49;00m\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(label, text):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#    tokens = tokenizer.tokenize(text_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=MAX_SEQ_LENGTH)\r\n",
      "\r\n",
      "    \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * MAX_SEQ_LENGTH\r\n",
      "\r\n",
      "    \u001b[37m# Label for our training data (star_rating 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[label]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_ids, \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mask, \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: segment_ids, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [label_id]}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform\u001b[39;49;00m(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            schema=schema,\r\n",
      "                            header=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            quote=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    \u001b[37m# This dataset should already be clean, but always good to double-check\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing null review_body rows...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv.where(col(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).isNull()).show()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing cleaned csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv_dropped = df_csv.na.drop(subset=[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    df_csv_dropped.show()\r\n",
      "\r\n",
      "    \u001b[37m# TODO:  Balance\u001b[39;49;00m\r\n",
      "    \r\n",
      "    features_df = df_csv_dropped.select([\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    features_df.show()\r\n",
      "\r\n",
      "    tfrecord_schema = StructType([\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m))\r\n",
      "    ])\r\n",
      "\r\n",
      "    bert_transformer = udf(\u001b[34mlambda\u001b[39;49;00m text, label: convert_input(text, label), tfrecord_schema)\r\n",
      "\r\n",
      "    spark.udf.register(\u001b[33m'\u001b[39;49;00m\u001b[33mbert_transformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bert_transformer)\r\n",
      "\r\n",
      "    transformed_df = features_df.select(bert_transformer(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).alias(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    transformed_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    flattened_df = transformed_df.select(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    flattened_df.show()\r\n",
      "\r\n",
      "    \u001b[37m# Split 90-5-5%\u001b[39;49;00m\r\n",
      "    train_df, validation_df, test_df = flattened_df.randomSplit([\u001b[34m0.9\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_train_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_train_data))\r\n",
      "    \r\n",
      "    validation_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_validation_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_test_data)    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_test_data))\r\n",
      "\r\n",
      "    restored_test_df = spark.read.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).load(path=s3_output_test_data)\r\n",
      "    restored_test_df.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    spark = SparkSession.builder.appName(\u001b[33m'\u001b[39;49;00m\u001b[33mAmazonReviewsSparkProcessor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Convert command line args into a map of args\u001b[39;49;00m\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "\r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, \r\n",
      "              s3_input_data, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        \u001b[37m# s3_output_train_data, s3_output_validation_data, s3_output_test_data\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-spark-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            env={'mode': 'python'},\n",
    "                            max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-train\n",
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-validation\n",
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "train_data_bert_output = 's3://{}/{}/output/bert-train'.format(bucket, output_prefix)\n",
    "validation_data_bert_output = 's3://{}/{}/output/bert-validation'.format(bucket, output_prefix)\n",
    "test_data_bert_output = 's3://{}/{}/output/bert-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_bert_output)\n",
    "print(validation_data_bert_output)\n",
    "print(test_data_bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-08-22-18-46-02-060\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-spark-text-to-bert.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_bert_output,\n",
    "                         's3_output_validation_data', validation_data_bert_output,\n",
    "                         's3_output_test_data', test_data_bert_output,                         \n",
    "              ],\n",
    "              # We need to define these outputs so we can later call  \n",
    "              #    ProcessingJob.from_processing_name() \n",
    "              # to describe the job and poll for Completed status\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/processing-jobs/spark-amazon-reviews-processor-2020-08-22-18-46-02-060\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-08-22-18-46-02-060;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-processor-2020-08-22-18-46-02-060/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-08-22-18-46-02-060', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}, 'AppSpecification': {'ImageUri': '250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-bert.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-train', 's3_output_validation_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-validation', 's3_output_test_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::250107111215:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:250107111215:processing-job/spark-amazon-reviews-processor-2020-08-22-18-46-02-060', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 22, 18, 46, 2, 590000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 22, 18, 46, 2, 590000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'c1600bf6-87d3-4f40-9ad8-edeaad33a143', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c1600bf6-87d3-4f40-9ad8-edeaad33a143', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2417', 'date': 'Sat, 22 Aug 2020 18:46:01 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:10,957 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.135.84\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachema\u001b[0m\n",
      "\u001b[34mnager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:10,967 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,048 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-7ac66180-58c7-44b6-9788-f3e1eebba4be\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,470 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,483 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,484 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,484 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,488 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,488 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,488 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,489 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,532 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,541 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,541 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,548 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,548 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Aug 22 18:50:11\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,549 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,549 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,551 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,551 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,591 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,591 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,597 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,598 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,599 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,621 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,621 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,621 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,621 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,634 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,634 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,634 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,634 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,651 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,651 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,652 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,652 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,656 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,657 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,661 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,661 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,661 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,661 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,667 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,667 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,668 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,670 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,671 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,672 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,672 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,672 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,672 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,694 INFO namenode.FSImage: Allocated new BlockPoolId: BP-959672285-10.0.135.84-1598122211687\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,708 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,733 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,816 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,828 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,831 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:11,832 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.135.84\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-135-84.us-west-2.compute.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-135-84.us-west-2.compute.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:24,332 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:25.270025: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:25.270119: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:25.270132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\u001b[0m\n",
      "\u001b[34m2.1.0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  15%|█▌        | 34.8k/232k [00:00<00:00, 243kB/s]#015Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 320kB/s] #015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 801kB/s]\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,310 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,333 INFO spark.SparkContext: Submitted application: AmazonReviewsSparkProcessor\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,369 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,369 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,369 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,369 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,369 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,573 INFO util.Utils: Successfully started service 'sparkDriver' on port 33113.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,592 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,604 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,606 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,606 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,613 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-208f9b80-1ea0-45ab-b52a-77920d010ac3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,627 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,668 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,731 INFO util.log: Logging initialized @4443ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,776 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,790 INFO server.Server: Started @4503ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,802 INFO server.AbstractConnector: Started ServerConnector@72ba6360{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,802 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,822 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72922f{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,822 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19d5d37a{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,823 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20009b7c{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,826 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31420d3{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,827 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@276b90b6{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,827 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8e0dd6c{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,828 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@578b2b14{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,829 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2becbb{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,830 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e76a421{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,831 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2738298a{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,831 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb76df1{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,832 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4260c08{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,833 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fcaf701{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,833 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5958ed0b{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,834 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39e0db34{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,835 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d92d64c{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,835 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65085a00{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,836 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ba6c31f{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,836 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44a08e24{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,837 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f1b104a{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,843 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@786489f3{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,844 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cbdde26{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,845 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12cc5092{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,846 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@666d1043{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,846 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@220be8ed{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:27,848 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.135.84:4040\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,528 INFO client.RMProxy: Connecting to ResourceManager at /10.0.135.84:8032\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,813 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,877 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,878 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,892 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,892 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,893 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,896 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,901 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:28,948 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:29,807 INFO yarn.Client: Uploading resource file:/tmp/spark-22a85107-f273-4cbb-bdb1-248b86f3c423/__spark_libs__805963246420870797.zip -> hdfs://10.0.135.84/user/root/.sparkStaging/application_1598122222765_0001/__spark_libs__805963246420870797.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:30,233 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:30,978 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,172 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.135.84/user/root/.sparkStaging/application_1598122222765_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,183 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,207 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.135.84/user/root/.sparkStaging/application_1598122222765_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,215 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,353 INFO yarn.Client: Uploading resource file:/tmp/spark-22a85107-f273-4cbb-bdb1-248b86f3c423/__spark_conf__2979095310598182929.zip -> hdfs://10.0.135.84/user/root/.sparkStaging/application_1598122222765_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,363 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,394 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,394 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,394 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,394 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:31,394 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:50:32,193 INFO yarn.Client: Submitting application application_1598122222765_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:32,385 INFO impl.YarnClientImpl: Submitted application application_1598122222765_0001\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:32,387 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1598122222765_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:33,395 INFO yarn.Client: Application report for application_1598122222765_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:33,399 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1598122232289\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1598122222765_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:34,401 INFO yarn.Client: Application report for application_1598122222765_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:35,403 INFO yarn.Client: Application report for application_1598122222765_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:36,406 INFO yarn.Client: Application report for application_1598122222765_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:37,409 INFO yarn.Client: Application report for application_1598122222765_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,074 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1598122222765_0001), /proxy/application_1598122222765_0001\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,216 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,411 INFO yarn.Client: Application report for application_1598122222765_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,411 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.158.39\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1598122232289\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1598122222765_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,413 INFO cluster.YarnClientSchedulerBackend: Application application_1598122222765_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,433 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44135.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,433 INFO netty.NettyBlockTransferService: Server created on 10.0.135.84:44135\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,434 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,454 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.135.84, 44135, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,456 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.135.84:44135 with 366.3 MB RAM, BlockManagerId(driver, 10.0.135.84, 44135, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,458 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.135.84, 44135, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,458 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.135.84, 44135, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,556 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:38,562 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f222efe{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:41,090 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.158.39:57920) with ID 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:41,171 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:35053 with 11.9 GB RAM, BlockManagerId(1, algo-2, 35053, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:57,996 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,230 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,230 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,236 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,237 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c053361{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,237 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,238 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51ffab33{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,238 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,239 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11fa811c{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,239 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,240 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29177c09{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,240 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,241 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24fed597{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,523 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-train\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-validation\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-processor-2020-08-22-18-46-02/output/bert-test\u001b[0m\n",
      "\u001b[34mProcessing s3a://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/ => /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,700 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,747 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:58,747 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-08-22 18:50:59,986 INFO datasources.InMemoryFileIndex: It took 111 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:00,421 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:00,427 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:00,430 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:00,437 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:00,813 INFO codegen.CodeGenerator: Code generated in 227.335889 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,014 INFO codegen.CodeGenerator: Code generated in 36.530454 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,067 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,112 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,114 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,116 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,137 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,207 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,220 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,220 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,221 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,222 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,225 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,280 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,282 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,282 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.135.84:44135 (size: 7.3 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,284 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,298 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,299 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,398 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:01,659 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:35053 (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:51:02,184 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,198 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2860 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,200 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,206 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.971 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,210 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 3.002318 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing null review_body rows...\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,291 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,292 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnull(review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,292 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,295 INFO execution.FileSourceScanExec: Pushed Filters: IsNull(review_body)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,360 INFO codegen.CodeGenerator: Code generated in 28.235561 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,368 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,386 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,387 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,388 INFO spark.SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,388 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,399 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,401 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,401 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,401 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,402 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,402 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,408 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,409 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,410 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.135.84:44135 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,410 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,411 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,411 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,413 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,446 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:35053 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:04,501 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,581 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2169 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,582 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,582 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.178 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,583 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.183151 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,586 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,587 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,587 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,587 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,587 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,588 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,590 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,591 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,592 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.135.84:44135 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,592 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,593 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,593 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,594 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:06,605 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:35053 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,092 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1498 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,092 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,094 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 1.506 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,095 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 1.508780 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   44947401| RKHI9WDB7S5WR|B008D7F47Q|     886655228|      FIFA Soccer 13|Digital_Video_Games|          5|            0|          0|   N|                N|          Five Stars|       null| 2015-05-08|\u001b[0m\n",
      "\u001b[34m|         US|    4190665|R2UJ6J6TUON90A|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          2|            0|          2|   N|                N|           Two Stars|       null| 2015-04-14|\u001b[0m\n",
      "\u001b[34m|         US|   51305484| RJVXCTIK9SIC0|B0040V4T8O|      97283480|Medieval II: Tota...|Digital_Video_Games|          5|            0|          0|   N|                Y|It's an amazing g...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48052209| R8PISK7HHT6BH|B004ZUFKEC|     321025745|The Sims 3 [Mac D...|Digital_Video_Games|          1|            0|          0|   N|                N| What a good wast...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   17134921|R2H22164810P1S|B007Z3RN2I|     137803476|Call of Duty: Bla...|Digital_Video_Games|          3|            0|          1|   N|                Y|I won't be wronge...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   47668954|R1N1YQ8RZOGAMP|B00506X3Y4|     692311664|  Duke Nukem Forever|Digital_Video_Games|          5|            1|          2|   N|                Y|What you waiting ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48104724|R2WUQWZTYWZYWY|B00PG8FFFQ|     245597084|Block Financial H...|   Digital_Software|          1|            0|          0|   N|                Y| I am very disapp...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   49035882|R3A8HHF16GNB9M|B005WX2X1Y|      35439242|        Rostta Stone|   Digital_Software|          1|            1|          4|   N|                Y| Very disappointe...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   46107169|R1G1KQDXWWAZGU|B00NG7JVSQ|     811978073|TurboTax Deluxe F...|   Digital_Software|          5|            0|          2|   N|                N| just as I would ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   18699318| RVYUT2W57G45K|B00NG7K2RA|     349370473|TurboTax Premier ...|   Digital_Software|          1|           24|         39|   N|                Y| Huge waste of ti...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   14757352|R1EAMCG69CDL91|B00I9FX20S|     676852534|Dupe Eliminator 1...|   Digital_Software|          5|            0|          0|   N|                N|The best tool for...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   52995039|R1VR3Q470S6Y5Q|B008S0IMCC|     534964191| Quicken Deluxe 2013|   Digital_Software|          1|            0|          0|   N|                Y|Tis Pity#011I have b...|       null|       null|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\n",
      "\u001b[0m\n",
      "\u001b[34mShowing cleaned csv\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,144 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,145 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,145 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,145 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,210 INFO codegen.CodeGenerator: Code generated in 27.675215 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,217 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 401.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,231 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,231 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,232 INFO spark.SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,232 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,241 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,242 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,242 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,243 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,243 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,243 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,247 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,248 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,249 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.135.84:44135 (size: 7.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,249 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,250 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,250 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,251 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,262 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:35053 (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,310 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,363 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 112 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,363 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,364 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.120 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,364 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.122868 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,402 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,403 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,403 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,403 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,424 INFO codegen.CodeGenerator: Code generated in 11.060216 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,440 INFO codegen.CodeGenerator: Code generated in 11.464983 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,445 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 401.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,457 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,457 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,458 INFO spark.SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,458 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,466 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,468 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,468 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,469 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,469 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,469 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,473 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.2 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,474 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,474 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.135.84:44135 (size: 6.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,475 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,475 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,476 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,477 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,488 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:35053 (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,528 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,617 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 141 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,617 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,618 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.148 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:08,619 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.152441 s\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|star_rating|         review_body|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|          2|I keep buying mad...|\u001b[0m\n",
      "\u001b[34m|          5|             Awesome|\u001b[0m\n",
      "\u001b[34m|          5|If you are preppi...|\u001b[0m\n",
      "\u001b[34m|          5|             Perfect|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|i like the new sk...|\u001b[0m\n",
      "\u001b[34m|          5|               Super|\u001b[0m\n",
      "\u001b[34m|          5|Excellent, fast a...|\u001b[0m\n",
      "\u001b[34m|          5|                  Ok|\u001b[0m\n",
      "\u001b[34m|          1|As has been writt...|\u001b[0m\n",
      "\u001b[34m|          5|Probably the best...|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|COOL BUT IT LAGES...|\u001b[0m\n",
      "\u001b[34m|          1|Lames purchase I ...|\u001b[0m\n",
      "\u001b[34m|          5|               Great|\u001b[0m\n",
      "\u001b[34m|          5|          Awesome!!!|\u001b[0m\n",
      "\u001b[34m|          4|pretty good but n...|\u001b[0m\n",
      "\u001b[34m|          5|What can I say......|\u001b[0m\n",
      "\u001b[34m|          5|Very fast to rece...|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,231 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,231 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,232 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,232 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,253 INFO codegen.CodeGenerator: Code generated in 8.160825 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,281 INFO codegen.CodeGenerator: Code generated in 19.840151 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,294 INFO codegen.CodeGenerator: Code generated in 9.294198 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,299 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 401.9 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,316 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,317 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,317 INFO spark.SparkContext: Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,318 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,378 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,379 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,379 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,379 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,379 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,379 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,386 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 849.5 KB, free 363.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,391 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 644.8 KB, free 362.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,392 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.135.84:44135 (size: 644.8 KB, free: 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,393 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,393 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,394 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,395 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:09,411 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:35053 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:10,129 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:51:12,173 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2779 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,173 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,174 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35081\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,175 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 2.795 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,176 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 2.797418 s\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|tfrecords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2562, 9343, 24890, 2296, 2095, 5327, 2027, 2131, 2067, 2000, 2374, 1012, 2023, 2086, 2544, 2003, 1037, 2210, 2488, 2084, 2197, 2086, 1011, 1011, 2021, 2008, 1005, 1055, 2025, 3038, 2172, 1012, 1996, 2208, 3504, 2307, 1012, 1996, 2069, 2518, 3308, 2007, 1996, 7284, 1010, 2003, 1996, 2126, 1996, 2867, 2024, 2467, 4440, 4691, 2006, 2169, 2060, 1012, 1026, 7987, 1013, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]   |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                          |\u001b[0m\n",
      "\u001b[34m|[[101, 2065, 2017, 2024, 17463, 4691, 2005, 1996, 2203, 1997, 1996, 2088, 2023, 2003, 2028, 1997, 2216, 2477, 2008, 2017, 2323, 2031, 5361, 2006, 2115, 1011, 2203, 1011, 1997, 1011, 1996, 1011, 2088, 1011, 6947, 7473, 1012, 16889, 2000, 1996, 2307, 14331, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                              |\u001b[0m\n",
      "\u001b[34m|[[101, 3819, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2066, 1996, 2047, 4813, 2066, 27849, 2964, 1999, 2023, 1010, 1998, 13215, 2003, 4569, 1012, 1045, 2036, 2066, 2035, 1996, 2047, 3857, 5549, 5167, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                 |\u001b[0m\n",
      "\u001b[34m|[[101, 3565, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 6581, 1010, 3435, 1998, 5851, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                         |\u001b[0m\n",
      "\u001b[34m|[[101, 7929, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 2004, 2038, 2042, 2517, 2011, 2061, 2116, 2500, 1010, 1045, 2855, 2439, 3037, 1999, 2023, 2208, 1012, 1045, 2572, 2145, 2652, 25022, 2615, 1018, 1998, 2293, 2009, 1012, 2009, 1005, 1055, 1037, 9467, 2138, 1045, 1005, 1049, 3201, 2005, 2019, 4423, 2544, 1997, 25022, 2615, 1018, 1998, 2031, 4741, 2005, 2055, 1037, 5476, 2005, 1037, 2488, 2544, 1997, 2009, 1012, 25022, 2615, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]] |\u001b[0m\n",
      "\u001b[34m|[[101, 2763, 1996, 2190, 2208, 2005, 4083, 5919, 1997, 2613, 3776, 2800, 1012, 6700, 15794, 2428, 2718, 1996, 3608, 2041, 1997, 1996, 2380, 2007, 2023, 2028, 1010, 2007, 2152, 4547, 3643, 1006, 2004, 2092, 2004, 2019, 14036, 2208, 1007, 1999, 3408, 1997, 2877, 2017, 2083, 1996, 24078, 1997, 2613, 3776, 2458, 1012, 2130, 2295, 2023, 2003, 2195, 2086, 2214, 2113, 1996, 11343, 1997, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]|\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 4658, 2021, 2009, 2474, 8449, 2632, 4140, 1997, 1996, 2051, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m|[[101, 20342, 2015, 5309, 1045, 2471, 2196, 2081, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 2307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 3492, 2204, 2021, 2025, 2004, 2204, 2004, 1996, 2034, 20911, 1997, 8298, 2208, 1011, 1011, 16092, 1026, 7987, 1013, 1028, 3897, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                             |\u001b[0m\n",
      "\u001b[34m|[[101, 2054, 2064, 1045, 2360, 1012, 1012, 1012, 12202, 2444, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m|[[101, 2200, 3435, 2000, 4374, 1010, 1998, 1997, 2278, 1037, 3404, 13966, 1998, 3647, 2173, 2000, 4965, 2478, 2115, 4923, 4003, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                               |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,260 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,261 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,261 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,261 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,286 INFO codegen.CodeGenerator: Code generated in 9.971118 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,311 INFO codegen.CodeGenerator: Code generated in 17.066247 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,316 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 401.9 KB, free 362.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,328 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.8 KB, free 362.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,329 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,329 INFO spark.SparkContext: Created broadcast 11 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,330 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,353 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,354 INFO scheduler.DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,355 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,355 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,355 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,355 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,360 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 849.1 KB, free 361.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,365 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 644.8 KB, free 360.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,366 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.135.84:44135 (size: 644.8 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,366 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,367 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,367 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,368 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,380 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:35053 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:12,406 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,188 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1821 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,189 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,190 INFO scheduler.DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 1.833 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,190 INFO scheduler.DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 1.836437 s\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|           input_ids|          input_mask|         segment_ids|label_ids|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2562,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 102,...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2065, 2017,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3819, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2066,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 3565, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 6581, 1010,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 7929, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2004, 2038,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2763, 1996,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 4658, 2021,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 20342, 2015...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2307, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3492, 2204,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 2054, 2064,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2200, 3435,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,393 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,393 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,393 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,393 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,401 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,402 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,402 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,403 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,403 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,403 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,425 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:35053 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,426 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.135.84:44135 in memory (size: 644.8 KB, free: 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,442 INFO codegen.CodeGenerator: Code generated in 33.845511 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,449 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 401.9 KB, free 361.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,453 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,454 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,455 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,458 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,459 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,461 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,463 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,464 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,467 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 42.8 KB, free 362.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,468 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,470 INFO spark.SparkContext: Created broadcast 13 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,470 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,482 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,482 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,483 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,484 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,485 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,485 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,485 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,496 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:35053 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,507 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.135.84:44135 in memory (size: 644.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,529 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,530 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,530 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,530 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,530 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,530 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,531 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,532 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,533 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,534 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,534 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,534 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,534 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,534 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,543 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:35053 in memory (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,545 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.135.84:44135 in memory (size: 7.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,556 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,559 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,559 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,559 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,561 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,566 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,566 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,568 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,570 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,572 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,572 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,572 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,574 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.135.84:44135 in memory (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,575 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:35053 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,581 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,584 INFO scheduler.DAGScheduler: Got job 7 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,584 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,584 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,585 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,585 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,595 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,597 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,598 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,621 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,622 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,623 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,624 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,625 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,626 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,646 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:35053 in memory (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,647 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.135.84:44135 in memory (size: 7.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,658 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,663 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:35053 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,667 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.135.84:44135 in memory (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,672 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,679 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 979.8 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,680 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:35053 in memory (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,681 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.135.84:44135 in memory (size: 6.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,684 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,685 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 692.2 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,692 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.135.84:44135 (size: 692.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,692 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,692 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,693 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,694 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,694 INFO cluster.YarnScheduler: Adding task set 7.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,697 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,697 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 8, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,707 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,713 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:35053 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:51:14,826 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 19:00:37,479 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 8) in 562782 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,284 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 775587 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,284 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,285 INFO scheduler.DAGScheduler: ResultStage 7 (runJob at SparkHadoopWriter.scala:78) finished in 775.698 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,285 INFO scheduler.DAGScheduler: Job 7 finished: runJob at SparkHadoopWriter.scala:78, took 775.704249 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,305 INFO io.SparkHadoopWriter: Job job_20200822185114_0038 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,347 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,347 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,347 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,347 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,384 INFO codegen.CodeGenerator: Code generated in 28.193192 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,390 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 401.9 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,402 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 42.8 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,402 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,403 INFO spark.SparkContext: Created broadcast 15 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,403 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,432 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,433 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,447 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,447 INFO scheduler.DAGScheduler: Got job 8 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,447 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,447 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,448 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,448 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,465 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 979.8 KB, free 362.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,469 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 692.3 KB, free 362.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,470 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.135.84:44135 (size: 692.3 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,470 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,470 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,470 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,471 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,471 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,484 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:35053 (size: 692.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:04:10,526 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:13:29,798 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 10) in 559327 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,166 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 771695 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,166 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,167 INFO scheduler.DAGScheduler: ResultStage 8 (runJob at SparkHadoopWriter.scala:78) finished in 771.718 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,168 INFO scheduler.DAGScheduler: Job 8 finished: runJob at SparkHadoopWriter.scala:78, took 771.720774 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,178 INFO io.SparkHadoopWriter: Job job_20200822190410_0049 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/validation\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,215 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,216 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,216 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,216 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,247 INFO codegen.CodeGenerator: Code generated in 25.617758 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,253 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 401.9 KB, free 361.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,269 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 42.8 KB, free 361.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,270 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.135.84:44135 (size: 42.8 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,270 INFO spark.SparkContext: Created broadcast 17 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,271 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,303 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,303 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,315 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,316 INFO scheduler.DAGScheduler: Got job 9 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,316 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,316 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,316 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,316 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,332 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 979.8 KB, free 360.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,337 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 692.2 KB, free 360.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,337 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.135.84:44135 (size: 692.2 KB, free: 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,337 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,338 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,338 INFO cluster.YarnScheduler: Adding task set 9.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,339 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,339 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 12, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,349 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:35053 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:17:02,406 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:35053 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,125 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,127 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.135.84:44135 in memory (size: 692.2 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,151 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:35053 in memory (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,172 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,172 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,172 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,173 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.135.84:44135 in memory (size: 692.3 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,177 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:35053 in memory (size: 692.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,206 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,207 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,207 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,207 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,208 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,222 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,225 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,225 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,225 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,225 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-08-22 19:19:33,225 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,728 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,732 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.135.84:44135 in memory (size: 42.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,744 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:35053 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:20:38,767 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-08-22 19:26:18,978 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 12) in 556639 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,363 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 772025 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,363 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,363 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 772.046 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,364 INFO scheduler.DAGScheduler: Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 772.048836 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,377 INFO io.SparkHadoopWriter: Job job_20200822191702_0060 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/test\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,391 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 389.7 KB, free 363.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,407 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.3 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,408 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.135.84:44135 (size: 42.3 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,408 INFO spark.SparkContext: Created broadcast 19 from newAPIHadoopFile at TensorflowRelation.scala:33\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,466 INFO input.FileInputFormat: Total input files to process : 2\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,487 INFO spark.SparkContext: Starting job: aggregate at TensorFlowInferSchema.scala:39\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,488 INFO scheduler.DAGScheduler: Got job 10 (aggregate at TensorFlowInferSchema.scala:39) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,488 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,488 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,488 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,488 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,493 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,494 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,494 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.135.84:44135 (size: 2.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,494 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,497 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,497 INFO cluster.YarnScheduler: Adding task set 10.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,508 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, algo-2, executor 1, partition 0, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,508 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14, algo-2, executor 1, partition 1, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,529 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:35053 (size: 2.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,552 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:35053 (size: 42.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:54,903 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 395 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,123 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 623 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,123 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,123 INFO scheduler.DAGScheduler: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39) finished in 0.631 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,123 INFO scheduler.DAGScheduler: Job 10 finished: aggregate at TensorFlowInferSchema.scala:39, took 0.636010 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,174 INFO codegen.CodeGenerator: Code generated in 16.636126 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,180 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,180 INFO scheduler.DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,180 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,180 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,181 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,181 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,185 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,186 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.4 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,186 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.135.84:44135 (size: 5.4 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,187 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,187 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,187 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,188 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15, algo-2, executor 1, partition 0, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,199 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:35053 (size: 5.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,323 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 135 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,323 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,323 INFO scheduler.DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.142 s\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,324 INFO scheduler.DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.143625 s\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|label_ids|           input_ids|         segment_ids|          input_mask|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|        3|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2184,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2322,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2410,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2423,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 2753,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2871,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 3438,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 4749,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        1|[101, 1006, 2517,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        3|[101, 1006, 3617,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        2|[101, 1006, 10651...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,565 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,568 INFO server.AbstractConnector: Stopped Spark@72ba6360{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,571 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.135.84:4040\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,574 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,589 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,589 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,591 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,592 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,597 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,604 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,605 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,605 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,607 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,612 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,612 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,613 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0b706f50-ce99-4493-aaeb-6ad21c86d0cf\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,615 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-22a85107-f273-4cbb-bdb1-248b86f3c423/pyspark-56d71b01-0a08-45dd-939f-991ee24546b8\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,617 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-22a85107-f273-4cbb-bdb1-248b86f3c423\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,619 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,620 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-08-22 19:29:55,620 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-08-22 19:29:56\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Dataset\n",
    "\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._\n",
    "\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
