{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [**Deequ**](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here is a great blog post on Deequ for more information:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset preprocessing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mopenjdk:8-jre-slim\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install py4j \u001b[31mpsutil\u001b[39;49;00m==\u001b[34m5\u001b[39;49;00m.6.5 \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.17.4\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get clean\n",
      "\u001b[34mRUN\u001b[39;49;00m rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "\u001b[37m# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONHASHSEED \u001b[34m0\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONIOENCODING UTF-8\n",
      "\u001b[34mENV\u001b[39;49;00m PIP_DISABLE_PIP_VERSION_CHECK \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Install Hadoop\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_VERSION \u001b[34m3\u001b[39;49;00m.2.1\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_HOME /usr/hadoop-\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHADOOP_CONF_DIR\u001b[39;49;00m=\u001b[31m$HADOOP_HOME\u001b[39;49;00m/etc/hadoop\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[31m$HADOOP_HOME\u001b[39;49;00m/bin\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m3\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttp://archive.apache.org/dist/hadoop/common/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\n",
      "  | tar -x -C /usr/ \u001b[33m\\\u001b[39;49;00m\n",
      " && rm -rf \u001b[31m$HADOOP_HOME\u001b[39;49;00m/share/doc \u001b[33m\\\u001b[39;49;00m\n",
      " && chown -R root:root \u001b[31m$HADOOP_HOME\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Install Spark\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_VERSION \u001b[34m2\u001b[39;49;00m.4.6\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_PACKAGE spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m-bin-without-hadoop\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_HOME /usr/spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mSPARK_DIST_CLASSPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/etc/hadoop/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/tools/lib/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_HOME\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/bin\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m3\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://archive.apache.org/dist/spark/spark-\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_PACKAGE\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.tgz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\n",
      "  | tar x -C /usr/ \u001b[33m\\\u001b[39;49;00m\n",
      " && mv /usr/\u001b[31m$SPARK_PACKAGE\u001b[39;49;00m \u001b[31m$SPARK_HOME\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      " && chown -R root:root \u001b[31m$SPARK_HOME\u001b[39;49;00m\n",
      " \n",
      "\u001b[37m# Point Spark at proper python binary\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYSPARK_PYTHON\u001b[39;49;00m=/usr/bin/python3\n",
      "\n",
      "\u001b[37m# Setup Spark/Yarn/HDFS user as root\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m/usr/bin:/opt/program:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mPATH\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_RESOURCEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_NODEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_NAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_DATANODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_SECONDARYNAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set up bootstrapping program and Spark configuration\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m program /opt/program\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /opt/program/submit\n",
      "\u001b[34mCOPY\u001b[39;49;00m hadoop-config /opt/hadoop-config\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m jars /usr/jars\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $SPARK_HOME\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Install Transformers and TensorFlow\u001b[39;49;00m\n",
      "\u001b[37m#RUN pip3 install -q pip --upgrade\u001b[39;49;00m\n",
      "\u001b[37m#RUN pip3 install -q wrapt --upgrade --ignore-installed\u001b[39;49;00m\n",
      "\u001b[37m#RUN pip3 install -q transformers==2.8.0\u001b[39;49;00m\n",
      "\u001b[37m#RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"/opt/program/submit\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-analyzer'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      "8-jre-slim: Pulling from library/openjdk\n",
      "\n",
      "\u001b[1B52930446: Pulling fs layer \n",
      "\u001b[1B9b8e633f: Pulling fs layer \n",
      "\u001b[1B86d6fc62: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:c8740afc9ec89f879bdb3d02cb885a8c3fff565f2f850020127194765922b631\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for openjdk:8-jre-slim\n",
      " ---> d2f9f3c77c25\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Running in 6d16313e96ee\n",
      "Get:1 http://deb.debian.org/debian buster InRelease [122 kB]\n",
      "Get:2 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:4 http://deb.debian.org/debian buster/main amd64 Packages [7906 kB]\n",
      "Get:5 http://security.debian.org/debian-security buster/updates/main amd64 Packages [220 kB]\n",
      "Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]\n",
      "Fetched 8374 kB in 1s (6438 kB/s)\n",
      "Reading package lists...\n",
      "Removing intermediate container 6d16313e96ee\n",
      " ---> cf302be67c70\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Running in a4904a98b75b\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential bzip2 cpp\n",
      "  cpp-8 dbus dh-python dirmngr dpkg-dev fakeroot file g++ g++-8 gcc gcc-8\n",
      "  gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm krb5-locales libalgorithm-diff-perl\n",
      "  libalgorithm-diff-xs-perl libalgorithm-merge-perl libapparmor1 libasan5\n",
      "  libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcurl4\n",
      "  libdbus-1-3 libdpkg-perl libexpat1 libexpat1-dev libfakeroot\n",
      "  libfile-fcntllock-perl libgcc-8-dev libgdbm-compat4 libgdbm6\n",
      "  libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libgssapi-krb5-2\n",
      "  libicu63 libisl19 libitm1 libk5crypto3 libkeyutils1 libkrb5-3\n",
      "  libkrb5support0 libksba8 libldap-2.4-2 libldap-common liblocale-gettext-perl\n",
      "  liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2 libmpfr6 libmpx2\n",
      "  libnghttp2-14 libnpth0 libperl5.28 libpsl5 libpython-dev libpython-stdlib\n",
      "  libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev\n",
      "  libpython2.7-minimal libpython2.7-stdlib libpython3-dev libpython3-stdlib\n",
      "  libpython3.7 libpython3.7-dev libpython3.7-minimal libpython3.7-stdlib\n",
      "  libquadmath0 libreadline7 librtmp1 libsasl2-2 libsasl2-modules\n",
      "  libsasl2-modules-db libsqlite3-0 libssh2-1 libstdc++-8-dev libtsan0\n",
      "  libubsan1 libxml2 linux-libc-dev make manpages manpages-dev mime-support\n",
      "  netbase patch perl perl-modules-5.28 pinentry-curses publicsuffix python\n",
      "  python-minimal python-pip-whl python2 python2-dev python2-minimal python2.7\n",
      "  python2.7-dev python2.7-minimal python3-asn1crypto python3-cffi-backend\n",
      "  python3-crypto python3-cryptography python3-dbus python3-distutils\n",
      "  python3-entrypoints python3-gi python3-keyring python3-keyrings.alt\n",
      "  python3-lib2to3 python3-minimal python3-pkg-resources python3-secretstorage\n",
      "  python3-six python3-wheel python3-xdg python3.7 python3.7-dev\n",
      "  python3.7-minimal readline-common shared-mime-info xdg-user-dirs xz-utils\n",
      "Suggested packages:\n",
      "  binutils-doc bzip2-doc cpp-doc gcc-8-locales default-dbus-session-bus\n",
      "  | dbus-session-bus dbus-user-session libpam-systemd pinentry-gnome3 tor\n",
      "  debian-keyring g++-multilib g++-8-multilib gcc-8-doc libstdc++6-8-dbg\n",
      "  gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-8-multilib\n",
      "  libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan5-dbg\n",
      "  liblsan0-dbg libtsan0-dbg libubsan1-dbg libmpx2-dbg libquadmath0-dbg\n",
      "  parcimonie xloadimage scdaemon glibc-doc sensible-utils git bzr gdbm-l10n\n",
      "  krb5-doc krb5-user libsasl2-modules-gssapi-mit\n",
      "  | libsasl2-modules-gssapi-heimdal libsasl2-modules-ldap libsasl2-modules-otp\n",
      "  libsasl2-modules-sql libstdc++-8-doc make-doc man-browser ed diffutils-doc\n",
      "  perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl\n",
      "  libb-debug-perl liblocale-codes-perl pinentry-doc python-doc python-tk\n",
      "  python-psutil-doc python2-doc python2.7-doc binfmt-support python3-doc\n",
      "  python3-tk python3-venv python-crypto-doc python-cryptography-doc\n",
      "  python3-cryptography-vectors python-dbus-doc python3-dbus-dbg gnome-keyring\n",
      "  libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-secretstorage-doc\n",
      "  python-setuptools-doc python3.7-venv python3.7-doc readline-doc zip\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential bzip2 cpp\n",
      "  cpp-8 curl dbus dh-python dirmngr dpkg-dev fakeroot file g++ g++-8 gcc gcc-8\n",
      "  gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm krb5-locales libalgorithm-diff-perl\n",
      "  libalgorithm-diff-xs-perl libalgorithm-merge-perl libapparmor1 libasan5\n",
      "  libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcurl4\n",
      "  libdbus-1-3 libdpkg-perl libexpat1 libexpat1-dev libfakeroot\n",
      "  libfile-fcntllock-perl libgcc-8-dev libgdbm-compat4 libgdbm6\n",
      "  libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libgssapi-krb5-2\n",
      "  libicu63 libisl19 libitm1 libk5crypto3 libkeyutils1 libkrb5-3\n",
      "  libkrb5support0 libksba8 libldap-2.4-2 libldap-common liblocale-gettext-perl\n",
      "  liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2 libmpfr6 libmpx2\n",
      "  libnghttp2-14 libnpth0 libperl5.28 libpsl5 libpython-dev libpython-stdlib\n",
      "  libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev\n",
      "  libpython2.7-minimal libpython2.7-stdlib libpython3-dev libpython3-stdlib\n",
      "  libpython3.7 libpython3.7-dev libpython3.7-minimal libpython3.7-stdlib\n",
      "  libquadmath0 libreadline7 librtmp1 libsasl2-2 libsasl2-modules\n",
      "  libsasl2-modules-db libsqlite3-0 libssh2-1 libstdc++-8-dev libtsan0\n",
      "  libubsan1 libxml2 linux-libc-dev make manpages manpages-dev mime-support\n",
      "  netbase patch perl perl-modules-5.28 pinentry-curses publicsuffix python\n",
      "  python-dev python-minimal python-pip-whl python-psutil python2 python2-dev\n",
      "  python2-minimal python2.7 python2.7-dev python2.7-minimal python3\n",
      "  python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography\n",
      "  python3-dbus python3-dev python3-distutils python3-entrypoints python3-gi\n",
      "  python3-keyring python3-keyrings.alt python3-lib2to3 python3-minimal\n",
      "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
      "  python3-six python3-wheel python3-xdg python3.7 python3.7-dev\n",
      "  python3.7-minimal readline-common shared-mime-info unzip xdg-user-dirs\n",
      "  xz-utils\n",
      "0 upgraded, 154 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 178 MB of archives.\n",
      "After this operation, 521 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian buster/main amd64 perl-modules-5.28 all 5.28.1-6+deb10u1 [2873 kB]\n",
      "Get:2 http://deb.debian.org/debian buster/main amd64 libgdbm6 amd64 1.18.1-4 [64.7 kB]\n",
      "Get:3 http://deb.debian.org/debian buster/main amd64 libgdbm-compat4 amd64 1.18.1-4 [44.1 kB]\n",
      "Get:4 http://deb.debian.org/debian buster/main amd64 libperl5.28 amd64 5.28.1-6+deb10u1 [3894 kB]\n",
      "Get:5 http://deb.debian.org/debian buster/main amd64 perl amd64 5.28.1-6+deb10u1 [204 kB]\n",
      "Get:6 http://deb.debian.org/debian buster/main amd64 libpython2.7-minimal amd64 2.7.16-2+deb10u1 [395 kB]\n",
      "Get:7 http://deb.debian.org/debian buster/main amd64 python2.7-minimal amd64 2.7.16-2+deb10u1 [1369 kB]\n",
      "Get:8 http://deb.debian.org/debian buster/main amd64 python2-minimal amd64 2.7.16-1 [41.4 kB]\n",
      "Get:9 http://deb.debian.org/debian buster/main amd64 python-minimal amd64 2.7.16-1 [21.0 kB]\n",
      "Get:10 http://deb.debian.org/debian buster/main amd64 mime-support all 3.62 [37.2 kB]\n",
      "Get:11 http://deb.debian.org/debian buster/main amd64 libexpat1 amd64 2.2.6-2+deb10u1 [106 kB]\n",
      "Get:12 http://deb.debian.org/debian buster/main amd64 readline-common all 7.0-5 [70.6 kB]\n",
      "Get:13 http://deb.debian.org/debian buster/main amd64 libreadline7 amd64 7.0-5 [151 kB]\n",
      "Get:14 http://deb.debian.org/debian buster/main amd64 libsqlite3-0 amd64 3.27.2-3 [641 kB]\n",
      "Get:15 http://deb.debian.org/debian buster/main amd64 libpython2.7-stdlib amd64 2.7.16-2+deb10u1 [1912 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:16 http://deb.debian.org/debian buster/main amd64 python2.7 amd64 2.7.16-2+deb10u1 [305 kB]\n",
      "Get:17 http://deb.debian.org/debian buster/main amd64 libpython2-stdlib amd64 2.7.16-1 [20.8 kB]\n",
      "Get:18 http://deb.debian.org/debian buster/main amd64 libpython-stdlib amd64 2.7.16-1 [20.8 kB]\n",
      "Get:19 http://deb.debian.org/debian buster/main amd64 python2 amd64 2.7.16-1 [41.6 kB]\n",
      "Get:20 http://deb.debian.org/debian buster/main amd64 python amd64 2.7.16-1 [22.8 kB]\n",
      "Get:21 http://deb.debian.org/debian buster/main amd64 liblocale-gettext-perl amd64 1.07-3+b4 [18.9 kB]\n",
      "Get:22 http://deb.debian.org/debian buster/main amd64 libpython3.7-minimal amd64 3.7.3-2+deb10u2 [589 kB]\n",
      "Get:23 http://deb.debian.org/debian buster/main amd64 python3.7-minimal amd64 3.7.3-2+deb10u2 [1731 kB]\n",
      "Get:24 http://deb.debian.org/debian buster/main amd64 python3-minimal amd64 3.7.3-1 [36.6 kB]\n",
      "Get:25 http://deb.debian.org/debian buster/main amd64 libmpdec2 amd64 2.4.2-2 [87.2 kB]\n",
      "Get:26 http://deb.debian.org/debian buster/main amd64 libpython3.7-stdlib amd64 3.7.3-2+deb10u2 [1732 kB]\n",
      "Get:27 http://deb.debian.org/debian buster/main amd64 python3.7 amd64 3.7.3-2+deb10u2 [330 kB]\n",
      "Get:28 http://deb.debian.org/debian buster/main amd64 libpython3-stdlib amd64 3.7.3-1 [20.0 kB]\n",
      "Get:29 http://deb.debian.org/debian buster/main amd64 python3 amd64 3.7.3-1 [61.5 kB]\n",
      "Get:30 http://deb.debian.org/debian buster/main amd64 netbase all 5.6 [19.4 kB]\n",
      "Get:31 http://deb.debian.org/debian buster/main amd64 bzip2 amd64 1.0.6-9.2~deb10u1 [48.4 kB]\n",
      "Get:32 http://deb.debian.org/debian buster/main amd64 libapparmor1 amd64 2.13.2-10 [94.7 kB]\n",
      "Get:33 http://deb.debian.org/debian buster/main amd64 libdbus-1-3 amd64 1.12.20-0+deb10u1 [215 kB]\n",
      "Get:34 http://deb.debian.org/debian buster/main amd64 dbus amd64 1.12.20-0+deb10u1 [236 kB]\n",
      "Get:35 http://deb.debian.org/debian buster/main amd64 libmagic-mgc amd64 1:5.35-4+deb10u1 [242 kB]\n",
      "Get:36 http://deb.debian.org/debian buster/main amd64 libmagic1 amd64 1:5.35-4+deb10u1 [117 kB]\n",
      "Get:37 http://deb.debian.org/debian buster/main amd64 file amd64 1:5.35-4+deb10u1 [66.4 kB]\n",
      "Get:38 http://deb.debian.org/debian buster/main amd64 krb5-locales all 1.17-3 [95.4 kB]\n",
      "Get:39 http://deb.debian.org/debian buster/main amd64 manpages all 4.16-2 [1295 kB]\n",
      "Get:40 http://deb.debian.org/debian buster/main amd64 xz-utils amd64 5.2.4-1 [183 kB]\n",
      "Get:41 http://deb.debian.org/debian buster/main amd64 binutils-common amd64 2.31.1-16 [2073 kB]\n",
      "Get:42 http://deb.debian.org/debian buster/main amd64 libbinutils amd64 2.31.1-16 [478 kB]\n",
      "Get:43 http://deb.debian.org/debian buster/main amd64 binutils-x86-64-linux-gnu amd64 2.31.1-16 [1823 kB]\n",
      "Get:44 http://deb.debian.org/debian buster/main amd64 binutils amd64 2.31.1-16 [56.8 kB]\n",
      "Get:45 http://deb.debian.org/debian buster/main amd64 libc-dev-bin amd64 2.28-10 [275 kB]\n",
      "Get:46 http://deb.debian.org/debian buster/main amd64 linux-libc-dev amd64 4.19.132-1 [1376 kB]\n",
      "Get:47 http://deb.debian.org/debian buster/main amd64 libc6-dev amd64 2.28-10 [2691 kB]\n",
      "Get:48 http://deb.debian.org/debian buster/main amd64 libisl19 amd64 0.20-2 [587 kB]\n",
      "Get:49 http://deb.debian.org/debian buster/main amd64 libmpfr6 amd64 4.0.2-1 [775 kB]\n",
      "Get:50 http://deb.debian.org/debian buster/main amd64 libmpc3 amd64 1.1.0-1 [41.3 kB]\n",
      "Get:51 http://deb.debian.org/debian buster/main amd64 cpp-8 amd64 8.3.0-6 [8914 kB]\n",
      "Get:52 http://deb.debian.org/debian buster/main amd64 cpp amd64 4:8.3.0-1 [19.4 kB]\n",
      "Get:53 http://deb.debian.org/debian buster/main amd64 libcc1-0 amd64 8.3.0-6 [46.6 kB]\n",
      "Get:54 http://deb.debian.org/debian buster/main amd64 libgomp1 amd64 8.3.0-6 [75.8 kB]\n",
      "Get:55 http://deb.debian.org/debian buster/main amd64 libitm1 amd64 8.3.0-6 [27.7 kB]\n",
      "Get:56 http://deb.debian.org/debian buster/main amd64 libatomic1 amd64 8.3.0-6 [9032 B]\n",
      "Get:57 http://deb.debian.org/debian buster/main amd64 libasan5 amd64 8.3.0-6 [362 kB]\n",
      "Get:58 http://deb.debian.org/debian buster/main amd64 liblsan0 amd64 8.3.0-6 [131 kB]\n",
      "Get:59 http://deb.debian.org/debian buster/main amd64 libtsan0 amd64 8.3.0-6 [283 kB]\n",
      "Get:60 http://deb.debian.org/debian buster/main amd64 libubsan1 amd64 8.3.0-6 [120 kB]\n",
      "Get:61 http://deb.debian.org/debian buster/main amd64 libmpx2 amd64 8.3.0-6 [11.4 kB]\n",
      "Get:62 http://deb.debian.org/debian buster/main amd64 libquadmath0 amd64 8.3.0-6 [133 kB]\n",
      "Get:63 http://deb.debian.org/debian buster/main amd64 libgcc-8-dev amd64 8.3.0-6 [2298 kB]\n",
      "Get:64 http://deb.debian.org/debian buster/main amd64 gcc-8 amd64 8.3.0-6 [9452 kB]\n",
      "Get:65 http://deb.debian.org/debian buster/main amd64 gcc amd64 4:8.3.0-1 [5196 B]\n",
      "Get:66 http://deb.debian.org/debian buster/main amd64 libstdc++-8-dev amd64 8.3.0-6 [1532 kB]\n",
      "Get:67 http://deb.debian.org/debian buster/main amd64 g++-8 amd64 8.3.0-6 [9752 kB]\n",
      "Get:68 http://deb.debian.org/debian buster/main amd64 g++ amd64 4:8.3.0-1 [1644 B]\n",
      "Get:69 http://deb.debian.org/debian buster/main amd64 make amd64 4.2.1-1.2 [341 kB]\n",
      "Get:70 http://deb.debian.org/debian buster/main amd64 libdpkg-perl all 1.19.7 [1414 kB]\n",
      "Get:71 http://deb.debian.org/debian buster/main amd64 patch amd64 2.7.6-3+deb10u1 [126 kB]\n",
      "Get:72 http://deb.debian.org/debian buster/main amd64 dpkg-dev all 1.19.7 [1773 kB]\n",
      "Get:73 http://deb.debian.org/debian buster/main amd64 build-essential amd64 12.6 [7576 B]\n",
      "Get:74 http://deb.debian.org/debian buster/main amd64 libkeyutils1 amd64 1.6-6 [15.0 kB]\n",
      "Get:75 http://deb.debian.org/debian buster/main amd64 libkrb5support0 amd64 1.17-3 [65.6 kB]\n",
      "Get:76 http://deb.debian.org/debian buster/main amd64 libk5crypto3 amd64 1.17-3 [121 kB]\n",
      "Get:77 http://deb.debian.org/debian buster/main amd64 libkrb5-3 amd64 1.17-3 [370 kB]\n",
      "Get:78 http://deb.debian.org/debian buster/main amd64 libgssapi-krb5-2 amd64 1.17-3 [158 kB]\n",
      "Get:79 http://deb.debian.org/debian buster/main amd64 libsasl2-modules-db amd64 2.1.27+dfsg-1+deb10u1 [69.1 kB]\n",
      "Get:80 http://deb.debian.org/debian buster/main amd64 libsasl2-2 amd64 2.1.27+dfsg-1+deb10u1 [106 kB]\n",
      "Get:81 http://deb.debian.org/debian buster/main amd64 libldap-common all 2.4.47+dfsg-3+deb10u2 [89.7 kB]\n",
      "Get:82 http://deb.debian.org/debian buster/main amd64 libldap-2.4-2 amd64 2.4.47+dfsg-3+deb10u2 [224 kB]\n",
      "Get:83 http://deb.debian.org/debian buster/main amd64 libnghttp2-14 amd64 1.36.0-2+deb10u1 [85.0 kB]\n",
      "Get:84 http://deb.debian.org/debian buster/main amd64 libpsl5 amd64 0.20.2-2 [53.7 kB]\n",
      "Get:85 http://deb.debian.org/debian buster/main amd64 librtmp1 amd64 2.4+20151223.gitfa8646d.1-2 [60.5 kB]\n",
      "Get:86 http://deb.debian.org/debian buster/main amd64 libssh2-1 amd64 1.8.0-2.1 [140 kB]\n",
      "Get:87 http://deb.debian.org/debian buster/main amd64 libcurl4 amd64 7.64.0-4+deb10u1 [331 kB]\n",
      "Get:88 http://deb.debian.org/debian buster/main amd64 curl amd64 7.64.0-4+deb10u1 [264 kB]\n",
      "Get:89 http://deb.debian.org/debian buster/main amd64 python3-lib2to3 all 3.7.3-1 [76.7 kB]\n",
      "Get:90 http://deb.debian.org/debian buster/main amd64 python3-distutils all 3.7.3-1 [142 kB]\n",
      "Get:91 http://deb.debian.org/debian buster/main amd64 dh-python all 3.20190308 [99.3 kB]\n",
      "Get:92 http://deb.debian.org/debian buster/main amd64 libassuan0 amd64 2.5.2-1 [49.4 kB]\n",
      "Get:93 http://deb.debian.org/debian buster/main amd64 gpgconf amd64 2.2.12-1+deb10u1 [510 kB]\n",
      "Get:94 http://deb.debian.org/debian buster/main amd64 libksba8 amd64 1.3.5-2 [99.7 kB]\n",
      "Get:95 http://deb.debian.org/debian buster/main amd64 libnpth0 amd64 1.6-1 [18.4 kB]\n",
      "Get:96 http://deb.debian.org/debian buster/main amd64 dirmngr amd64 2.2.12-1+deb10u1 [712 kB]\n",
      "Get:97 http://deb.debian.org/debian buster/main amd64 libfakeroot amd64 1.23-1 [45.9 kB]\n",
      "Get:98 http://deb.debian.org/debian buster/main amd64 fakeroot amd64 1.23-1 [85.8 kB]\n",
      "Get:99 http://deb.debian.org/debian buster/main amd64 libglib2.0-0 amd64 2.58.3-2+deb10u2 [1258 kB]\n",
      "Get:100 http://deb.debian.org/debian buster/main amd64 libgirepository-1.0-1 amd64 1.58.3-2 [92.8 kB]\n",
      "Get:101 http://deb.debian.org/debian buster/main amd64 gir1.2-glib-2.0 amd64 1.58.3-2 [143 kB]\n",
      "Get:102 http://deb.debian.org/debian buster/main amd64 gnupg-l10n all 2.2.12-1+deb10u1 [1010 kB]\n",
      "Get:103 http://deb.debian.org/debian buster/main amd64 gnupg-utils amd64 2.2.12-1+deb10u1 [861 kB]\n",
      "Get:104 http://deb.debian.org/debian buster/main amd64 gpg amd64 2.2.12-1+deb10u1 [865 kB]\n",
      "Get:105 http://deb.debian.org/debian buster/main amd64 pinentry-curses amd64 1.1.0-2 [64.5 kB]\n",
      "Get:106 http://deb.debian.org/debian buster/main amd64 gpg-agent amd64 2.2.12-1+deb10u1 [617 kB]\n",
      "Get:107 http://deb.debian.org/debian buster/main amd64 gpg-wks-client amd64 2.2.12-1+deb10u1 [485 kB]\n",
      "Get:108 http://deb.debian.org/debian buster/main amd64 gpg-wks-server amd64 2.2.12-1+deb10u1 [478 kB]\n",
      "Get:109 http://deb.debian.org/debian buster/main amd64 gpgsm amd64 2.2.12-1+deb10u1 [604 kB]\n",
      "Get:110 http://deb.debian.org/debian buster/main amd64 gnupg all 2.2.12-1+deb10u1 [715 kB]\n",
      "Get:111 http://deb.debian.org/debian buster/main amd64 libalgorithm-diff-perl all 1.19.03-2 [47.9 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:112 http://deb.debian.org/debian buster/main amd64 libalgorithm-diff-xs-perl amd64 0.04-5+b1 [11.8 kB]\n",
      "Get:113 http://deb.debian.org/debian buster/main amd64 libalgorithm-merge-perl all 0.08-3 [12.7 kB]\n",
      "Get:114 http://deb.debian.org/debian buster/main amd64 libexpat1-dev amd64 2.2.6-2+deb10u1 [153 kB]\n",
      "Get:115 http://deb.debian.org/debian buster/main amd64 libfile-fcntllock-perl amd64 0.22-3+b5 [35.4 kB]\n",
      "Get:116 http://deb.debian.org/debian buster/main amd64 libglib2.0-data all 2.58.3-2+deb10u2 [1110 kB]\n",
      "Get:117 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u1 [8300 kB]\n",
      "Get:118 http://deb.debian.org/debian buster/main amd64 libpython2.7 amd64 2.7.16-2+deb10u1 [1036 kB]\n",
      "Get:119 http://deb.debian.org/debian buster/main amd64 libpython2.7-dev amd64 2.7.16-2+deb10u1 [31.6 MB]\n",
      "Get:120 http://deb.debian.org/debian buster/main amd64 libpython2-dev amd64 2.7.16-1 [20.9 kB]\n",
      "Get:121 http://deb.debian.org/debian buster/main amd64 libpython-dev amd64 2.7.16-1 [20.9 kB]\n",
      "Get:122 http://deb.debian.org/debian buster/main amd64 libpython3.7 amd64 3.7.3-2+deb10u2 [1498 kB]\n",
      "Get:123 http://deb.debian.org/debian buster/main amd64 libpython3.7-dev amd64 3.7.3-2+deb10u2 [48.4 MB]\n",
      "Get:124 http://deb.debian.org/debian buster/main amd64 libpython3-dev amd64 3.7.3-1 [20.1 kB]\n",
      "Get:125 http://deb.debian.org/debian buster/main amd64 libsasl2-modules amd64 2.1.27+dfsg-1+deb10u1 [104 kB]\n",
      "Get:126 http://deb.debian.org/debian buster/main amd64 libxml2 amd64 2.9.4+dfsg1-7+b3 [687 kB]\n",
      "Get:127 http://deb.debian.org/debian buster/main amd64 manpages-dev all 4.16-2 [2232 kB]\n",
      "Get:128 http://deb.debian.org/debian buster/main amd64 publicsuffix all 20190415.1030-1 [116 kB]\n",
      "Get:129 http://deb.debian.org/debian buster/main amd64 python2.7-dev amd64 2.7.16-2+deb10u1 [294 kB]\n",
      "Get:130 http://deb.debian.org/debian buster/main amd64 python2-dev amd64 2.7.16-1 [1212 B]\n",
      "Get:131 http://deb.debian.org/debian buster/main amd64 python-dev amd64 2.7.16-1 [1192 B]\n",
      "Get:132 http://deb.debian.org/debian buster/main amd64 python-pip-whl all 18.1-5 [1591 kB]\n",
      "Get:133 http://deb.debian.org/debian buster/main amd64 python-psutil amd64 5.5.1-1 [166 kB]\n",
      "Get:134 http://deb.debian.org/debian buster/main amd64 python3-asn1crypto all 0.24.0-1 [78.2 kB]\n",
      "Get:135 http://deb.debian.org/debian buster/main amd64 python3-cffi-backend amd64 1.12.2-1 [79.7 kB]\n",
      "Get:136 http://deb.debian.org/debian buster/main amd64 python3-crypto amd64 2.6.1-9+b1 [263 kB]\n",
      "Get:137 http://deb.debian.org/debian buster/main amd64 python3-six all 1.12.0-1 [15.7 kB]\n",
      "Get:138 http://deb.debian.org/debian buster/main amd64 python3-cryptography amd64 2.6.1-3+deb10u2 [219 kB]\n",
      "Get:139 http://deb.debian.org/debian buster/main amd64 python3-dbus amd64 1.2.8-3 [103 kB]\n",
      "Get:140 http://deb.debian.org/debian buster/main amd64 python3.7-dev amd64 3.7.3-2+deb10u2 [510 kB]\n",
      "Get:141 http://deb.debian.org/debian buster/main amd64 python3-dev amd64 3.7.3-1 [1264 B]\n",
      "Get:142 http://deb.debian.org/debian buster/main amd64 python3-entrypoints all 0.3-1 [5508 B]\n",
      "Get:143 http://deb.debian.org/debian buster/main amd64 python3-gi amd64 3.30.4-1 [180 kB]\n",
      "Get:144 http://deb.debian.org/debian buster/main amd64 python3-secretstorage all 2.3.1-2 [14.2 kB]\n",
      "Get:145 http://deb.debian.org/debian buster/main amd64 python3-keyring all 17.1.1-1 [43.1 kB]\n",
      "Get:146 http://deb.debian.org/debian buster/main amd64 python3-keyrings.alt all 3.1.1-1 [18.2 kB]\n",
      "Get:147 http://deb.debian.org/debian buster/main amd64 python3-pip all 18.1-5 [171 kB]\n",
      "Get:148 http://deb.debian.org/debian buster/main amd64 python3-pkg-resources all 40.8.0-1 [153 kB]\n",
      "Get:149 http://deb.debian.org/debian buster/main amd64 python3-setuptools all 40.8.0-1 [306 kB]\n",
      "Get:150 http://deb.debian.org/debian buster/main amd64 python3-wheel all 0.32.3-2 [19.4 kB]\n",
      "Get:151 http://deb.debian.org/debian buster/main amd64 python3-xdg all 0.25-5 [35.9 kB]\n",
      "Get:152 http://deb.debian.org/debian buster/main amd64 shared-mime-info amd64 1.10-1 [766 kB]\n",
      "Get:153 http://deb.debian.org/debian buster/main amd64 unzip amd64 6.0-23+deb10u1 [172 kB]\n",
      "Get:154 http://deb.debian.org/debian buster/main amd64 xdg-user-dirs amd64 0.17-2 [53.8 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 178 MB in 3s (60.4 MB/s)\n",
      "Selecting previously unselected package perl-modules-5.28.\n",
      "(Reading database ... 6889 files and directories currently installed.)\n",
      "Preparing to unpack .../00-perl-modules-5.28_5.28.1-6+deb10u1_all.deb ...\n",
      "Unpacking perl-modules-5.28 (5.28.1-6+deb10u1) ...\n",
      "Selecting previously unselected package libgdbm6:amd64.\n",
      "Preparing to unpack .../01-libgdbm6_1.18.1-4_amd64.deb ...\n",
      "Unpacking libgdbm6:amd64 (1.18.1-4) ...\n",
      "Selecting previously unselected package libgdbm-compat4:amd64.\n",
      "Preparing to unpack .../02-libgdbm-compat4_1.18.1-4_amd64.deb ...\n",
      "Unpacking libgdbm-compat4:amd64 (1.18.1-4) ...\n",
      "Selecting previously unselected package libperl5.28:amd64.\n",
      "Preparing to unpack .../03-libperl5.28_5.28.1-6+deb10u1_amd64.deb ...\n",
      "Unpacking libperl5.28:amd64 (5.28.1-6+deb10u1) ...\n",
      "Selecting previously unselected package perl.\n",
      "Preparing to unpack .../04-perl_5.28.1-6+deb10u1_amd64.deb ...\n",
      "Unpacking perl (5.28.1-6+deb10u1) ...\n",
      "Selecting previously unselected package libpython2.7-minimal:amd64.\n",
      "Preparing to unpack .../05-libpython2.7-minimal_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-minimal:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2.7-minimal.\n",
      "Preparing to unpack .../06-python2.7-minimal_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7-minimal (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2-minimal.\n",
      "Preparing to unpack .../07-python2-minimal_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python-minimal.\n",
      "Preparing to unpack .../08-python-minimal_2.7.16-1_amd64.deb ...\n",
      "Unpacking python-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package mime-support.\n",
      "Preparing to unpack .../09-mime-support_3.62_all.deb ...\n",
      "Unpacking mime-support (3.62) ...\n",
      "Selecting previously unselected package libexpat1:amd64.\n",
      "Preparing to unpack .../10-libexpat1_2.2.6-2+deb10u1_amd64.deb ...\n",
      "Unpacking libexpat1:amd64 (2.2.6-2+deb10u1) ...\n",
      "Selecting previously unselected package readline-common.\n",
      "Preparing to unpack .../11-readline-common_7.0-5_all.deb ...\n",
      "Unpacking readline-common (7.0-5) ...\n",
      "Selecting previously unselected package libreadline7:amd64.\n",
      "Preparing to unpack .../12-libreadline7_7.0-5_amd64.deb ...\n",
      "Unpacking libreadline7:amd64 (7.0-5) ...\n",
      "Selecting previously unselected package libsqlite3-0:amd64.\n",
      "Preparing to unpack .../13-libsqlite3-0_3.27.2-3_amd64.deb ...\n",
      "Unpacking libsqlite3-0:amd64 (3.27.2-3) ...\n",
      "Selecting previously unselected package libpython2.7-stdlib:amd64.\n",
      "Preparing to unpack .../14-libpython2.7-stdlib_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-stdlib:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2.7.\n",
      "Preparing to unpack .../15-python2.7_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython2-stdlib:amd64.\n",
      "Preparing to unpack .../16-libpython2-stdlib_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython2-stdlib:amd64 (2.7.16-1) ...\n",
      "Selecting previously unselected package libpython-stdlib:amd64.\n",
      "Preparing to unpack .../17-libpython-stdlib_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up libpython2.7-minimal:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up python2.7-minimal (2.7.16-2+deb10u1) ...\n",
      "Linking and byte-compiling packages for runtime python2.7...\n",
      "Setting up python2-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python2.\n",
      "(Reading database ... 9661 files and directories currently installed.)\n",
      "Preparing to unpack .../python2_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2 (2.7.16-1) ...\n",
      "Setting up python-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python.\n",
      "(Reading database ... 9694 files and directories currently installed.)\n",
      "Preparing to unpack .../python_2.7.16-1_amd64.deb ...\n",
      "Unpacking python (2.7.16-1) ...\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-3+b4_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-3+b4) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
      "Preparing to unpack .../libpython3.7-minimal_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libpython3.7-minimal:amd64 (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package python3.7-minimal.\n",
      "Preparing to unpack .../python3.7-minimal_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking python3.7-minimal (3.7.3-2+deb10u2) ...\n",
      "Setting up libpython3.7-minimal:amd64 (3.7.3-2+deb10u2) ...\n",
      "Setting up libexpat1:amd64 (2.2.6-2+deb10u1) ...\n",
      "Setting up python3.7-minimal (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package python3-minimal.\n",
      "(Reading database ... 9963 files and directories currently installed.)\n",
      "Preparing to unpack .../python3-minimal_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3-minimal (3.7.3-1) ...\n",
      "Selecting previously unselected package libmpdec2:amd64.\n",
      "Preparing to unpack .../libmpdec2_2.4.2-2_amd64.deb ...\n",
      "Unpacking libmpdec2:amd64 (2.4.2-2) ...\n",
      "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3.7-stdlib_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libpython3.7-stdlib:amd64 (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package python3.7.\n",
      "Preparing to unpack .../python3.7_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking python3.7 (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libpython3-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3-stdlib_3.7.3-1_amd64.deb ...\n",
      "Unpacking libpython3-stdlib:amd64 (3.7.3-1) ...\n",
      "Setting up python3-minimal (3.7.3-1) ...\n",
      "Selecting previously unselected package python3.\n",
      "(Reading database ... 10375 files and directories currently installed.)\n",
      "Preparing to unpack .../000-python3_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3 (3.7.3-1) ...\n",
      "Selecting previously unselected package netbase.\n",
      "Preparing to unpack .../001-netbase_5.6_all.deb ...\n",
      "Unpacking netbase (5.6) ...\n",
      "Selecting previously unselected package bzip2.\n",
      "Preparing to unpack .../002-bzip2_1.0.6-9.2~deb10u1_amd64.deb ...\n",
      "Unpacking bzip2 (1.0.6-9.2~deb10u1) ...\n",
      "Selecting previously unselected package libapparmor1:amd64.\n",
      "Preparing to unpack .../003-libapparmor1_2.13.2-10_amd64.deb ...\n",
      "Unpacking libapparmor1:amd64 (2.13.2-10) ...\n",
      "Selecting previously unselected package libdbus-1-3:amd64.\n",
      "Preparing to unpack .../004-libdbus-1-3_1.12.20-0+deb10u1_amd64.deb ...\n",
      "Unpacking libdbus-1-3:amd64 (1.12.20-0+deb10u1) ...\n",
      "Selecting previously unselected package dbus.\n",
      "Preparing to unpack .../005-dbus_1.12.20-0+deb10u1_amd64.deb ...\n",
      "Unpacking dbus (1.12.20-0+deb10u1) ...\n",
      "Selecting previously unselected package libmagic-mgc.\n",
      "Preparing to unpack .../006-libmagic-mgc_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking libmagic-mgc (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package libmagic1:amd64.\n",
      "Preparing to unpack .../007-libmagic1_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking libmagic1:amd64 (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package file.\n",
      "Preparing to unpack .../008-file_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking file (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package krb5-locales.\n",
      "Preparing to unpack .../009-krb5-locales_1.17-3_all.deb ...\n",
      "Unpacking krb5-locales (1.17-3) ...\n",
      "Selecting previously unselected package manpages.\n",
      "Preparing to unpack .../010-manpages_4.16-2_all.deb ...\n",
      "Unpacking manpages (4.16-2) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "Preparing to unpack .../011-xz-utils_5.2.4-1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.4-1) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../012-binutils-common_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.31.1-16) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../013-libbinutils_2.31.1-16_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.31.1-16) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../014-binutils-x86-64-linux-gnu_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.31.1-16) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../015-binutils_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils (2.31.1-16) ...\n",
      "Selecting previously unselected package libc-dev-bin.\n",
      "Preparing to unpack .../016-libc-dev-bin_2.28-10_amd64.deb ...\n",
      "Unpacking libc-dev-bin (2.28-10) ...\n",
      "Selecting previously unselected package linux-libc-dev:amd64.\n",
      "Preparing to unpack .../017-linux-libc-dev_4.19.132-1_amd64.deb ...\n",
      "Unpacking linux-libc-dev:amd64 (4.19.132-1) ...\n",
      "Selecting previously unselected package libc6-dev:amd64.\n",
      "Preparing to unpack .../018-libc6-dev_2.28-10_amd64.deb ...\n",
      "Unpacking libc6-dev:amd64 (2.28-10) ...\n",
      "Selecting previously unselected package libisl19:amd64.\n",
      "Preparing to unpack .../019-libisl19_0.20-2_amd64.deb ...\n",
      "Unpacking libisl19:amd64 (0.20-2) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../020-libmpfr6_4.0.2-1_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.0.2-1) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../021-libmpc3_1.1.0-1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.1.0-1) ...\n",
      "Selecting previously unselected package cpp-8.\n",
      "Preparing to unpack .../022-cpp-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking cpp-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../023-cpp_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking cpp (4:8.3.0-1) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../024-libcc1-0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libgomp1:amd64.\n",
      "Preparing to unpack .../025-libgomp1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libgomp1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../026-libitm1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../027-libatomic1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libasan5:amd64.\n",
      "Preparing to unpack .../028-libasan5_8.3.0-6_amd64.deb ...\n",
      "Unpacking libasan5:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../029-liblsan0_8.3.0-6_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../030-libtsan0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libubsan1:amd64.\n",
      "Preparing to unpack .../031-libubsan1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libubsan1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libmpx2:amd64.\n",
      "Preparing to unpack .../032-libmpx2_8.3.0-6_amd64.deb ...\n",
      "Unpacking libmpx2:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../033-libquadmath0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libgcc-8-dev:amd64.\n",
      "Preparing to unpack .../034-libgcc-8-dev_8.3.0-6_amd64.deb ...\n",
      "Unpacking libgcc-8-dev:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package gcc-8.\n",
      "Preparing to unpack .../035-gcc-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking gcc-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../036-gcc_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking gcc (4:8.3.0-1) ...\n",
      "Selecting previously unselected package libstdc++-8-dev:amd64.\n",
      "Preparing to unpack .../037-libstdc++-8-dev_8.3.0-6_amd64.deb ...\n",
      "Unpacking libstdc++-8-dev:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package g++-8.\n",
      "Preparing to unpack .../038-g++-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking g++-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../039-g++_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking g++ (4:8.3.0-1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../040-make_4.2.1-1.2_amd64.deb ...\n",
      "Unpacking make (4.2.1-1.2) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../041-libdpkg-perl_1.19.7_all.deb ...\n",
      "Unpacking libdpkg-perl (1.19.7) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../042-patch_2.7.6-3+deb10u1_amd64.deb ...\n",
      "Unpacking patch (2.7.6-3+deb10u1) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../043-dpkg-dev_1.19.7_all.deb ...\n",
      "Unpacking dpkg-dev (1.19.7) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../044-build-essential_12.6_amd64.deb ...\n",
      "Unpacking build-essential (12.6) ...\n",
      "Selecting previously unselected package libkeyutils1:amd64.\n",
      "Preparing to unpack .../045-libkeyutils1_1.6-6_amd64.deb ...\n",
      "Unpacking libkeyutils1:amd64 (1.6-6) ...\n",
      "Selecting previously unselected package libkrb5support0:amd64.\n",
      "Preparing to unpack .../046-libkrb5support0_1.17-3_amd64.deb ...\n",
      "Unpacking libkrb5support0:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libk5crypto3:amd64.\n",
      "Preparing to unpack .../047-libk5crypto3_1.17-3_amd64.deb ...\n",
      "Unpacking libk5crypto3:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libkrb5-3:amd64.\n",
      "Preparing to unpack .../048-libkrb5-3_1.17-3_amd64.deb ...\n",
      "Unpacking libkrb5-3:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libgssapi-krb5-2:amd64.\n",
      "Preparing to unpack .../049-libgssapi-krb5-2_1.17-3_amd64.deb ...\n",
      "Unpacking libgssapi-krb5-2:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libsasl2-modules-db:amd64.\n",
      "Preparing to unpack .../050-libsasl2-modules-db_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-modules-db:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libsasl2-2:amd64.\n",
      "Preparing to unpack .../051-libsasl2-2_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-2:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libldap-common.\n",
      "Preparing to unpack .../052-libldap-common_2.4.47+dfsg-3+deb10u2_all.deb ...\n",
      "Unpacking libldap-common (2.4.47+dfsg-3+deb10u2) ...\n",
      "Selecting previously unselected package libldap-2.4-2:amd64.\n",
      "Preparing to unpack .../053-libldap-2.4-2_2.4.47+dfsg-3+deb10u2_amd64.deb ...\n",
      "Unpacking libldap-2.4-2:amd64 (2.4.47+dfsg-3+deb10u2) ...\n",
      "Selecting previously unselected package libnghttp2-14:amd64.\n",
      "Preparing to unpack .../054-libnghttp2-14_1.36.0-2+deb10u1_amd64.deb ...\n",
      "Unpacking libnghttp2-14:amd64 (1.36.0-2+deb10u1) ...\n",
      "Selecting previously unselected package libpsl5:amd64.\n",
      "Preparing to unpack .../055-libpsl5_0.20.2-2_amd64.deb ...\n",
      "Unpacking libpsl5:amd64 (0.20.2-2) ...\n",
      "Selecting previously unselected package librtmp1:amd64.\n",
      "Preparing to unpack .../056-librtmp1_2.4+20151223.gitfa8646d.1-2_amd64.deb ...\n",
      "Unpacking librtmp1:amd64 (2.4+20151223.gitfa8646d.1-2) ...\n",
      "Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../057-libssh2-1_1.8.0-2.1_amd64.deb ...\n",
      "Unpacking libssh2-1:amd64 (1.8.0-2.1) ...\n",
      "Selecting previously unselected package libcurl4:amd64.\n",
      "Preparing to unpack .../058-libcurl4_7.64.0-4+deb10u1_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.64.0-4+deb10u1) ...\n",
      "Selecting previously unselected package curl.\n",
      "Preparing to unpack .../059-curl_7.64.0-4+deb10u1_amd64.deb ...\n",
      "Unpacking curl (7.64.0-4+deb10u1) ...\n",
      "Selecting previously unselected package python3-lib2to3.\n",
      "Preparing to unpack .../060-python3-lib2to3_3.7.3-1_all.deb ...\n",
      "Unpacking python3-lib2to3 (3.7.3-1) ...\n",
      "Selecting previously unselected package python3-distutils.\n",
      "Preparing to unpack .../061-python3-distutils_3.7.3-1_all.deb ...\n",
      "Unpacking python3-distutils (3.7.3-1) ...\n",
      "Selecting previously unselected package dh-python.\n",
      "Preparing to unpack .../062-dh-python_3.20190308_all.deb ...\n",
      "Unpacking dh-python (3.20190308) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../063-libassuan0_2.5.2-1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.2-1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../064-gpgconf_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../065-libksba8_1.3.5-2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.3.5-2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../066-libnpth0_1.6-1_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-1) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../067-dirmngr_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../068-libfakeroot_1.23-1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.23-1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../069-fakeroot_1.23-1_amd64.deb ...\n",
      "Unpacking fakeroot (1.23-1) ...\n",
      "Selecting previously unselected package libglib2.0-0:amd64.\n",
      "Preparing to unpack .../070-libglib2.0-0_2.58.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libglib2.0-0:amd64 (2.58.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libgirepository-1.0-1:amd64.\n",
      "Preparing to unpack .../071-libgirepository-1.0-1_1.58.3-2_amd64.deb ...\n",
      "Unpacking libgirepository-1.0-1:amd64 (1.58.3-2) ...\n",
      "Selecting previously unselected package gir1.2-glib-2.0:amd64.\n",
      "Preparing to unpack .../072-gir1.2-glib-2.0_1.58.3-2_amd64.deb ...\n",
      "Unpacking gir1.2-glib-2.0:amd64 (1.58.3-2) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../073-gnupg-l10n_2.2.12-1+deb10u1_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../074-gnupg-utils_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../075-gpg_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../076-pinentry-curses_1.1.0-2_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-2) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../077-gpg-agent_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../078-gpg-wks-client_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../079-gpg-wks-server_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../080-gpgsm_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../081-gnupg_2.2.12-1+deb10u1_all.deb ...\n",
      "Unpacking gnupg (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../082-libalgorithm-diff-perl_1.19.03-2_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../083-libalgorithm-diff-xs-perl_0.04-5+b1_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-5+b1) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../084-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../085-libexpat1-dev_2.2.6-2+deb10u1_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.6-2+deb10u1) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../086-libfile-fcntllock-perl_0.22-3+b5_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3+b5) ...\n",
      "Selecting previously unselected package libglib2.0-data.\n",
      "Preparing to unpack .../087-libglib2.0-data_2.58.3-2+deb10u2_all.deb ...\n",
      "Unpacking libglib2.0-data (2.58.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libicu63:amd64.\n",
      "Preparing to unpack .../088-libicu63_63.1-6+deb10u1_amd64.deb ...\n",
      "Unpacking libicu63:amd64 (63.1-6+deb10u1) ...\n",
      "Selecting previously unselected package libpython2.7:amd64.\n",
      "Preparing to unpack .../089-libpython2.7_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython2.7-dev:amd64.\n",
      "Preparing to unpack .../090-libpython2.7-dev_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-dev:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython2-dev:amd64.\n",
      "Preparing to unpack .../091-libpython2-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython2-dev:amd64 (2.7.16-1) ...\n",
      "Selecting previously unselected package libpython-dev:amd64.\n",
      "Preparing to unpack .../092-libpython-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython-dev:amd64 (2.7.16-1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libpython3.7:amd64.\n",
      "Preparing to unpack .../093-libpython3.7_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libpython3.7:amd64 (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libpython3.7-dev:amd64.\n",
      "Preparing to unpack .../094-libpython3.7-dev_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libpython3.7-dev:amd64 (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../095-libpython3-dev_3.7.3-1_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.7.3-1) ...\n",
      "Selecting previously unselected package libsasl2-modules:amd64.\n",
      "Preparing to unpack .../096-libsasl2-modules_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-modules:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libxml2:amd64.\n",
      "Preparing to unpack .../097-libxml2_2.9.4+dfsg1-7+b3_amd64.deb ...\n",
      "Unpacking libxml2:amd64 (2.9.4+dfsg1-7+b3) ...\n",
      "Selecting previously unselected package manpages-dev.\n",
      "Preparing to unpack .../098-manpages-dev_4.16-2_all.deb ...\n",
      "Unpacking manpages-dev (4.16-2) ...\n",
      "Selecting previously unselected package publicsuffix.\n",
      "Preparing to unpack .../099-publicsuffix_20190415.1030-1_all.deb ...\n",
      "Unpacking publicsuffix (20190415.1030-1) ...\n",
      "Selecting previously unselected package python2.7-dev.\n",
      "Preparing to unpack .../100-python2.7-dev_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7-dev (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2-dev.\n",
      "Preparing to unpack .../101-python2-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2-dev (2.7.16-1) ...\n",
      "Selecting previously unselected package python-dev.\n",
      "Preparing to unpack .../102-python-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking python-dev (2.7.16-1) ...\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "Preparing to unpack .../103-python-pip-whl_18.1-5_all.deb ...\n",
      "Unpacking python-pip-whl (18.1-5) ...\n",
      "Selecting previously unselected package python-psutil.\n",
      "Preparing to unpack .../104-python-psutil_5.5.1-1_amd64.deb ...\n",
      "Unpacking python-psutil (5.5.1-1) ...\n",
      "Selecting previously unselected package python3-asn1crypto.\n",
      "Preparing to unpack .../105-python3-asn1crypto_0.24.0-1_all.deb ...\n",
      "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
      "Selecting previously unselected package python3-cffi-backend.\n",
      "Preparing to unpack .../106-python3-cffi-backend_1.12.2-1_amd64.deb ...\n",
      "Unpacking python3-cffi-backend (1.12.2-1) ...\n",
      "Selecting previously unselected package python3-crypto.\n",
      "Preparing to unpack .../107-python3-crypto_2.6.1-9+b1_amd64.deb ...\n",
      "Unpacking python3-crypto (2.6.1-9+b1) ...\n",
      "Selecting previously unselected package python3-six.\n",
      "Preparing to unpack .../108-python3-six_1.12.0-1_all.deb ...\n",
      "Unpacking python3-six (1.12.0-1) ...\n",
      "Selecting previously unselected package python3-cryptography.\n",
      "Preparing to unpack .../109-python3-cryptography_2.6.1-3+deb10u2_amd64.deb ...\n",
      "Unpacking python3-cryptography (2.6.1-3+deb10u2) ...\n",
      "Selecting previously unselected package python3-dbus.\n",
      "Preparing to unpack .../110-python3-dbus_1.2.8-3_amd64.deb ...\n",
      "Unpacking python3-dbus (1.2.8-3) ...\n",
      "Selecting previously unselected package python3.7-dev.\n",
      "Preparing to unpack .../111-python3.7-dev_3.7.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking python3.7-dev (3.7.3-2+deb10u2) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../112-python3-dev_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3-dev (3.7.3-1) ...\n",
      "Selecting previously unselected package python3-entrypoints.\n",
      "Preparing to unpack .../113-python3-entrypoints_0.3-1_all.deb ...\n",
      "Unpacking python3-entrypoints (0.3-1) ...\n",
      "Selecting previously unselected package python3-gi.\n",
      "Preparing to unpack .../114-python3-gi_3.30.4-1_amd64.deb ...\n",
      "Unpacking python3-gi (3.30.4-1) ...\n",
      "Selecting previously unselected package python3-secretstorage.\n",
      "Preparing to unpack .../115-python3-secretstorage_2.3.1-2_all.deb ...\n",
      "Unpacking python3-secretstorage (2.3.1-2) ...\n",
      "Selecting previously unselected package python3-keyring.\n",
      "Preparing to unpack .../116-python3-keyring_17.1.1-1_all.deb ...\n",
      "Unpacking python3-keyring (17.1.1-1) ...\n",
      "Selecting previously unselected package python3-keyrings.alt.\n",
      "Preparing to unpack .../117-python3-keyrings.alt_3.1.1-1_all.deb ...\n",
      "Unpacking python3-keyrings.alt (3.1.1-1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../118-python3-pip_18.1-5_all.deb ...\n",
      "Unpacking python3-pip (18.1-5) ...\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "Preparing to unpack .../119-python3-pkg-resources_40.8.0-1_all.deb ...\n",
      "Unpacking python3-pkg-resources (40.8.0-1) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../120-python3-setuptools_40.8.0-1_all.deb ...\n",
      "Unpacking python3-setuptools (40.8.0-1) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../121-python3-wheel_0.32.3-2_all.deb ...\n",
      "Unpacking python3-wheel (0.32.3-2) ...\n",
      "Selecting previously unselected package python3-xdg.\n",
      "Preparing to unpack .../122-python3-xdg_0.25-5_all.deb ...\n",
      "Unpacking python3-xdg (0.25-5) ...\n",
      "Selecting previously unselected package shared-mime-info.\n",
      "Preparing to unpack .../123-shared-mime-info_1.10-1_amd64.deb ...\n",
      "Unpacking shared-mime-info (1.10-1) ...\n",
      "Selecting previously unselected package unzip.\n",
      "Preparing to unpack .../124-unzip_6.0-23+deb10u1_amd64.deb ...\n",
      "Unpacking unzip (6.0-23+deb10u1) ...\n",
      "Selecting previously unselected package xdg-user-dirs.\n",
      "Preparing to unpack .../125-xdg-user-dirs_0.17-2_amd64.deb ...\n",
      "Unpacking xdg-user-dirs (0.17-2) ...\n",
      "Setting up perl-modules-5.28 (5.28.1-6+deb10u1) ...\n",
      "Setting up libksba8:amd64 (1.3.5-2) ...\n",
      "Setting up libkeyutils1:amd64 (1.6-6) ...\n",
      "Setting up libapparmor1:amd64 (2.13.2-10) ...\n",
      "Setting up libpsl5:amd64 (0.20.2-2) ...\n",
      "Setting up mime-support (3.62) ...\n",
      "Setting up xdg-user-dirs (0.17-2) ...\n",
      "Setting up libmagic-mgc (1:5.35-4+deb10u1) ...\n",
      "Setting up libglib2.0-0:amd64 (2.58.3-2+deb10u2) ...\n",
      "No schema files found: doing nothing.\n",
      "Setting up manpages (4.16-2) ...\n",
      "Setting up unzip (6.0-23+deb10u1) ...\n",
      "Setting up libsqlite3-0:amd64 (3.27.2-3) ...\n",
      "Setting up libsasl2-modules:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up binutils-common:amd64 (2.31.1-16) ...\n",
      "Setting up libnghttp2-14:amd64 (1.36.0-2+deb10u1) ...\n",
      "Setting up libmagic1:amd64 (1:5.35-4+deb10u1) ...\n",
      "Setting up linux-libc-dev:amd64 (4.19.132-1) ...\n",
      "Setting up libnpth0:amd64 (1.6-1) ...\n",
      "Setting up krb5-locales (1.17-3) ...\n",
      "Setting up file (1:5.35-4+deb10u1) ...\n",
      "Setting up libassuan0:amd64 (2.5.2-1) ...\n",
      "Setting up libgomp1:amd64 (8.3.0-6) ...\n",
      "Setting up bzip2 (1.0.6-9.2~deb10u1) ...\n",
      "Setting up libldap-common (2.4.47+dfsg-3+deb10u2) ...\n",
      "Setting up libicu63:amd64 (63.1-6+deb10u1) ...\n",
      "Setting up libfakeroot:amd64 (1.23-1) ...\n",
      "Setting up libkrb5support0:amd64 (1.17-3) ...\n",
      "Setting up libsasl2-modules-db:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up fakeroot (1.23-1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up libasan5:amd64 (8.3.0-6) ...\n",
      "Setting up libglib2.0-data (2.58.3-2+deb10u2) ...\n",
      "Setting up make (4.2.1-1.2) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libmpfr6:amd64 (4.0.2-1) ...\n",
      "Setting up gnupg-l10n (2.2.12-1+deb10u1) ...\n",
      "Setting up librtmp1:amd64 (2.4+20151223.gitfa8646d.1-2) ...\n",
      "Setting up libdbus-1-3:amd64 (1.12.20-0+deb10u1) ...\n",
      "Setting up dbus (1.12.20-0+deb10u1) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up xz-utils (5.2.4-1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up libquadmath0:amd64 (8.3.0-6) ...\n",
      "Setting up libmpc3:amd64 (1.1.0-1) ...\n",
      "Setting up libatomic1:amd64 (8.3.0-6) ...\n",
      "Setting up patch (2.7.6-3+deb10u1) ...\n",
      "Setting up libk5crypto3:amd64 (1.17-3) ...\n",
      "Setting up libsasl2-2:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up libmpx2:amd64 (8.3.0-6) ...\n",
      "Setting up libubsan1:amd64 (8.3.0-6) ...\n",
      "Setting up libisl19:amd64 (0.20-2) ...\n",
      "Setting up libgirepository-1.0-1:amd64 (1.58.3-2) ...\n",
      "Setting up libssh2-1:amd64 (1.8.0-2.1) ...\n",
      "Setting up netbase (5.6) ...\n",
      "Setting up python-pip-whl (18.1-5) ...\n",
      "Setting up libkrb5-3:amd64 (1.17-3) ...\n",
      "Setting up libmpdec2:amd64 (2.4.2-2) ...\n",
      "Setting up libbinutils:amd64 (2.31.1-16) ...\n",
      "Setting up cpp-8 (8.3.0-6) ...\n",
      "Setting up libc-dev-bin (2.28-10) ...\n",
      "Setting up readline-common (7.0-5) ...\n",
      "Setting up publicsuffix (20190415.1030-1) ...\n",
      "Setting up libxml2:amd64 (2.9.4+dfsg1-7+b3) ...\n",
      "Setting up libcc1-0:amd64 (8.3.0-6) ...\n",
      "Setting up liblocale-gettext-perl (1.07-3+b4) ...\n",
      "Setting up liblsan0:amd64 (8.3.0-6) ...\n",
      "Setting up libitm1:amd64 (8.3.0-6) ...\n",
      "Setting up libreadline7:amd64 (7.0-5) ...\n",
      "Setting up libgdbm6:amd64 (1.18.1-4) ...\n",
      "Setting up gnupg-utils (2.2.12-1+deb10u1) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.31.1-16) ...\n",
      "Setting up libtsan0:amd64 (8.3.0-6) ...\n",
      "Setting up pinentry-curses (1.1.0-2) ...\n",
      "Setting up manpages-dev (4.16-2) ...\n",
      "Setting up libpython3.7-stdlib:amd64 (3.7.3-2+deb10u2) ...\n",
      "Setting up libpython3.7:amd64 (3.7.3-2+deb10u2) ...\n",
      "Setting up libldap-2.4-2:amd64 (2.4.47+dfsg-3+deb10u2) ...\n",
      "Setting up binutils (2.31.1-16) ...\n",
      "Setting up libpython2.7-stdlib:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up shared-mime-info (1.10-1) ...\n",
      "Setting up libgssapi-krb5-2:amd64 (1.17-3) ...\n",
      "Setting up libgdbm-compat4:amd64 (1.18.1-4) ...\n",
      "Setting up gir1.2-glib-2.0:amd64 (1.58.3-2) ...\n",
      "Setting up libgcc-8-dev:amd64 (8.3.0-6) ...\n",
      "Setting up libperl5.28:amd64 (5.28.1-6+deb10u1) ...\n",
      "Setting up cpp (4:8.3.0-1) ...\n",
      "Setting up gpgconf (2.2.12-1+deb10u1) ...\n",
      "Setting up libcurl4:amd64 (7.64.0-4+deb10u1) ...\n",
      "Setting up libc6-dev:amd64 (2.28-10) ...\n",
      "Setting up curl (7.64.0-4+deb10u1) ...\n",
      "Setting up gpg (2.2.12-1+deb10u1) ...\n",
      "Setting up libpython3-stdlib:amd64 (3.7.3-1) ...\n",
      "Setting up libstdc++-8-dev:amd64 (8.3.0-6) ...\n",
      "Setting up python3.7 (3.7.3-2+deb10u2) ...\n",
      "Setting up libpython2.7:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up gcc-8 (8.3.0-6) ...\n",
      "Setting up gpg-agent (2.2.12-1+deb10u1) ...\n",
      "Setting up python2.7 (2.7.16-2+deb10u1) ...\n",
      "Setting up libpython2-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up gpgsm (2.2.12-1+deb10u1) ...\n",
      "Setting up python3 (3.7.3-1) ...\n",
      "running python rtupdate hooks for python3.7...\n",
      "running python post-rtupdate hooks for python3.7...\n",
      "Setting up python3-xdg (0.25-5) ...\n",
      "Setting up python3-wheel (0.32.3-2) ...\n",
      "Setting up python2 (2.7.16-1) ...\n",
      "Setting up gcc (4:8.3.0-1) ...\n",
      "Setting up python3-six (1.12.0-1) ...\n",
      "Setting up dirmngr (2.2.12-1+deb10u1) ...\n",
      "Setting up libpython-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up perl (5.28.1-6+deb10u1) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.6-2+deb10u1) ...\n",
      "Setting up python3-gi (3.30.4-1) ...\n",
      "Setting up libdpkg-perl (1.19.7) ...\n",
      "Setting up gpg-wks-server (2.2.12-1+deb10u1) ...\n",
      "Setting up g++-8 (8.3.0-6) ...\n",
      "Setting up python3-crypto (2.6.1-9+b1) ...\n",
      "Setting up python3-lib2to3 (3.7.3-1) ...\n",
      "Setting up python (2.7.16-1) ...\n",
      "Setting up python3-asn1crypto (0.24.0-1) ...\n",
      "Setting up python3-cffi-backend (1.12.2-1) ...\n",
      "Setting up python3-pkg-resources (40.8.0-1) ...\n",
      "Setting up python3-entrypoints (0.3-1) ...\n",
      "Setting up python3-distutils (3.7.3-1) ...\n",
      "Setting up dh-python (3.20190308) ...\n",
      "Setting up python3-dbus (1.2.8-3) ...\n",
      "Setting up libpython2.7-dev:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up python3-setuptools (40.8.0-1) ...\n",
      "Setting up gpg-wks-client (2.2.12-1+deb10u1) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3+b5) ...\n",
      "Setting up libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Setting up libpython3.7-dev:amd64 (3.7.3-2+deb10u2) ...\n",
      "Setting up python3.7-dev (3.7.3-2+deb10u2) ...\n",
      "Setting up dpkg-dev (1.19.7) ...\n",
      "Setting up python3-cryptography (2.6.1-3+deb10u2) ...\n",
      "Setting up python3-pip (18.1-5) ...\n",
      "Setting up python-psutil (5.5.1-1) ...\n",
      "Setting up g++ (4:8.3.0-1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "Setting up python3-keyrings.alt (3.1.1-1) ...\n",
      "Setting up gnupg (2.2.12-1+deb10u1) ...\n",
      "Setting up build-essential (12.6) ...\n",
      "Setting up libpython2-dev:amd64 (2.7.16-1) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-5+b1) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up python2.7-dev (2.7.16-2+deb10u1) ...\n",
      "Setting up libpython3-dev:amd64 (3.7.3-1) ...\n",
      "Setting up python2-dev (2.7.16-1) ...\n",
      "Setting up libpython-dev:amd64 (2.7.16-1) ...\n",
      "Setting up python3-secretstorage (2.3.1-2) ...\n",
      "Setting up python3-dev (3.7.3-1) ...\n",
      "Setting up python3-keyring (17.1.1-1) ...\n",
      "Setting up python-dev (2.7.16-1) ...\n",
      "Processing triggers for libc-bin (2.28-10) ...\n",
      "Removing intermediate container a4904a98b75b\n",
      " ---> 62d56031da94\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Running in 9a43d26fddf7\n",
      "Collecting py4j\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "Collecting psutil==5.6.5\n",
      "  Downloading https://files.pythonhosted.org/packages/03/9a/95c4b3d0424426e5fd94b5302ff74cea44d5d4f53466e1228ac8e73e14b4/psutil-5.6.5.tar.gz (447kB)\n",
      "Collecting numpy==1.17.4\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/af/4fc72f9d38e43b092e91e5b8cb9956d25b2e3ff8c75aed95df5569e4734e/numpy-1.17.4-cp37-cp37m-manylinux1_x86_64.whl (20.0MB)\n",
      "Building wheels for collected packages: psutil\n",
      "  Running setup.py bdist_wheel for psutil: started\n",
      "  Running setup.py bdist_wheel for psutil: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/48/b6/72b7243c5caf65b7d5b460e9fad82b1256992284e870b7db59\n",
      "Successfully built psutil\n",
      "Installing collected packages: py4j, psutil, numpy\n",
      "Successfully installed numpy-1.17.4 psutil-5.6.5 py4j-0.10.9\n",
      "Removing intermediate container 9a43d26fddf7\n",
      " ---> 381148b87d0e\n",
      "Step 5/33 : RUN apt-get clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> Running in b32d5ee1f464\n",
      "Removing intermediate container b32d5ee1f464\n",
      " ---> f0ec1306ff8a\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 32a2d682f085\n",
      "Removing intermediate container 32a2d682f085\n",
      " ---> dba6330dec66\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Running in 68e547beb104\n",
      "Removing intermediate container 68e547beb104\n",
      " ---> 4a8cdcf14e77\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Running in c34d65e9fb57\n",
      "Removing intermediate container c34d65e9fb57\n",
      " ---> 911008bea772\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Running in dc0b5a3f8147\n",
      "Removing intermediate container dc0b5a3f8147\n",
      " ---> ede4fd31963b\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Running in b140bbd3c76c\n",
      "Removing intermediate container b140bbd3c76c\n",
      " ---> f844be4ebfd9\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Running in 33da60a8577b\n",
      "Removing intermediate container 33da60a8577b\n",
      " ---> d36f503d5444\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Running in fdabf2db5679\n",
      "Removing intermediate container fdabf2db5679\n",
      " ---> 2529e926dddc\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Running in d7c956125cb6\n",
      "Removing intermediate container d7c956125cb6\n",
      " ---> 6b52f8636bb3\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Running in ca43c7379113\n",
      "Removing intermediate container ca43c7379113\n",
      " ---> 591e32f55b86\n",
      "Step 15/33 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Running in db987c711baa\n",
      "Removing intermediate container db987c711baa\n",
      " ---> de157b4978b4\n",
      "Step 16/33 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Running in 57b5ba0a3b98\n",
      "Removing intermediate container 57b5ba0a3b98\n",
      " ---> 262e7cf11102\n",
      "Step 17/33 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Running in 24c3a7c4c83f\n",
      "Removing intermediate container 24c3a7c4c83f\n",
      " ---> b679ab216ccb\n",
      "Step 18/33 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Running in a7a8115afd21\n",
      "Removing intermediate container a7a8115afd21\n",
      " ---> b4246d49021f\n",
      "Step 19/33 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Running in 28b860fc125e\n",
      "Removing intermediate container 28b860fc125e\n",
      " ---> 34ff43c460f2\n",
      "Step 20/33 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Running in 78f93782b317\n",
      "Removing intermediate container 78f93782b317\n",
      " ---> f293c5a9eab0\n",
      "Step 21/33 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Running in 2060f9d6707a\n",
      "Removing intermediate container 2060f9d6707a\n",
      " ---> f11370c07428\n",
      "Step 22/33 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Running in 41b57b00b0cc\n",
      "Removing intermediate container 41b57b00b0cc\n",
      " ---> b0d9910839b1\n",
      "Step 23/33 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Running in 7f8ceb2e85a7\n",
      "Removing intermediate container 7f8ceb2e85a7\n",
      " ---> bd05c3e1da04\n",
      "Step 24/33 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Running in 3129a353d796\n",
      "Removing intermediate container 3129a353d796\n",
      " ---> cb11934f88f3\n",
      "Step 25/33 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Running in d9b58125d5ce\n",
      "Removing intermediate container d9b58125d5ce\n",
      " ---> c51491ba9a5f\n",
      "Step 26/33 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Running in 286736e49cc8\n",
      "Removing intermediate container 286736e49cc8\n",
      " ---> 47649c226465\n",
      "Step 27/33 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Running in 68ca2e4e59b1\n",
      "Removing intermediate container 68ca2e4e59b1\n",
      " ---> d10a56eb1748\n",
      "Step 28/33 : COPY program /opt/program\n",
      " ---> 8f9e9f07b7cf\n",
      "Step 29/33 : RUN chmod +x /opt/program/submit\n",
      " ---> Running in 797956c8c49b\n",
      "Removing intermediate container 797956c8c49b\n",
      " ---> 5f3fc3af9dcb\n",
      "Step 30/33 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> c8c48609b7f7\n",
      "Step 31/33 : COPY jars /usr/jars\n",
      " ---> 9c7e0205980a\n",
      "Step 32/33 : WORKDIR $SPARK_HOME\n",
      " ---> Running in 8cb60e167e80\n",
      "Removing intermediate container 8cb60e167e80\n",
      " ---> c5b3aac0b80d\n",
      "Step 33/33 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in 19cc91d8ed37\n",
      "Removing intermediate container 19cc91d8ed37\n",
      " ---> b252e044dc40\n",
      "Successfully built b252e044dc40\n",
      "Successfully tagged amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Docker Image\n",
    "If the image did not build properly, re-run the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"Id\": \"sha256:b252e044dc4018ea0a357b8098dccc58a711fccb56a320c6418b91212010405b\",\r\n",
      "        \"RepoTags\": [\r\n",
      "            \"amazon-reviews-spark-analyzer:latest\"\r\n",
      "        ],\r\n",
      "        \"RepoDigests\": [],\r\n",
      "        \"Parent\": \"sha256:c5b3aac0b80d85df605e494480ffae11d5005c6a247313877eeb837672aeb8c4\",\r\n",
      "        \"Comment\": \"\",\r\n",
      "        \"Created\": \"2020-08-22T18:18:14.295376748Z\",\r\n",
      "        \"Container\": \"19cc91d8ed37486511052e62d864b7cc0a7ee6c4a0bf6a7bba32dfb63c19aa12\",\r\n",
      "        \"ContainerConfig\": {\r\n",
      "            \"Hostname\": \"19cc91d8ed37\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": [\r\n",
      "                \"PATH=/usr/bin:/opt/program:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/hadoop-3.2.1/bin:/usr/spark-2.4.6/bin\",\r\n",
      "                \"LANG=C.UTF-8\",\r\n",
      "                \"JAVA_HOME=/usr/local/openjdk-8\",\r\n",
      "                \"JAVA_VERSION=8u265\",\r\n",
      "                \"PYTHONHASHSEED=0\",\r\n",
      "                \"PYTHONIOENCODING=UTF-8\",\r\n",
      "                \"PIP_DISABLE_PIP_VERSION_CHECK=1\",\r\n",
      "                \"HADOOP_VERSION=3.2.1\",\r\n",
      "                \"HADOOP_HOME=/usr/hadoop-3.2.1\",\r\n",
      "                \"HADOOP_CONF_DIR=/usr/hadoop-3.2.1/etc/hadoop\",\r\n",
      "                \"SPARK_VERSION=2.4.6\",\r\n",
      "                \"SPARK_PACKAGE=spark-2.4.6-bin-without-hadoop\",\r\n",
      "                \"SPARK_HOME=/usr/spark-2.4.6\",\r\n",
      "                \"SPARK_DIST_CLASSPATH=/usr/hadoop-3.2.1/etc/hadoop/*:/usr/hadoop-3.2.1/share/hadoop/common/lib/*:/usr/hadoop-3.2.1/share/hadoop/common/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/*:/usr/hadoop-3.2.1/share/hadoop/yarn/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/*:/usr/hadoop-3.2.1/share/hadoop/tools/lib/*\",\r\n",
      "                \"PYSPARK_PYTHON=/usr/bin/python3\",\r\n",
      "                \"YARN_RESOURCEMANAGER_USER=root\",\r\n",
      "                \"YARN_NODEMANAGER_USER=root\",\r\n",
      "                \"HDFS_NAMENODE_USER=root\",\r\n",
      "                \"HDFS_DATANODE_USER=root\",\r\n",
      "                \"HDFS_SECONDARYNAMENODE_USER=root\"\r\n",
      "            ],\r\n",
      "            \"Cmd\": [\r\n",
      "                \"/bin/sh\",\r\n",
      "                \"-c\",\r\n",
      "                \"#(nop) \",\r\n",
      "                \"ENTRYPOINT [\\\"/opt/program/submit\\\"]\"\r\n",
      "            ],\r\n",
      "            \"Image\": \"sha256:c5b3aac0b80d85df605e494480ffae11d5005c6a247313877eeb837672aeb8c4\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"/usr/spark-2.4.6\",\r\n",
      "            \"Entrypoint\": [\r\n",
      "                \"/opt/program/submit\"\r\n",
      "            ],\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": {}\r\n",
      "        },\r\n",
      "        \"DockerVersion\": \"19.03.6-ce\",\r\n",
      "        \"Author\": \"\",\r\n",
      "        \"Config\": {\r\n",
      "            \"Hostname\": \"\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": [\r\n",
      "                \"PATH=/usr/bin:/opt/program:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/hadoop-3.2.1/bin:/usr/spark-2.4.6/bin\",\r\n",
      "                \"LANG=C.UTF-8\",\r\n",
      "                \"JAVA_HOME=/usr/local/openjdk-8\",\r\n",
      "                \"JAVA_VERSION=8u265\",\r\n",
      "                \"PYTHONHASHSEED=0\",\r\n",
      "                \"PYTHONIOENCODING=UTF-8\",\r\n",
      "                \"PIP_DISABLE_PIP_VERSION_CHECK=1\",\r\n",
      "                \"HADOOP_VERSION=3.2.1\",\r\n",
      "                \"HADOOP_HOME=/usr/hadoop-3.2.1\",\r\n",
      "                \"HADOOP_CONF_DIR=/usr/hadoop-3.2.1/etc/hadoop\",\r\n",
      "                \"SPARK_VERSION=2.4.6\",\r\n",
      "                \"SPARK_PACKAGE=spark-2.4.6-bin-without-hadoop\",\r\n",
      "                \"SPARK_HOME=/usr/spark-2.4.6\",\r\n",
      "                \"SPARK_DIST_CLASSPATH=/usr/hadoop-3.2.1/etc/hadoop/*:/usr/hadoop-3.2.1/share/hadoop/common/lib/*:/usr/hadoop-3.2.1/share/hadoop/common/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/usr/hadoop-3.2.1/share/hadoop/hdfs/*:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/*:/usr/hadoop-3.2.1/share/hadoop/yarn/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/usr/hadoop-3.2.1/share/hadoop/mapreduce/*:/usr/hadoop-3.2.1/share/hadoop/tools/lib/*\",\r\n",
      "                \"PYSPARK_PYTHON=/usr/bin/python3\",\r\n",
      "                \"YARN_RESOURCEMANAGER_USER=root\",\r\n",
      "                \"YARN_NODEMANAGER_USER=root\",\r\n",
      "                \"HDFS_NAMENODE_USER=root\",\r\n",
      "                \"HDFS_DATANODE_USER=root\",\r\n",
      "                \"HDFS_SECONDARYNAMENODE_USER=root\"\r\n",
      "            ],\r\n",
      "            \"Cmd\": null,\r\n",
      "            \"Image\": \"sha256:c5b3aac0b80d85df605e494480ffae11d5005c6a247313877eeb837672aeb8c4\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"/usr/spark-2.4.6\",\r\n",
      "            \"Entrypoint\": [\r\n",
      "                \"/opt/program/submit\"\r\n",
      "            ],\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": null\r\n",
      "        },\r\n",
      "        \"Architecture\": \"amd64\",\r\n",
      "        \"Os\": \"linux\",\r\n",
      "        \"Size\": 1399236388,\r\n",
      "        \"VirtualSize\": 1399236388,\r\n",
      "        \"GraphDriver\": {\r\n",
      "            \"Data\": {\r\n",
      "                \"LowerDir\": \"/var/lib/docker/overlay2/8de4aa6979503cc0b37ae7b7a79d14421313697e20187aa2c2d57821deb6b7f4/diff:/var/lib/docker/overlay2/6ae74cacf3a3dcf89480f7d71fbca454dd1aa85e94dc1650c944b55c2238f197/diff:/var/lib/docker/overlay2/07f7c660d9633a27db9442b8f5e51066d894e270557bc68bb32b8555b3b14ff8/diff:/var/lib/docker/overlay2/9373f89a85145a39d2e2894063d335184d12b53c46a19bef28afbe08c0718070/diff:/var/lib/docker/overlay2/39c933662d4f221626813741fc7f2628268caf5815d20455c541530bd4dbad93/diff:/var/lib/docker/overlay2/4892563a50cc2aa953c3c1336c0fc982ea8560be156780e107abd5f2beac2721/diff:/var/lib/docker/overlay2/daa131b055a81993cf5e32c0035dfa888bba2aefefcd3ae43f8484ea4e910f97/diff:/var/lib/docker/overlay2/d076aa8722dcfbd9ae09882991e14f2e7d2a4f5b251fb361dced3e854d95b39b/diff:/var/lib/docker/overlay2/e06b8eef919046502cc04e92385571e318fb8f1ef7feff06dc37ff9f011a6609/diff:/var/lib/docker/overlay2/9347119e169aef2c66d941d98b92f5bdaba84e2eaedcd20b32d56bfd23492ea8/diff:/var/lib/docker/overlay2/d0c30e4ab86c579b34c97d6464073714d16358473f891a03cc59a24b3e8e65c1/diff:/var/lib/docker/overlay2/9022dbfadf9d14f36c40ad196fe464584f738b8279875a1fca7623f23a9efb0e/diff:/var/lib/docker/overlay2/9a050f6f812c069a040a7144c13a7f1c7e902231110dc94887887b3f6970063e/diff:/var/lib/docker/overlay2/4c685b6d2b4e7b610179d8f234a8d5472dfbb732a7057d02a9320ec75cb3e640/diff\",\r\n",
      "                \"MergedDir\": \"/var/lib/docker/overlay2/a4879836f43d955770097aed878c59deb7b029421eb239df816545eaaa1e2d6b/merged\",\r\n",
      "                \"UpperDir\": \"/var/lib/docker/overlay2/a4879836f43d955770097aed878c59deb7b029421eb239df816545eaaa1e2d6b/diff\",\r\n",
      "                \"WorkDir\": \"/var/lib/docker/overlay2/a4879836f43d955770097aed878c59deb7b029421eb239df816545eaaa1e2d6b/work\"\r\n",
      "            },\r\n",
      "            \"Name\": \"overlay2\"\r\n",
      "        },\r\n",
      "        \"RootFS\": {\r\n",
      "            \"Type\": \"layers\",\r\n",
      "            \"Layers\": [\r\n",
      "                \"sha256:d0f104dc0a1f9c744b65b23b3fd4d4d3236b4656e67f776fe13f8ad8423b955c\",\r\n",
      "                \"sha256:95c20fa5728d237b86ad3a7f2997e3ad71ec472165297872c119108de68e7114\",\r\n",
      "                \"sha256:ed7152ed4cbd0de85948780e8b33ffe66728f56b864bb8db4b520089b48c4883\",\r\n",
      "                \"sha256:43ee8102614ddbd62242c06c4a2f7dc50fd81a9dbd72daf75467c65264d95d4f\",\r\n",
      "                \"sha256:3ccbffc3afeac065230de307a96b5af1b39b12e3005f66eaa7cc5da42a32fad7\",\r\n",
      "                \"sha256:aaae35ba2eea36f7980d7274c2f93188f658dea064ce085a4d60a8f1984e7582\",\r\n",
      "                \"sha256:597b99d2e6753650c11b5ab26d936219c5d6ac75a9139adef136eb49b536f058\",\r\n",
      "                \"sha256:16e96348f10328085fe9b6746f83ccb19ab27256b46392c1ff328f4418eb03ad\",\r\n",
      "                \"sha256:6a07aa6518ef987ff72e2a0d36cab867ae653dc9f2c958b984d62e0f5c8e83ed\",\r\n",
      "                \"sha256:58fe703633f44f724790b6e8b3fca611788045bf36a8d2c4fb198b0d7b8beacf\",\r\n",
      "                \"sha256:1f208b738420557716af2bbdd523be8898c89a5aab6acf4abaf9dc8327528f6e\",\r\n",
      "                \"sha256:364e16b2b6aecbd1aedde2c16a16f1265973c2610da161d6cd35e8e85dfdcf93\",\r\n",
      "                \"sha256:dcade877c17555aa64e1af491bf79bf906853fa82380d2234a34a04bb3586e11\",\r\n",
      "                \"sha256:4adf92807213542cd57c07d5d94542e9224ef59e04eb63429fad67a85ce55476\",\r\n",
      "                \"sha256:78c40503c2cf3355d04830b70f43ac78f50f208bd8682b0b5db0bb64f984bb22\"\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"Metadata\": {\r\n",
      "            \"LastTagTime\": \"2020-08-22T18:18:14.329552552Z\"\r\n",
      "        }\r\n",
      "    }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!docker inspect $docker_repo:$docker_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Image to a Private Docker Repo (Amazon ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-analyzer' does not exist in the registry with id '250107111215'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:250107111215:repository/amazon-reviews-spark-analyzer\",\n",
      "        \"registryId\": \"250107111215\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-analyzer\",\n",
      "        \"repositoryUri\": \"250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer\",\n",
      "        \"createdAt\": 1598120300.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        },\n",
      "        \"encryptionConfiguration\": {\n",
      "            \"encryptionType\": \"AES256\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer]\n",
      "\n",
      "\u001b[1B0503c2cf: Preparing \n",
      "\u001b[1B92807213: Preparing \n",
      "\u001b[1Be877c175: Preparing \n",
      "\u001b[1B16b2b6ae: Preparing \n",
      "\u001b[1B8b738420: Preparing \n",
      "\u001b[1B703633f4: Preparing \n",
      "\u001b[1Baa6518ef: Preparing \n",
      "\u001b[1B6348f103: Preparing \n",
      "\u001b[1B99d2e675: Preparing \n",
      "\u001b[1B35ba2eea: Preparing \n",
      "\u001b[1Bffc3afea: Preparing \n",
      "\u001b[1B8102614d: Preparing \n",
      "\u001b[1B52ed4cbd: Preparing \n",
      "\u001b[1B0fa5728d: Preparing \n",
      "\u001b[6B35ba2eea: Pushed   490.3MB/481.8MB\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2Klatest: digest: sha256:406c53e939c21a0a14234bb7260b09025ddf5b528f79841883ceec1a711b5f5d size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Analysis Job using a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "    \r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "    s3_output_analyze_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_analyze_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon_Reviews_Spark_Analyzer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Invoke Main from preprocess-deequ.jar\u001b[39;49;00m\r\n",
      "    \u001b[36mgetattr\u001b[39;49;00m(spark._jvm.SparkAmazonReviewsAnalyzer, \u001b[33m\"\u001b[39;49;00m\u001b[33mrun\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.\u001b[39;49;00m{\u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m, \u001b[04m\u001b[32mAnalyzerContext\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.\u001b[39;49;00m{\u001b[04m\u001b[32mCompliance\u001b[39;49;00m, \u001b[04m\u001b[32mCorrelation\u001b[39;49;00m, \u001b[04m\u001b[32mSize\u001b[39;49;00m, \u001b[04m\u001b[32mCompleteness\u001b[39;49;00m, \u001b[04m\u001b[32mMean\u001b[39;49;00m, \u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.\u001b[39;49;00m{\u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m, \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.VerificationResult.checkResultsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.checks.\u001b[39;49;00m{\u001b[04m\u001b[32mCheck\u001b[39;49;00m, \u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.suggestions.\u001b[39;49;00m{\u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m, \u001b[04m\u001b[32mRules\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SaveMode\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.types.\u001b[39;49;00m{\u001b[04m\u001b[32mStructType\u001b[39;49;00m, \u001b[04m\u001b[32mStructField\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mobject\u001b[39;49;00m \u001b[04m\u001b[32mSparkAmazonReviewsAnalyzer\u001b[39;49;00m {\r\n",
      "  \u001b[34mdef\u001b[39;49;00m run(s3InputData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m, s3OutputAnalyzeData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m)\u001b[34m:\u001b[39;49;00m \u001b[36mUnit\u001b[39;49;00m = {\r\n",
      "\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_input_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3InputData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_output_analyze_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m spark \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m\r\n",
      "      .builder\r\n",
      "      .appName(\u001b[33m\"SparkAmazonReviewsAnalyzer\"\u001b[39;49;00m)\r\n",
      "      .getOrCreate()\r\n",
      "    \r\n",
      "    \u001b[34mval\u001b[39;49;00m schema \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mStructType\u001b[39;49;00m(\u001b[04m\u001b[32mArray\u001b[39;49;00m(\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"customer_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_parent\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_title\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_category\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"helpful_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"vine\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"verified_purchase\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_headline\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_body\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_date\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "    ))\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m dataset \u001b[34m=\u001b[39;49;00m spark.read.option(\u001b[33m\"sep\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"quote\"\u001b[39;49;00m, \u001b[33m\"\"\u001b[39;49;00m)\r\n",
      "                            .schema(schema)\r\n",
      "                            .csv(s3InputData)\r\n",
      "\r\n",
      "    \u001b[37m// define analyzers that compute metrics\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m analysisResult\u001b[34m:\u001b[39;49;00m \u001b[36mAnalyzerContext\u001b[39;49;00m = { \u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mSize\u001b[39;49;00m())\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompleteness\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mMean\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompliance\u001b[39;49;00m(\u001b[33m\"top star_rating\"\u001b[39;49;00m, \u001b[33m\"star_rating >= 4.0\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"helpful_votes\"\u001b[39;49;00m))\r\n",
      "          \u001b[37m// compute metrics\u001b[39;49;00m\r\n",
      "          .run()\r\n",
      "        }\r\n",
      "\r\n",
      "    \u001b[37m// retrieve successfully computed metrics as a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m metrics \u001b[34m=\u001b[39;49;00m successMetricsAsDataFrame(spark, analysisResult)\r\n",
      "    metrics.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    metrics\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)      \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/dataset-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m// define data quality checks,\u001b[39;49;00m\r\n",
      "    \u001b[37m// compute metrics \u001b[39;49;00m\r\n",
      "    \u001b[37m// verify check conditions\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationResult\u001b[34m:\u001b[39;49;00m \u001b[36mVerificationResult\u001b[39;49;00m = { \u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m()\r\n",
      "          \u001b[37m// data to run the verification on\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addCheck(\r\n",
      "            \u001b[04m\u001b[32mCheck\u001b[39;49;00m(\u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m.\u001b[04m\u001b[32mError\u001b[39;49;00m, \u001b[33m\"Review Check\"\u001b[39;49;00m) \r\n",
      "              .hasSize(\u001b[34m_\u001b[39;49;00m >= \u001b[34m200000\u001b[39;49;00m) \u001b[37m// at least 200.000 rows\u001b[39;49;00m\r\n",
      "              .hasMin(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m1.0\u001b[39;49;00m) \u001b[37m// min is 1.0\u001b[39;49;00m\r\n",
      "              .hasMax(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m5.0\u001b[39;49;00m) \u001b[37m// max is 5.0\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isUnique(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should not contain duplicates\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"marketplace\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isContainedIn(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mArray\u001b[39;49;00m(\u001b[33m\"US\"\u001b[39;49;00m, \u001b[33m\"UK\"\u001b[39;49;00m, \u001b[33m\"DE\"\u001b[39;49;00m, \u001b[33m\"JP\"\u001b[39;49;00m, \u001b[33m\"FR\"\u001b[39;49;00m)) \r\n",
      "              )\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[37m// convert check results to a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m resultsDataFrame \u001b[34m=\u001b[39;49;00m checkResultsAsDataFrame(spark, verificationResult)\r\n",
      "    resultsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    resultsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m// generate the success metrics as a dataframe\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationSuccessMetricsDataFrame \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m\r\n",
      "      .successMetricsAsDataFrame(spark, verificationResult)\r\n",
      "\r\n",
      "    verificationSuccessMetricsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    verificationSuccessMetricsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "\r\n",
      "    \u001b[37m// We ask deequ to compute constraint suggestions for us on the data\u001b[39;49;00m\r\n",
      "    \u001b[37m// using a default set of rules for constraint suggestion\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsResult \u001b[34m=\u001b[39;49;00m { \u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m()\r\n",
      "          .onData(dataset)\r\n",
      "          .addConstraintRules(\u001b[04m\u001b[32mRules\u001b[39;49;00m.\u001b[04m\u001b[32mDEFAULT\u001b[39;49;00m)\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mspark.implicits._\u001b[39;49;00m \u001b[37m// for toDS method below\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m// We can now investigate the constraints that Deequ suggested. \u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsDataFrame \u001b[34m=\u001b[39;49;00m suggestionsResult.constraintSuggestions.flatMap { \r\n",
      "          \u001b[34mcase\u001b[39;49;00m (column, suggestions) \u001b[34m=>\u001b[39;49;00m \r\n",
      "            suggestions.map { constraint \u001b[34m=>\u001b[39;49;00m\r\n",
      "              (column, constraint.description, constraint.codeForConstraint)\r\n",
      "            } \r\n",
      "    }.toSeq.toDS()\r\n",
      "      \r\n",
      "    suggestionsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    suggestionsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)      \r\n",
      "      .write      \r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)  \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize deequ/preprocess-deequ.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.2xlarge',\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 17:41:31   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-08-22 17:41:44   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-08-22-18-18-50\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'null-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349/output/null-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              # See https://github.com/aws/sagemaker-python-sdk/issues/1341 \n",
    "              #   for why we need to specify a null-output\n",
    "              outputs=[\n",
    "                  ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                   output_name='null-output',\n",
    "                                   source='/opt/ml/processing/output')\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'null-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-250107111215/spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349/output/null-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '250107111215.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-deequ.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output']}, 'Environment': {'main_class': 'Main', 'mode': 'jar'}, 'RoleArn': 'arn:aws:iam::250107111215:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:250107111215:processing-job/spark-amazon-reviews-analyzer-2020-08-22-18-18-50-349', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 22, 18, 18, 50, 746000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 22, 18, 18, 50, 746000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'f6dc951f-2537-489b-aa2e-dd41f38427c1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f6dc951f-2537-489b-aa2e-dd41f38427c1', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1626', 'date': 'Sat, 22 Aug 2020 18:18:50 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:15,914 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.153.140\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.\u001b[0m\n",
      "\u001b[34m2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:15,922 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:15,999 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-69bdfa4e-bfb9-4d5e-a8bd-6e0805a20e7f\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,322 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,335 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,336 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,337 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,341 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,341 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,341 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,341 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,383 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,393 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,393 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,397 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,400 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Aug 22 18:22:16\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,401 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,401 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,402 INFO util.GSet: 2.0% max memory 13.7 GB = 280.5 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,402 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,476 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,476 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,482 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,482 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,482 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,482 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,483 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,506 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,506 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,506 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,506 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,517 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,517 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,518 INFO util.GSet: 1.0% max memory 13.7 GB = 140.2 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,518 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,746 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,746 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,746 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,746 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,751 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,752 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,756 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,756 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,756 INFO util.GSet: 0.25% max memory 13.7 GB = 35.1 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,756 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,766 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,766 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,768 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,768 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,768 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,768 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,789 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1261050227-10.0.153.140-1598120536783\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,801 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,825 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,919 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,930 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,934 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:16,934 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.153.140\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-153-140.us-west-2.compute.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-153-140.us-west-2.compute.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,162 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,733 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,751 INFO spark.SparkContext: Submitted application: Amazon_Reviews_Spark_Analyzer\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,796 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,796 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,796 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,796 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:28,796 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,010 INFO util.Utils: Successfully started service 'sparkDriver' on port 45493.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,031 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,045 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,048 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,048 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,055 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-74971084-989a-409b-90f7-e3cadbe455d1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,068 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,108 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,179 INFO util.log: Logging initialized @1925ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,236 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,248 INFO server.Server: Started @1994ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,263 INFO server.AbstractConnector: Started ServerConnector@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,263 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,284 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5194e618{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,285 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9aaf373{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,285 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48e9e006{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f0ce844{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18e2d273{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,290 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49f9c315{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,290 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15155060{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,292 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64ba74c6{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,292 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b13ae61{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,293 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28e100f9{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,293 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2caf354c{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,294 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b84685b{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5da2adec{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d648869{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2834d7dc{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10caa431{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,297 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@584fa609{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,298 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1314aa9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,298 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f35f764{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,299 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a9b219f{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,307 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fc1a92b{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,308 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1506ea8d{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,309 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ed22c4c{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,310 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c608af4{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,310 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e0a828f{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,312 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.153.140:4040\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:29,908 INFO client.RMProxy: Connecting to ResourceManager at /10.0.153.140:8032\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,179 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,242 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,243 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,258 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63625 MB per container)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,258 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,259 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,261 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,267 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:30,311 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:31,144 INFO yarn.Client: Uploading resource file:/tmp/spark-76abf22b-e765-48b0-8715-9afff5d2f356/__spark_libs__8398626494484295929.zip -> hdfs://10.0.153.140/user/root/.sparkStaging/application_1598120546867_0001/__spark_libs__8398626494484295929.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:31,497 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,029 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,214 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.153.140/user/root/.sparkStaging/application_1598120546867_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,224 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,242 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.153.140/user/root/.sparkStaging/application_1598120546867_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,250 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,394 INFO yarn.Client: Uploading resource file:/tmp/spark-76abf22b-e765-48b0-8715-9afff5d2f356/__spark_conf__758890187204482443.zip -> hdfs://10.0.153.140/user/root/.sparkStaging/application_1598120546867_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,403 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,437 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,437 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,437 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,437 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:32,437 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:33,197 INFO yarn.Client: Submitting application application_1598120546867_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:33,381 INFO impl.YarnClientImpl: Submitted application application_1598120546867_0001\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:33,383 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1598120546867_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:34,389 INFO yarn.Client: Application report for application_1598120546867_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:34,391 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sat Aug 22 18:22:34 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1598120553287\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1598120546867_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:35,394 INFO yarn.Client: Application report for application_1598120546867_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:36,396 INFO yarn.Client: Application report for application_1598120546867_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:37,399 INFO yarn.Client: Application report for application_1598120546867_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,402 INFO yarn.Client: Application report for application_1598120546867_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,402 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.152.82\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1598120553287\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1598120546867_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,403 INFO cluster.YarnClientSchedulerBackend: Application application_1598120546867_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,408 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41813.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,409 INFO netty.NettyBlockTransferService: Server created on 10.0.153.140:41813\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,410 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,423 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1598120546867_0001), /proxy/application_1598120546867_0001\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,426 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.153.140, 41813, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,429 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.153.140:41813 with 366.3 MB RAM, BlockManagerId(driver, 10.0.153.140, 41813, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,430 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.153.140, 41813, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,431 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.153.140, 41813, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,557 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,561 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:38,566 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41935c0e{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:22:41,394 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.152.82:49926) with ID 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:41,471 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:39307 with 24.1 GB RAM, BlockManagerId(1, algo-2, 39307, None)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,399 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,590 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,591 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,596 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,597 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39865ef7{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,597 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,598 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62a003c3{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,598 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,599 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e9e8725{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,599 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,599 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5385de72{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,600 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,600 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94b98cf{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,912 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3_input_data: s3a://sagemaker-us-west-2-250107111215/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3_output_analyze_data: s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output\u001b[0m\n",
      "\u001b[34m2020-08-22 18:22:59,925 WARN sql.SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:00,051 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:00,097 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:00,097 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:01,310 INFO datasources.InMemoryFileIndex: It took 101 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,279 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,281 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,283 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string, star_rating: int, helpful_votes: int, total_votes: int ... 2 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,291 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,343 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,383 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,666 INFO codegen.CodeGenerator: Code generated in 192.538481 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,818 INFO codegen.CodeGenerator: Code generated in 9.086705 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,856 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 400.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,896 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,897 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,899 INFO spark.SparkContext: Created broadcast 0 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:02,922 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,133 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,151 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at AnalysisRunner.scala:303) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,154 INFO scheduler.DAGScheduler: Got job 0 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,154 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,155 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,157 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,162 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,184 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,187 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.6 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,187 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.153.140:41813 (size: 13.6 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,188 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,198 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,198 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,355 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:03,519 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:39307 (size: 13.6 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:04,121 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,182 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3826 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,491 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4276 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,492 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,493 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at AnalysisRunner.scala:303) finished in 4.315 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,494 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,494 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,495 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,495 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,498 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,506 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 36.9 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,508 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,509 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.153.140:41813 (size: 15.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,509 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,510 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,510 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,515 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,546 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:39307 (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,574 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,767 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 254 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,767 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,768 INFO scheduler.DAGScheduler: ResultStage 1 (collect at AnalysisRunner.scala:303) finished in 0.264 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,772 INFO scheduler.DAGScheduler: Job 0 finished: collect at AnalysisRunner.scala:303, took 4.639241 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:07,958 INFO codegen.CodeGenerator: Code generated in 31.150375 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,016 INFO codegen.CodeGenerator: Code generated in 8.390207 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,027 INFO codegen.CodeGenerator: Code generated in 7.15648 ms\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|entity     |instance                 |name               |value              |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |Completeness       |1.0                |\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |ApproxCountDistinct|238027.0           |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,star_rating  |Correlation        |-0.0808806564857777|\u001b[0m\n",
      "\u001b[34m|Dataset    |*                        |Size               |247515.0           |\u001b[0m\n",
      "\u001b[34m|Column     |star_rating              |Mean               |3.7237056340019796 |\u001b[0m\n",
      "\u001b[34m|Column     |top star_rating          |Compliance         |0.6633375755004747 |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9805294402834748 |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,191 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,191 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,192 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,192 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200822182308_0000}; taskId=attempt_20200822182308_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e00c7ce}; outputPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics, workPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics/_temporary/0/_temporary/attempt_20200822182308_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,192 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,571 INFO codegen.CodeGenerator: Code generated in 8.168232 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,607 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:77\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,609 INFO scheduler.DAGScheduler: Registering RDD 9 (csv at preprocess-deequ.scala:77) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,609 INFO scheduler.DAGScheduler: Got job 1 (csv at preprocess-deequ.scala:77) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,609 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (csv at preprocess-deequ.scala:77)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,609 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,609 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,610 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,614 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.1 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,615 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,616 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.153.140:41813 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,616 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,617 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,617 INFO cluster.YarnScheduler: Adding task set 2.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,620 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,621 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8116 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,621 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8205 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,621 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,621 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,635 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:39307 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,648 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 30 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,648 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 28 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,649 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 28 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,650 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 29 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,651 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 30 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,651 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (csv at preprocess-deequ.scala:77) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,652 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,674 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 245.2 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,676 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,676 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.153.140:41813 (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,677 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,677 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,677 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,678 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,687 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:39307 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:08,723 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.152.82:49926\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:23:10,001 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 1322 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,001 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,001 INFO scheduler.DAGScheduler: ResultStage 3 (csv at preprocess-deequ.scala:77) finished in 1.348 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,002 INFO scheduler.DAGScheduler: Job 1 finished: csv at preprocess-deequ.scala:77, took 1.394423 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,459 INFO datasources.FileFormatWriter: Write Job 36a64c23-0f86-4c08-bac2-110dd73e5bbb committed.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,462 INFO datasources.FileFormatWriter: Finished processing stats for write job 36a64c23-0f86-4c08-bac2-110dd73e5bbb.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,572 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,573 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,573 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, review_id: string, star_rating: int ... 1 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,573 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,591 INFO codegen.CodeGenerator: Code generated in 7.732265 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,631 INFO codegen.CodeGenerator: Code generated in 15.925132 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,656 INFO codegen.CodeGenerator: Code generated in 16.053564 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,660 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,672 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,673 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,673 INFO spark.SparkContext: Created broadcast 5 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,673 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,683 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,684 INFO scheduler.DAGScheduler: Registering RDD 15 (collect at AnalysisRunner.scala:303) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,684 INFO scheduler.DAGScheduler: Got job 2 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,684 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,684 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,685 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,685 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,688 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.4 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,689 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.3 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,690 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.153.140:41813 (size: 9.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,690 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,691 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,691 INFO cluster.YarnScheduler: Adding task set 4.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,691 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,692 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,702 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:39307 (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:10,744 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:11,867 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 1176 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,171 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 1480 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,171 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (collect at AnalysisRunner.scala:303) finished in 1.485 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,172 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,173 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.7 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,175 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,175 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.153.140:41813 (size: 5.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,175 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,176 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,176 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,177 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,184 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:39307 (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,187 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,206 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 30 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,206 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,207 INFO scheduler.DAGScheduler: ResultStage 5 (collect at AnalysisRunner.scala:303) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,207 INFO scheduler.DAGScheduler: Job 2 finished: collect at AnalysisRunner.scala:303, took 1.524046 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,256 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,256 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,256 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,256 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,285 INFO codegen.CodeGenerator: Code generated in 22.136748 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,286 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,297 INFO codegen.CodeGenerator: Code generated in 9.29506 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,298 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,299 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,300 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,302 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 400.9 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,313 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:39307 in memory (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,315 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.153.140:41813 in memory (size: 15.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,319 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,319 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,320 INFO spark.SparkContext: Created broadcast 8 from count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,320 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,327 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,327 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,329 INFO spark.SparkContext: Starting job: count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,330 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.153.140:41813 in memory (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,330 INFO scheduler.DAGScheduler: Registering RDD 21 (count at GroupingAnalyzers.scala:76) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,330 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:39307 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,330 INFO scheduler.DAGScheduler: Got job 3 (count at GroupingAnalyzers.scala:76) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,331 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (count at GroupingAnalyzers.scala:76)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,331 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,331 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,331 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,339 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.0 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,340 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,341 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.153.140:41813 (size: 6.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,341 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,342 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,342 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,343 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,343 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,349 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,350 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,355 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:39307 in memory (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,355 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.153.140:41813 in memory (size: 5.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,363 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:39307 (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,364 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,365 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.153.140:41813 in memory (size: 9.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,366 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:39307 in memory (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,374 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,374 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,374 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,375 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,375 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,375 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,375 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,378 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.153.140:41813 in memory (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,378 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:39307 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,382 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,382 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,383 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,384 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,384 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.153.140:41813 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,386 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:39307 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,392 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:12,996 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 653 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,191 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 849 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,191 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at GroupingAnalyzers.scala:76) finished in 0.860 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 7)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,192 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,194 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.3 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,196 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,196 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.153.140:41813 (size: 3.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,196 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,197 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,197 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,198 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,204 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:39307 (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,207 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,220 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 23 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,220 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,221 INFO scheduler.DAGScheduler: ResultStage 7 (count at GroupingAnalyzers.scala:76) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,221 INFO scheduler.DAGScheduler: Job 3 finished: count at GroupingAnalyzers.scala:76, took 0.892114 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,324 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,325 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,325 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,326 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,346 INFO codegen.CodeGenerator: Code generated in 5.924446 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,360 INFO codegen.CodeGenerator: Code generated in 9.574288 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,394 INFO codegen.CodeGenerator: Code generated in 21.055688 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,443 INFO codegen.CodeGenerator: Code generated in 30.776616 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,447 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,459 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,459 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,460 INFO spark.SparkContext: Created broadcast 11 from collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,460 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,481 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,481 INFO scheduler.DAGScheduler: Registering RDD 27 (collect at AnalysisRunner.scala:499) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,481 INFO scheduler.DAGScheduler: Registering RDD 30 (collect at AnalysisRunner.scala:499) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,482 INFO scheduler.DAGScheduler: Got job 4 (collect at AnalysisRunner.scala:499) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,482 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (collect at AnalysisRunner.scala:499)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,482 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,482 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,482 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,486 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.2 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,487 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 12.7 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,487 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.153.140:41813 (size: 12.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,487 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,488 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,488 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,489 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,489 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,497 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:39307 (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:13,613 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:14,762 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 1273 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,175 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 1686 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,175 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (collect at AnalysisRunner.scala:499) finished in 1.693 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 9, ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,176 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,196 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 29.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,198 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.2 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,198 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.153.140:41813 (size: 14.2 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,199 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,201 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,201 INFO cluster.YarnScheduler: Adding task set 9.0 with 200 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,205 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17, algo-2, executor 1, partition 0, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,206 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18, algo-2, executor 1, partition 1, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,206 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 19, algo-2, executor 1, partition 2, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,206 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 20, algo-2, executor 1, partition 3, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,206 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 21, algo-2, executor 1, partition 4, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,217 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:39307 (size: 14.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,230 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,275 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 22, algo-2, executor 1, partition 5, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,275 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 70 ms on algo-2 (executor 1) (1/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,275 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 9.0 (TID 23, algo-2, executor 1, partition 6, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,276 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 71 ms on algo-2 (executor 1) (2/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,301 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 9.0 (TID 24, algo-2, executor 1, partition 7, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,302 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 9.0 (TID 25, algo-2, executor 1, partition 8, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,302 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 20) in 96 ms on algo-2 (executor 1) (3/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,302 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 19) in 96 ms on algo-2 (executor 1) (4/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,307 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 9.0 (TID 26, algo-2, executor 1, partition 9, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,307 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 21) in 101 ms on algo-2 (executor 1) (5/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,320 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 9.0 (TID 27, algo-2, executor 1, partition 10, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,320 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 22) in 46 ms on algo-2 (executor 1) (6/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,321 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 9.0 (TID 28, algo-2, executor 1, partition 11, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,321 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 9.0 (TID 23) in 46 ms on algo-2 (executor 1) (7/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,323 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 9.0 (TID 29, algo-2, executor 1, partition 12, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,323 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 9.0 (TID 25) in 22 ms on algo-2 (executor 1) (8/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,327 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 9.0 (TID 30, algo-2, executor 1, partition 13, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,327 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 9.0 (TID 24) in 26 ms on algo-2 (executor 1) (9/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,327 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 9.0 (TID 31, algo-2, executor 1, partition 14, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,328 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 9.0 (TID 26) in 21 ms on algo-2 (executor 1) (10/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,339 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 9.0 (TID 32, algo-2, executor 1, partition 15, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,339 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 9.0 (TID 27) in 19 ms on algo-2 (executor 1) (11/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,341 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 9.0 (TID 33, algo-2, executor 1, partition 16, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,341 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 9.0 (TID 28) in 20 ms on algo-2 (executor 1) (12/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,344 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 9.0 (TID 34, algo-2, executor 1, partition 17, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,345 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 9.0 (TID 30) in 18 ms on algo-2 (executor 1) (13/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,350 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 9.0 (TID 35, algo-2, executor 1, partition 18, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,350 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 9.0 (TID 31) in 23 ms on algo-2 (executor 1) (14/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,351 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 9.0 (TID 36, algo-2, executor 1, partition 19, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,351 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 9.0 (TID 29) in 29 ms on algo-2 (executor 1) (15/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,357 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 9.0 (TID 37, algo-2, executor 1, partition 20, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,357 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 9.0 (TID 33) in 16 ms on algo-2 (executor 1) (16/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,359 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 9.0 (TID 38, algo-2, executor 1, partition 21, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,359 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 9.0 (TID 32) in 20 ms on algo-2 (executor 1) (17/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,363 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 9.0 (TID 39, algo-2, executor 1, partition 22, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,363 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 9.0 (TID 34) in 19 ms on algo-2 (executor 1) (18/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,364 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 9.0 (TID 40, algo-2, executor 1, partition 23, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,364 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 9.0 (TID 35) in 15 ms on algo-2 (executor 1) (19/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,370 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 9.0 (TID 41, algo-2, executor 1, partition 24, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,370 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 9.0 (TID 36) in 19 ms on algo-2 (executor 1) (20/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,372 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 9.0 (TID 42, algo-2, executor 1, partition 25, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,372 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 9.0 (TID 37) in 15 ms on algo-2 (executor 1) (21/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,374 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 9.0 (TID 43, algo-2, executor 1, partition 26, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,374 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 9.0 (TID 38) in 15 ms on algo-2 (executor 1) (22/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,377 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 9.0 (TID 44, algo-2, executor 1, partition 27, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,377 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 9.0 (TID 39) in 14 ms on algo-2 (executor 1) (23/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,381 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 9.0 (TID 45, algo-2, executor 1, partition 28, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,382 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 9.0 (TID 40) in 18 ms on algo-2 (executor 1) (24/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,387 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 9.0 (TID 46, algo-2, executor 1, partition 29, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,387 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 9.0 (TID 41) in 17 ms on algo-2 (executor 1) (25/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,390 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 9.0 (TID 47, algo-2, executor 1, partition 30, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,390 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 9.0 (TID 43) in 16 ms on algo-2 (executor 1) (26/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,391 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 9.0 (TID 48, algo-2, executor 1, partition 31, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,391 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 9.0 (TID 42) in 19 ms on algo-2 (executor 1) (27/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,392 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 9.0 (TID 49, algo-2, executor 1, partition 32, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,393 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 9.0 (TID 44) in 16 ms on algo-2 (executor 1) (28/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,396 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 9.0 (TID 50, algo-2, executor 1, partition 33, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,397 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 9.0 (TID 45) in 16 ms on algo-2 (executor 1) (29/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,401 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 9.0 (TID 51, algo-2, executor 1, partition 34, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,401 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 9.0 (TID 46) in 14 ms on algo-2 (executor 1) (30/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,404 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 9.0 (TID 52, algo-2, executor 1, partition 35, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,404 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 9.0 (TID 48) in 13 ms on algo-2 (executor 1) (31/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,409 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 9.0 (TID 53, algo-2, executor 1, partition 36, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,409 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 9.0 (TID 47) in 19 ms on algo-2 (executor 1) (32/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,409 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 9.0 (TID 54, algo-2, executor 1, partition 37, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,410 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 9.0 (TID 49) in 18 ms on algo-2 (executor 1) (33/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,414 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 9.0 (TID 55, algo-2, executor 1, partition 38, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,414 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 9.0 (TID 50) in 18 ms on algo-2 (executor 1) (34/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,415 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 9.0 (TID 56, algo-2, executor 1, partition 39, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,415 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 9.0 (TID 51) in 14 ms on algo-2 (executor 1) (35/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,424 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 9.0 (TID 57, algo-2, executor 1, partition 40, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,424 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 9.0 (TID 52) in 20 ms on algo-2 (executor 1) (36/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,427 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 9.0 (TID 58, algo-2, executor 1, partition 41, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,427 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 9.0 (TID 54) in 18 ms on algo-2 (executor 1) (37/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,429 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 9.0 (TID 59, algo-2, executor 1, partition 42, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,429 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 9.0 (TID 55) in 15 ms on algo-2 (executor 1) (38/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,435 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 9.0 (TID 60, algo-2, executor 1, partition 43, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,435 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 9.0 (TID 53) in 27 ms on algo-2 (executor 1) (39/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,436 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 9.0 (TID 61, algo-2, executor 1, partition 44, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,436 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 9.0 (TID 56) in 21 ms on algo-2 (executor 1) (40/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,437 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 9.0 (TID 62, algo-2, executor 1, partition 45, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,437 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 9.0 (TID 57) in 14 ms on algo-2 (executor 1) (41/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,445 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 9.0 (TID 63, algo-2, executor 1, partition 46, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,445 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 9.0 (TID 58) in 19 ms on algo-2 (executor 1) (42/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,445 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 9.0 (TID 64, algo-2, executor 1, partition 47, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,445 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 9.0 (TID 59) in 16 ms on algo-2 (executor 1) (43/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,448 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 9.0 (TID 65, algo-2, executor 1, partition 48, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,448 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 9.0 (TID 60) in 13 ms on algo-2 (executor 1) (44/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,450 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 9.0 (TID 66, algo-2, executor 1, partition 49, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,450 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 9.0 (TID 62) in 13 ms on algo-2 (executor 1) (45/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,455 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 9.0 (TID 67, algo-2, executor 1, partition 50, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,455 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 9.0 (TID 61) in 19 ms on algo-2 (executor 1) (46/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,462 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 9.0 (TID 68, algo-2, executor 1, partition 51, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,462 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 9.0 (TID 64) in 17 ms on algo-2 (executor 1) (47/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,463 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 9.0 (TID 69, algo-2, executor 1, partition 52, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,463 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 9.0 (TID 63) in 19 ms on algo-2 (executor 1) (48/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,465 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 9.0 (TID 70, algo-2, executor 1, partition 53, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,465 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 9.0 (TID 65) in 17 ms on algo-2 (executor 1) (49/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,468 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 9.0 (TID 71, algo-2, executor 1, partition 54, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,468 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 9.0 (TID 66) in 18 ms on algo-2 (executor 1) (50/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,472 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 9.0 (TID 72, algo-2, executor 1, partition 55, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,472 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 9.0 (TID 67) in 17 ms on algo-2 (executor 1) (51/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,477 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 9.0 (TID 73, algo-2, executor 1, partition 56, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,477 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 9.0 (TID 69) in 15 ms on algo-2 (executor 1) (52/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,481 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 9.0 (TID 74, algo-2, executor 1, partition 57, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,481 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 9.0 (TID 75, algo-2, executor 1, partition 58, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,481 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 9.0 (TID 68) in 19 ms on algo-2 (executor 1) (53/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,482 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 9.0 (TID 70) in 17 ms on algo-2 (executor 1) (54/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,483 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 9.0 (TID 76, algo-2, executor 1, partition 59, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,483 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 9.0 (TID 71) in 15 ms on algo-2 (executor 1) (55/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,487 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 9.0 (TID 77, algo-2, executor 1, partition 60, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,487 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 9.0 (TID 72) in 15 ms on algo-2 (executor 1) (56/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,491 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 9.0 (TID 78, algo-2, executor 1, partition 61, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,492 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 9.0 (TID 73) in 15 ms on algo-2 (executor 1) (57/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,496 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 9.0 (TID 79, algo-2, executor 1, partition 62, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,496 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 9.0 (TID 74) in 15 ms on algo-2 (executor 1) (58/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,500 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 9.0 (TID 80, algo-2, executor 1, partition 63, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,500 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 9.0 (TID 75) in 19 ms on algo-2 (executor 1) (59/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,503 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 9.0 (TID 81, algo-2, executor 1, partition 64, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,504 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 9.0 (TID 77) in 17 ms on algo-2 (executor 1) (60/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,504 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 9.0 (TID 82, algo-2, executor 1, partition 65, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,504 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 9.0 (TID 76) in 21 ms on algo-2 (executor 1) (61/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,506 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 9.0 (TID 83, algo-2, executor 1, partition 66, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,507 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 9.0 (TID 78) in 16 ms on algo-2 (executor 1) (62/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,513 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 9.0 (TID 84, algo-2, executor 1, partition 67, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,513 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 9.0 (TID 79) in 17 ms on algo-2 (executor 1) (63/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,515 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 9.0 (TID 85, algo-2, executor 1, partition 68, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,516 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 9.0 (TID 81) in 13 ms on algo-2 (executor 1) (64/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,519 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 9.0 (TID 86, algo-2, executor 1, partition 69, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,519 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 9.0 (TID 82) in 15 ms on algo-2 (executor 1) (65/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,520 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 9.0 (TID 87, algo-2, executor 1, partition 70, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,520 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 9.0 (TID 80) in 20 ms on algo-2 (executor 1) (66/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,521 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 9.0 (TID 88, algo-2, executor 1, partition 71, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,521 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 9.0 (TID 83) in 15 ms on algo-2 (executor 1) (67/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,531 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 9.0 (TID 89, algo-2, executor 1, partition 72, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,531 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 9.0 (TID 84) in 18 ms on algo-2 (executor 1) (68/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,537 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 9.0 (TID 90, algo-2, executor 1, partition 73, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,537 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 9.0 (TID 86) in 18 ms on algo-2 (executor 1) (69/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,539 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 9.0 (TID 91, algo-2, executor 1, partition 74, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,539 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 9.0 (TID 87) in 19 ms on algo-2 (executor 1) (70/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,540 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 9.0 (TID 92, algo-2, executor 1, partition 75, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,540 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 9.0 (TID 85) in 25 ms on algo-2 (executor 1) (71/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,541 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 9.0 (TID 93, algo-2, executor 1, partition 76, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,541 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 9.0 (TID 88) in 20 ms on algo-2 (executor 1) (72/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,558 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 9.0 (TID 94, algo-2, executor 1, partition 77, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,558 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 9.0 (TID 95, algo-2, executor 1, partition 78, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,559 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 9.0 (TID 96, algo-2, executor 1, partition 79, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,559 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 9.0 (TID 90) in 22 ms on algo-2 (executor 1) (73/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,559 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 9.0 (TID 92) in 19 ms on algo-2 (executor 1) (74/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,559 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 9.0 (TID 97, algo-2, executor 1, partition 80, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,560 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 9.0 (TID 91) in 21 ms on algo-2 (executor 1) (75/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,560 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 9.0 (TID 89) in 29 ms on algo-2 (executor 1) (76/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,561 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 9.0 (TID 98, algo-2, executor 1, partition 81, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,561 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 9.0 (TID 93) in 20 ms on algo-2 (executor 1) (77/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,577 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 9.0 (TID 99, algo-2, executor 1, partition 82, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,577 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 9.0 (TID 95) in 19 ms on algo-2 (executor 1) (78/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,577 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 9.0 (TID 100, algo-2, executor 1, partition 83, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,578 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 9.0 (TID 96) in 19 ms on algo-2 (executor 1) (79/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,578 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 9.0 (TID 101, algo-2, executor 1, partition 84, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,579 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 9.0 (TID 102, algo-2, executor 1, partition 85, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,579 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 9.0 (TID 97) in 20 ms on algo-2 (executor 1) (80/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,579 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 9.0 (TID 98) in 19 ms on algo-2 (executor 1) (81/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,581 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 9.0 (TID 103, algo-2, executor 1, partition 86, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,581 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 9.0 (TID 94) in 24 ms on algo-2 (executor 1) (82/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,595 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 9.0 (TID 104, algo-2, executor 1, partition 87, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,595 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 9.0 (TID 100) in 18 ms on algo-2 (executor 1) (83/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,598 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 9.0 (TID 105, algo-2, executor 1, partition 88, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,598 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 9.0 (TID 103) in 17 ms on algo-2 (executor 1) (84/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,600 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 9.0 (TID 106, algo-2, executor 1, partition 89, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,601 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 9.0 (TID 99) in 25 ms on algo-2 (executor 1) (85/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,601 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 9.0 (TID 107, algo-2, executor 1, partition 90, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,602 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 9.0 (TID 108, algo-2, executor 1, partition 91, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,602 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 9.0 (TID 102) in 24 ms on algo-2 (executor 1) (86/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,602 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 9.0 (TID 101) in 24 ms on algo-2 (executor 1) (87/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,609 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 9.0 (TID 109, algo-2, executor 1, partition 92, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,610 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 9.0 (TID 104) in 15 ms on algo-2 (executor 1) (88/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,611 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 9.0 (TID 110, algo-2, executor 1, partition 93, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,612 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 9.0 (TID 105) in 14 ms on algo-2 (executor 1) (89/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,616 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 9.0 (TID 111, algo-2, executor 1, partition 94, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,616 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 9.0 (TID 106) in 16 ms on algo-2 (executor 1) (90/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,618 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 9.0 (TID 112, algo-2, executor 1, partition 95, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,619 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 9.0 (TID 113, algo-2, executor 1, partition 96, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,619 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 9.0 (TID 107) in 18 ms on algo-2 (executor 1) (91/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,619 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 9.0 (TID 108) in 17 ms on algo-2 (executor 1) (92/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,624 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 9.0 (TID 114, algo-2, executor 1, partition 97, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,624 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 9.0 (TID 109) in 15 ms on algo-2 (executor 1) (93/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,627 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 9.0 (TID 115, algo-2, executor 1, partition 98, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,627 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 9.0 (TID 110) in 16 ms on algo-2 (executor 1) (94/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,628 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 9.0 (TID 116, algo-2, executor 1, partition 99, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,628 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 9.0 (TID 111) in 12 ms on algo-2 (executor 1) (95/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,630 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 9.0 (TID 117, algo-2, executor 1, partition 100, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,630 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 9.0 (TID 112) in 12 ms on algo-2 (executor 1) (96/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,634 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 9.0 (TID 118, algo-2, executor 1, partition 101, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,634 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 9.0 (TID 113) in 15 ms on algo-2 (executor 1) (97/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,645 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 9.0 (TID 119, algo-2, executor 1, partition 102, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,645 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 9.0 (TID 115) in 18 ms on algo-2 (executor 1) (98/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,646 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 9.0 (TID 120, algo-2, executor 1, partition 103, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,646 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 9.0 (TID 116) in 18 ms on algo-2 (executor 1) (99/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,647 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 9.0 (TID 121, algo-2, executor 1, partition 104, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,648 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 9.0 (TID 117) in 18 ms on algo-2 (executor 1) (100/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,648 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 9.0 (TID 122, algo-2, executor 1, partition 105, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,649 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 9.0 (TID 114) in 25 ms on algo-2 (executor 1) (101/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,652 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 9.0 (TID 123, algo-2, executor 1, partition 106, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,652 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 9.0 (TID 118) in 19 ms on algo-2 (executor 1) (102/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,661 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 9.0 (TID 124, algo-2, executor 1, partition 107, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,661 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 9.0 (TID 119) in 16 ms on algo-2 (executor 1) (103/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,662 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 9.0 (TID 125, algo-2, executor 1, partition 108, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,663 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 9.0 (TID 126, algo-2, executor 1, partition 109, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,663 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 9.0 (TID 127, algo-2, executor 1, partition 110, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,664 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 9.0 (TID 121) in 17 ms on algo-2 (executor 1) (104/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,664 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 9.0 (TID 120) in 18 ms on algo-2 (executor 1) (105/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,664 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 9.0 (TID 122) in 16 ms on algo-2 (executor 1) (106/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,673 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 9.0 (TID 128, algo-2, executor 1, partition 111, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,674 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 9.0 (TID 124) in 14 ms on algo-2 (executor 1) (107/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,674 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 9.0 (TID 129, algo-2, executor 1, partition 112, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,674 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 9.0 (TID 123) in 22 ms on algo-2 (executor 1) (108/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,675 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 9.0 (TID 130, algo-2, executor 1, partition 113, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,675 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 9.0 (TID 125) in 13 ms on algo-2 (executor 1) (109/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,676 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 9.0 (TID 131, algo-2, executor 1, partition 114, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,677 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 9.0 (TID 127) in 14 ms on algo-2 (executor 1) (110/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,681 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 9.0 (TID 132, algo-2, executor 1, partition 115, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,682 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 9.0 (TID 126) in 19 ms on algo-2 (executor 1) (111/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,688 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 9.0 (TID 133, algo-2, executor 1, partition 116, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,688 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 9.0 (TID 130) in 13 ms on algo-2 (executor 1) (112/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,689 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 9.0 (TID 134, algo-2, executor 1, partition 117, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,690 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 9.0 (TID 131) in 14 ms on algo-2 (executor 1) (113/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,691 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 9.0 (TID 135, algo-2, executor 1, partition 118, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,691 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 9.0 (TID 129) in 17 ms on algo-2 (executor 1) (114/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,692 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 9.0 (TID 136, algo-2, executor 1, partition 119, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,692 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 9.0 (TID 128) in 19 ms on algo-2 (executor 1) (115/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,693 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 9.0 (TID 137, algo-2, executor 1, partition 120, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,694 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 9.0 (TID 132) in 13 ms on algo-2 (executor 1) (116/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,703 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 9.0 (TID 138, algo-2, executor 1, partition 121, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,704 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 9.0 (TID 133) in 15 ms on algo-2 (executor 1) (117/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,711 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 9.0 (TID 139, algo-2, executor 1, partition 122, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,712 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 9.0 (TID 137) in 19 ms on algo-2 (executor 1) (118/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,712 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 9.0 (TID 140, algo-2, executor 1, partition 123, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,712 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 9.0 (TID 134) in 23 ms on algo-2 (executor 1) (119/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,713 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 9.0 (TID 141, algo-2, executor 1, partition 124, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,713 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 9.0 (TID 135) in 22 ms on algo-2 (executor 1) (120/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,714 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 9.0 (TID 142, algo-2, executor 1, partition 125, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,714 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 9.0 (TID 136) in 22 ms on algo-2 (executor 1) (121/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,717 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 9.0 (TID 143, algo-2, executor 1, partition 126, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,717 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 9.0 (TID 138) in 14 ms on algo-2 (executor 1) (122/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,724 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 9.0 (TID 144, algo-2, executor 1, partition 127, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,725 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 9.0 (TID 139) in 14 ms on algo-2 (executor 1) (123/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,725 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 9.0 (TID 145, algo-2, executor 1, partition 128, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,725 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 9.0 (TID 140) in 13 ms on algo-2 (executor 1) (124/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,726 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 9.0 (TID 146, algo-2, executor 1, partition 129, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,727 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 9.0 (TID 141) in 14 ms on algo-2 (executor 1) (125/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,728 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 9.0 (TID 147, algo-2, executor 1, partition 130, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,728 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 9.0 (TID 143) in 12 ms on algo-2 (executor 1) (126/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,729 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 9.0 (TID 148, algo-2, executor 1, partition 131, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,729 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 9.0 (TID 142) in 15 ms on algo-2 (executor 1) (127/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,737 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 9.0 (TID 149, algo-2, executor 1, partition 132, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,738 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 9.0 (TID 144) in 14 ms on algo-2 (executor 1) (128/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,738 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 9.0 (TID 150, algo-2, executor 1, partition 133, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,738 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 9.0 (TID 145) in 13 ms on algo-2 (executor 1) (129/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,740 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 9.0 (TID 151, algo-2, executor 1, partition 134, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,740 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 9.0 (TID 147) in 12 ms on algo-2 (executor 1) (130/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,741 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 9.0 (TID 152, algo-2, executor 1, partition 135, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,741 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 9.0 (TID 146) in 15 ms on algo-2 (executor 1) (131/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,742 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 9.0 (TID 153, algo-2, executor 1, partition 136, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,743 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 9.0 (TID 148) in 15 ms on algo-2 (executor 1) (132/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,750 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 9.0 (TID 154, algo-2, executor 1, partition 137, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,750 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 9.0 (TID 150) in 12 ms on algo-2 (executor 1) (133/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,752 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 9.0 (TID 155, algo-2, executor 1, partition 138, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,752 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 9.0 (TID 149) in 15 ms on algo-2 (executor 1) (134/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,753 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 9.0 (TID 156, algo-2, executor 1, partition 139, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,753 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 9.0 (TID 151) in 13 ms on algo-2 (executor 1) (135/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,754 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 9.0 (TID 157, algo-2, executor 1, partition 140, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,754 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 9.0 (TID 153) in 12 ms on algo-2 (executor 1) (136/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,756 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 9.0 (TID 158, algo-2, executor 1, partition 141, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,756 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 9.0 (TID 152) in 15 ms on algo-2 (executor 1) (137/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,762 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 9.0 (TID 159, algo-2, executor 1, partition 142, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,762 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 9.0 (TID 154) in 13 ms on algo-2 (executor 1) (138/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,763 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 9.0 (TID 160, algo-2, executor 1, partition 143, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,763 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 9.0 (TID 155) in 11 ms on algo-2 (executor 1) (139/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,765 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 9.0 (TID 161, algo-2, executor 1, partition 144, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,765 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 9.0 (TID 157) in 11 ms on algo-2 (executor 1) (140/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,766 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 9.0 (TID 162, algo-2, executor 1, partition 145, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,767 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 9.0 (TID 158) in 11 ms on algo-2 (executor 1) (141/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,768 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 9.0 (TID 163, algo-2, executor 1, partition 146, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,768 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 9.0 (TID 156) in 15 ms on algo-2 (executor 1) (142/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,776 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 9.0 (TID 164, algo-2, executor 1, partition 147, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,776 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 9.0 (TID 160) in 13 ms on algo-2 (executor 1) (143/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,776 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 9.0 (TID 165, algo-2, executor 1, partition 148, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,777 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 9.0 (TID 159) in 15 ms on algo-2 (executor 1) (144/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,777 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 9.0 (TID 166, algo-2, executor 1, partition 149, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,777 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 9.0 (TID 161) in 12 ms on algo-2 (executor 1) (145/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,779 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 9.0 (TID 167, algo-2, executor 1, partition 150, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,779 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 9.0 (TID 162) in 13 ms on algo-2 (executor 1) (146/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,780 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 9.0 (TID 168, algo-2, executor 1, partition 151, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,780 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 9.0 (TID 163) in 12 ms on algo-2 (executor 1) (147/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,787 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 9.0 (TID 169, algo-2, executor 1, partition 152, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,788 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 9.0 (TID 164) in 12 ms on algo-2 (executor 1) (148/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,788 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 9.0 (TID 170, algo-2, executor 1, partition 153, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,789 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 9.0 (TID 165) in 13 ms on algo-2 (executor 1) (149/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,789 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 9.0 (TID 171, algo-2, executor 1, partition 154, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,789 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 9.0 (TID 166) in 12 ms on algo-2 (executor 1) (150/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,791 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 9.0 (TID 172, algo-2, executor 1, partition 155, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,791 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 9.0 (TID 167) in 13 ms on algo-2 (executor 1) (151/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,792 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 9.0 (TID 173, algo-2, executor 1, partition 156, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,792 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 9.0 (TID 168) in 12 ms on algo-2 (executor 1) (152/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,800 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 9.0 (TID 174, algo-2, executor 1, partition 157, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,800 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 9.0 (TID 169) in 13 ms on algo-2 (executor 1) (153/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,802 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 9.0 (TID 175, algo-2, executor 1, partition 158, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,802 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 9.0 (TID 172) in 11 ms on algo-2 (executor 1) (154/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,804 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 9.0 (TID 176, algo-2, executor 1, partition 159, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,804 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 9.0 (TID 173) in 12 ms on algo-2 (executor 1) (155/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,806 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 9.0 (TID 177, algo-2, executor 1, partition 160, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,806 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 9.0 (TID 178, algo-2, executor 1, partition 161, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,807 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 9.0 (TID 171) in 18 ms on algo-2 (executor 1) (156/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,807 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 9.0 (TID 170) in 19 ms on algo-2 (executor 1) (157/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,812 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 9.0 (TID 179, algo-2, executor 1, partition 162, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,813 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 9.0 (TID 174) in 13 ms on algo-2 (executor 1) (158/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,813 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 9.0 (TID 180, algo-2, executor 1, partition 163, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,814 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 9.0 (TID 175) in 12 ms on algo-2 (executor 1) (159/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,818 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 9.0 (TID 181, algo-2, executor 1, partition 164, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,818 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 9.0 (TID 176) in 14 ms on algo-2 (executor 1) (160/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,824 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 9.0 (TID 182, algo-2, executor 1, partition 165, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,825 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 9.0 (TID 178) in 19 ms on algo-2 (executor 1) (161/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,825 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 9.0 (TID 183, algo-2, executor 1, partition 166, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,825 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 9.0 (TID 177) in 19 ms on algo-2 (executor 1) (162/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,827 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 9.0 (TID 184, algo-2, executor 1, partition 167, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,827 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 9.0 (TID 179) in 15 ms on algo-2 (executor 1) (163/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,827 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 9.0 (TID 185, algo-2, executor 1, partition 168, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,828 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 9.0 (TID 180) in 15 ms on algo-2 (executor 1) (164/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,832 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 9.0 (TID 186, algo-2, executor 1, partition 169, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,833 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 9.0 (TID 181) in 15 ms on algo-2 (executor 1) (165/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,839 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 9.0 (TID 187, algo-2, executor 1, partition 170, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,839 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 9.0 (TID 182) in 15 ms on algo-2 (executor 1) (166/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,841 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 9.0 (TID 188, algo-2, executor 1, partition 171, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,841 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 9.0 (TID 185) in 14 ms on algo-2 (executor 1) (167/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,841 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 9.0 (TID 189, algo-2, executor 1, partition 172, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,842 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 9.0 (TID 183) in 16 ms on algo-2 (executor 1) (168/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,842 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 9.0 (TID 190, algo-2, executor 1, partition 173, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,842 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 9.0 (TID 184) in 15 ms on algo-2 (executor 1) (169/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,845 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 9.0 (TID 191, algo-2, executor 1, partition 174, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,845 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 9.0 (TID 186) in 13 ms on algo-2 (executor 1) (170/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,851 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 9.0 (TID 192, algo-2, executor 1, partition 175, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,852 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 9.0 (TID 187) in 13 ms on algo-2 (executor 1) (171/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,853 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 9.0 (TID 193, algo-2, executor 1, partition 176, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,854 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 9.0 (TID 189) in 12 ms on algo-2 (executor 1) (172/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,854 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 9.0 (TID 194, algo-2, executor 1, partition 177, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,854 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 9.0 (TID 190) in 12 ms on algo-2 (executor 1) (173/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,855 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 9.0 (TID 195, algo-2, executor 1, partition 178, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:15,855 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 9.0 (TID 188) in 15 ms on algo-2 (executor 1) (174/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,004 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 9.0 (TID 196, algo-2, executor 1, partition 179, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,005 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 9.0 (TID 191) in 160 ms on algo-2 (executor 1) (175/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,014 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 9.0 (TID 197, algo-2, executor 1, partition 180, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,015 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 9.0 (TID 192) in 164 ms on algo-2 (executor 1) (176/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,015 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 9.0 (TID 198, algo-2, executor 1, partition 181, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,015 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 9.0 (TID 195) in 161 ms on algo-2 (executor 1) (177/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,016 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 9.0 (TID 199, algo-2, executor 1, partition 182, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,016 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 9.0 (TID 196) in 12 ms on algo-2 (executor 1) (178/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,021 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 9.0 (TID 200, algo-2, executor 1, partition 183, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,021 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 9.0 (TID 194) in 167 ms on algo-2 (executor 1) (179/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,022 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 9.0 (TID 201, algo-2, executor 1, partition 184, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,022 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 9.0 (TID 193) in 169 ms on algo-2 (executor 1) (180/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,026 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 9.0 (TID 202, algo-2, executor 1, partition 185, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,027 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 9.0 (TID 203, algo-2, executor 1, partition 186, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,027 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 9.0 (TID 197) in 13 ms on algo-2 (executor 1) (181/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,027 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 9.0 (TID 198) in 12 ms on algo-2 (executor 1) (182/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,028 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 9.0 (TID 204, algo-2, executor 1, partition 187, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,028 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 9.0 (TID 200) in 8 ms on algo-2 (executor 1) (183/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,033 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 9.0 (TID 205, algo-2, executor 1, partition 188, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,033 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 9.0 (TID 199) in 17 ms on algo-2 (executor 1) (184/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,035 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 9.0 (TID 206, algo-2, executor 1, partition 189, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,035 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 9.0 (TID 207, algo-2, executor 1, partition 190, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,035 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 9.0 (TID 202) in 9 ms on algo-2 (executor 1) (185/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,036 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 9.0 (TID 203) in 9 ms on algo-2 (executor 1) (186/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,038 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 9.0 (TID 208, algo-2, executor 1, partition 191, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,039 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 9.0 (TID 201) in 16 ms on algo-2 (executor 1) (187/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,039 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 9.0 (TID 209, algo-2, executor 1, partition 192, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,040 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 9.0 (TID 204) in 12 ms on algo-2 (executor 1) (188/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,043 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 9.0 (TID 210, algo-2, executor 1, partition 193, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,043 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 9.0 (TID 206) in 8 ms on algo-2 (executor 1) (189/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,044 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 9.0 (TID 211, algo-2, executor 1, partition 194, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,044 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 9.0 (TID 207) in 9 ms on algo-2 (executor 1) (190/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,051 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 9.0 (TID 212, algo-2, executor 1, partition 195, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,051 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 9.0 (TID 205) in 18 ms on algo-2 (executor 1) (191/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,051 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 9.0 (TID 213, algo-2, executor 1, partition 196, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,052 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 9.0 (TID 209) in 13 ms on algo-2 (executor 1) (192/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,052 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 9.0 (TID 214, algo-2, executor 1, partition 197, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,052 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 9.0 (TID 208) in 14 ms on algo-2 (executor 1) (193/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,055 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 9.0 (TID 215, algo-2, executor 1, partition 198, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,055 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 9.0 (TID 211) in 11 ms on algo-2 (executor 1) (194/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,058 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 9.0 (TID 216, algo-2, executor 1, partition 199, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,058 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 9.0 (TID 210) in 15 ms on algo-2 (executor 1) (195/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,060 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 9.0 (TID 212) in 10 ms on algo-2 (executor 1) (196/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,061 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 9.0 (TID 214) in 9 ms on algo-2 (executor 1) (197/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,063 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 9.0 (TID 215) in 8 ms on algo-2 (executor 1) (198/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,066 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 9.0 (TID 216) in 8 ms on algo-2 (executor 1) (199/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,066 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 9.0 (TID 213) in 15 ms on algo-2 (executor 1) (200/200)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at AnalysisRunner.scala:499) finished in 0.873 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,067 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,069 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.5 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,071 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.4 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,071 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.153.140:41813 (size: 4.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,072 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,072 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,072 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,073 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 217, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,079 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:39307 (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,082 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,126 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 217) in 52 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,126 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,126 INFO scheduler.DAGScheduler: ResultStage 10 (collect at AnalysisRunner.scala:499) finished in 0.057 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,127 INFO scheduler.DAGScheduler: Job 4 finished: collect at AnalysisRunner.scala:499, took 2.645741 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,219 INFO codegen.CodeGenerator: Code generated in 25.518294 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,265 INFO codegen.CodeGenerator: Code generated in 10.613313 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,276 INFO codegen.CodeGenerator: Code generated in 8.476368 ms\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|check       |check_level|check_status|constraint                                                                                                                                         |constraint_status|constraint_message|\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |SizeConstraint(Size(None))                                                                                                                         |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MinimumConstraint(Minimum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MaximumConstraint(Maximum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(review_id,None))                                                                                               |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |UniquenessConstraint(Uniqueness(List(review_id)))                                                                                                  |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(marketplace,None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,`marketplace` IS NULL OR `marketplace` IN ('US','UK','DE','JP','FR'),None))|Success          |                  |\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,362 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,362 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,362 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,362 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200822182316_0000}; taskId=attempt_20200822182316_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@263d98c1}; outputPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks, workPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks/_temporary/0/_temporary/attempt_20200822182316_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,362 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,631 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:110\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Registering RDD 36 (csv at preprocess-deequ.scala:110) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Got job 5 (csv at preprocess-deequ.scala:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (csv at preprocess-deequ.scala:110)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,633 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,635 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.2 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,636 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,636 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.153.140:41813 (size: 3.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,636 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,637 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,637 INFO cluster.YarnScheduler: Adding task set 11.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,637 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 218, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8156 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,637 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 219, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8172 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,638 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 220, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,638 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 11.0 (TID 221, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,638 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 11.0 (TID 222, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8448 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,645 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:39307 (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,648 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 219) in 11 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,648 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 11.0 (TID 221) in 10 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,648 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 218) in 11 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 11.0 (TID 222) in 10 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 220) in 11 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (csv at preprocess-deequ.scala:110) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,649 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,670 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 245.3 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,672 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,672 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.153.140:41813 (size: 90.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,673 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,673 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,673 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,674 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 223, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,679 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:39307 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:16,689 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:17,661 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 223) in 987 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:17,661 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:17,662 INFO scheduler.DAGScheduler: ResultStage 12 (csv at preprocess-deequ.scala:110) finished in 1.012 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:17,662 INFO scheduler.DAGScheduler: Job 5 finished: csv at preprocess-deequ.scala:110, took 1.030456 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,050 INFO datasources.FileFormatWriter: Write Job 9a5e4738-a450-4edb-ae35-dc0fe264df26 committed.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,050 INFO datasources.FileFormatWriter: Finished processing stats for write job 9a5e4738-a450-4edb-ae35-dc0fe264df26.\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|entity |instance                               |name        |value   |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Uniqueness  |1.0     |\u001b[0m\n",
      "\u001b[34m|Dataset|*                                      |Size        |247515.0|\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Maximum     |5.0     |\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Minimum     |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace contained in US,UK,DE,JP,FR|Compliance  |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace                            |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,098 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,098 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,099 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.153.140:41813 in memory (size: 3.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,101 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:39307 in memory (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,102 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,104 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.153.140:41813 in memory (size: 12.7 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,104 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:39307 in memory (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,106 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,107 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.153.140:41813 in memory (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,108 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:39307 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,109 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,113 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,113 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,113 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,115 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:39307 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,116 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.153.140:41813 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,118 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,119 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.153.140:41813 in memory (size: 14.2 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,120 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:39307 in memory (size: 14.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,121 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,121 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,122 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,124 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.153.140:41813 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,124 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:39307 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,125 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,126 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,128 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.153.140:41813 in memory (size: 6.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,128 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:39307 in memory (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,130 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,131 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.153.140:41813 in memory (size: 4.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,132 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:39307 in memory (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,133 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,134 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.153.140:41813 in memory (size: 3.1 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,134 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:39307 in memory (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,135 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,135 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,135 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,135 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,136 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,160 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,160 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,160 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,160 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200822182318_0000}; taskId=attempt_20200822182318_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2922851}; outputPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics, workPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics/_temporary/0/_temporary/attempt_20200822182318_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,160 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,419 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:122\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,420 INFO scheduler.DAGScheduler: Registering RDD 42 (csv at preprocess-deequ.scala:122) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,420 INFO scheduler.DAGScheduler: Got job 6 (csv at preprocess-deequ.scala:122) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,420 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (csv at preprocess-deequ.scala:122)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,420 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,420 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,421 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,423 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,424 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,424 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.153.140:41813 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,424 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,424 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,424 INFO cluster.YarnScheduler: Adding task set 13.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,425 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 224, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,425 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 225, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,425 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 226, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8181 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,425 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 13.0 (TID 227, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,426 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 13.0 (TID 228, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,433 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:39307 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,436 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 13.0 (TID 227) in 11 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,437 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 13.0 (TID 228) in 12 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,437 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 224) in 12 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,437 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 226) in 12 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,437 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 225) in 12 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,437 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (csv at preprocess-deequ.scala:122) finished in 0.017 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,438 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,459 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 245.2 KB, free 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,460 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 90.5 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,461 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.153.140:41813 (size: 90.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,461 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,461 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,461 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,462 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 229, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,467 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:39307 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:18,476 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,364 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 229) in 902 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,364 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,364 INFO scheduler.DAGScheduler: ResultStage 14 (csv at preprocess-deequ.scala:122) finished in 0.926 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,365 INFO scheduler.DAGScheduler: Job 6 finished: csv at preprocess-deequ.scala:122, took 0.945857 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,764 INFO datasources.FileFormatWriter: Write Job f92a1c08-2f02-4a6a-840e-70b070a62bd6 committed.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,764 INFO datasources.FileFormatWriter: Finished processing stats for write job f92a1c08-2f02-4a6a-840e-70b070a62bd6.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:23:19,943 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,943 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,944 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,944 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,982 INFO codegen.CodeGenerator: Code generated in 10.789745 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:19,990 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 400.9 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,001 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,001 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,002 INFO spark.SparkContext: Created broadcast 19 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,002 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,071 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Registering RDD 49 (collect at AnalysisRunner.scala:303) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Got job 7 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Final stage: ResultStage 16 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,072 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,077 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 143.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,079 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 45.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,079 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.153.140:41813 (size: 45.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,079 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,080 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,080 INFO cluster.YarnScheduler: Adding task set 15.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,081 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 230, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,081 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 231, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,088 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:39307 (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:20,119 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:23,468 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 231) in 3387 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 230) in 4321 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:303) finished in 4.330 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 16)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,402 INFO scheduler.DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,407 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 174.5 KB, free 364.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,408 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 56.4 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,409 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.153.140:41813 (size: 56.4 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,409 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,409 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,410 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,410 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 232, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,417 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:39307 (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,422 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,602 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 232) in 192 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,602 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,602 INFO scheduler.DAGScheduler: ResultStage 16 (collect at AnalysisRunner.scala:303) finished in 0.199 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,603 INFO scheduler.DAGScheduler: Job 7 finished: collect at AnalysisRunner.scala:303, took 4.531371 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,711 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,712 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,712 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: string, product_parent: string, star_rating: int, helpful_votes: int, total_votes: int ... 3 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,712 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,770 INFO codegen.CodeGenerator: Code generated in 24.52883 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,784 INFO codegen.CodeGenerator: Code generated in 8.809199 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,789 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 400.9 KB, free 364.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,805 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,805 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,806 INFO spark.SparkContext: Created broadcast 22 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,806 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,833 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Registering RDD 56 (collect at AnalysisRunner.scala:303) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Got job 8 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,835 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,838 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.2 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,839 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.9 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,839 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.153.140:41813 (size: 13.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,839 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,840 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,840 INFO cluster.YarnScheduler: Adding task set 17.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,840 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 233, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,841 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 234, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,847 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:39307 (size: 13.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:24,877 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,323 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 234) in 1482 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,900 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 233) in 2060 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: ShuffleMapStage 17 (collect at AnalysisRunner.scala:303) finished in 2.065 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 18)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,901 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,903 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 38.2 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,914 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 15.5 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,914 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.153.140:41813 (size: 15.5 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,914 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,915 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,915 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,915 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 235, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,917 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,917 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,917 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,917 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,917 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned shuffle 8\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 550\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 540\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 519\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 522\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 528\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 551\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 523\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 543\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 552\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 542\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 545\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 511\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 516\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 520\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 512\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 526\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 534\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,918 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,920 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.153.140:41813 in memory (size: 3.0 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,921 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-2:39307 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,921 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:39307 (size: 15.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 539\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 549\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 533\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 530\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 529\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 536\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 513\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,922 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,923 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.153.140:41813 in memory (size: 45.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,924 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:39307 in memory (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,924 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,927 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.153.140:41813 in memory (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,928 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:39307 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 517\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 537\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 538\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 524\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 544\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 514\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 532\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 548\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,929 INFO spark.ContextCleaner: Cleaned accumulator 515\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,930 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.153.140:41813 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,931 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:39307 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,932 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,932 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,932 INFO spark.ContextCleaner: Cleaned accumulator 546\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,932 INFO spark.ContextCleaner: Cleaned accumulator 541\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,934 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.153.140:41813 in memory (size: 56.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,934 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-2:39307 in memory (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 521\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 535\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 547\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 527\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 525\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 531\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 518\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:26,938 INFO spark.ContextCleaner: Cleaned shuffle 7\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,002 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 235) in 87 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,002 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,002 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:303) finished in 0.100 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,002 INFO scheduler.DAGScheduler: Job 8 finished: collect at AnalysisRunner.scala:303, took 2.168850 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,041 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,041 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,042 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,042 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,046 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 400.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,057 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,057 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.153.140:41813 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,058 INFO spark.SparkContext: Created broadcast 25 from rdd at ColumnProfiler.scala:533\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,058 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,083 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:547\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Registering RDD 66 (countByKey at ColumnProfiler.scala:547) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Got job 9 (countByKey at ColumnProfiler.scala:547) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (countByKey at ColumnProfiler.scala:547)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,086 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,098 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.5 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,099 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 9.4 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,099 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.153.140:41813 (size: 9.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,099 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,101 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,102 INFO cluster.YarnScheduler: Adding task set 19.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,102 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 236, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,102 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 237, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,108 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:39307 (size: 9.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:27,967 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:39307 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:29,640 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 237) in 2538 ms on algo-2 (executor 1) (1/2)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 236) in 3266 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (countByKey at ColumnProfiler.scala:547) finished in 3.281 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 20)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,368 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,369 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.0 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,370 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1795.0 B, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,370 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.153.140:41813 (size: 1795.0 B, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,371 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,371 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,371 INFO cluster.YarnScheduler: Adding task set 20.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,372 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 238, algo-2, executor 1, partition 0, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,372 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 239, algo-2, executor 1, partition 1, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,378 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:39307 (size: 1795.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,382 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,394 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 239) in 22 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,394 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 238) in 23 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,394 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,394 INFO scheduler.DAGScheduler: ResultStage 20 (countByKey at ColumnProfiler.scala:547) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,395 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at ColumnProfiler.scala:547, took 3.311523 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,485 INFO codegen.CodeGenerator: Code generated in 11.992488 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,507 INFO codegen.CodeGenerator: Code generated in 6.466738 ms\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,513 INFO codegen.CodeGenerator: Code generated in 4.479667 ms\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|_1              |_2                                                                          |_3                                                                                  |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|review_id       |'review_id' is not null                                                     |.isComplete(\"review_id\")                                                            |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' is not null                                                   |.isComplete(\"customer_id\")                                                          |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has type Integral                                             |.hasDataType(\"customer_id\", ConstrainableDataTypes.Integral)                        |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has no negative values                                        |.isNonNegative(\"customer_id\")                                                       |\u001b[0m\n",
      "\u001b[34m|review_date     |'review_date' is not null                                                   |.isComplete(\"review_date\")                                                          |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' is not null                                                 |.isComplete(\"helpful_votes\")                                                        |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' has no negative values                                      |.isNonNegative(\"helpful_votes\")                                                     |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' is not null                                                   |.isComplete(\"star_rating\")                                                          |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' has no negative values                                        |.isNonNegative(\"star_rating\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_title   |'product_title' is not null                                                 |.isComplete(\"product_title\")                                                        |\u001b[0m\n",
      "\u001b[34m|review_headline |'review_headline' is not null                                               |.isComplete(\"review_headline\")                                                      |\u001b[0m\n",
      "\u001b[34m|product_id      |'product_id' is not null                                                    |.isComplete(\"product_id\")                                                           |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' is not null                                                   |.isComplete(\"total_votes\")                                                          |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' has no negative values                                        |.isNonNegative(\"total_votes\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' is not null                                              |.isComplete(\"product_category\")                                                     |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' has value range 'Digital_Video_Games', 'Digital_Software'|.isContainedIn(\"product_category\", Array(\"Digital_Video_Games\", \"Digital_Software\"))|\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' is not null                                                |.isComplete(\"product_parent\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has type Integral                                          |.hasDataType(\"product_parent\", ConstrainableDataTypes.Integral)                     |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has no negative values                                     |.isNonNegative(\"product_parent\")                                                    |\u001b[0m\n",
      "\u001b[34m|review_body     |'review_body' has less than 1% missing values                               |.hasCompleteness(\"review_body\", _ >= 0.99, Some(\"It should be above 0.99!\"))        |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,580 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,580 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,580 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,580 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200822182330_0000}; taskId=attempt_20200822182330_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b9cf0a8}; outputPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions, workPath=s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions/_temporary/0/_temporary/attempt_20200822182330_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,580 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,888 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:151\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,889 INFO scheduler.DAGScheduler: Registering RDD 70 (csv at preprocess-deequ.scala:151) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,890 INFO scheduler.DAGScheduler: Got job 10 (csv at preprocess-deequ.scala:151) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,890 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (csv at preprocess-deequ.scala:151)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,890 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,890 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,890 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,891 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.0 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,892 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.0 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,892 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.153.140:41813 (size: 3.0 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,892 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,892 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,892 INFO cluster.YarnScheduler: Adding task set 21.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,893 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 240, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8680 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,893 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 241, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8672 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,894 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 242, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8656 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,894 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 21.0 (TID 243, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8872 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,894 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 21.0 (TID 244, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8841 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,901 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-2:39307 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,904 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 21.0 (TID 244) in 10 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,904 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 240) in 11 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,904 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 21.0 (TID 243) in 10 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 241) in 11 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 242) in 12 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (csv at preprocess-deequ.scala:151) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 22)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,905 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,926 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 245.1 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,927 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 90.5 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,928 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.153.140:41813 (size: 90.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,928 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,928 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,928 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,929 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 245, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,934 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-2:39307 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:30,943 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.152.82:49926\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:31,869 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 245) in 940 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:31,869 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:31,869 INFO scheduler.DAGScheduler: ResultStage 22 (csv at preprocess-deequ.scala:151) finished in 0.964 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:31,870 INFO scheduler.DAGScheduler: Job 10 finished: csv at preprocess-deequ.scala:151, took 0.981516 s\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,261 INFO datasources.FileFormatWriter: Write Job a4e17330-c66b-4b33-868b-5dbb7c8fe13b committed.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,262 INFO datasources.FileFormatWriter: Finished processing stats for write job a4e17330-c66b-4b33-868b-5dbb7c8fe13b.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,293 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,297 INFO server.AbstractConnector: Stopped Spark@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,298 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.153.140:4040\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,302 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,315 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,315 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,317 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,318 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,323 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,331 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,331 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,332 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,342 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,351 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,352 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,352 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6fcb231b-9bb4-4081-bc85-c2d91b1c6474\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,355 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-76abf22b-e765-48b0-8715-9afff5d2f356\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,357 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-76abf22b-e765-48b0-8715-9afff5d2f356/pyspark-b67e5805-58a6-40af-a76b-deb59d0f90e3\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,360 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,360 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-08-22 18:23:32,360 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[35m2020-08-22 18:23:33\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 18:23:18          0 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks/_SUCCESS\r\n",
      "2020-08-22 18:23:18        768 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks/part-00000-3b85fc28-6313-41af-8623-0c1e24fcda39-c000.csv\r\n",
      "2020-08-22 18:23:33          0 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions/_SUCCESS\r\n",
      "2020-08-22 18:23:32       2289 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions/part-00000-3a522aba-1bfb-447a-be63-9655ab82740e-c000.csv\r\n",
      "2020-08-22 18:23:11          0 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics/_SUCCESS\r\n",
      "2020-08-22 18:23:10        364 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics/part-00000-bde640ae-f589-4539-a047-314a54da7b57-c000.csv\r\n",
      "2020-08-22 18:23:20          0 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics/_SUCCESS\r\n",
      "2020-08-22 18:23:20        277 amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics/part-00000-db46d024-c6cf-4ddd-9e00-b023824b79c7-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/dataset-metrics/part-00000-bde640ae-f589-4539-a047-314a54da7b57-c000.csv to amazon-reviews-spark-analyzer/dataset-metrics/part-00000-bde640ae-f589-4539-a047-314a54da7b57-c000.csv\n",
      "download: s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-suggestions/part-00000-3a522aba-1bfb-447a-be63-9655ab82740e-c000.csv to amazon-reviews-spark-analyzer/constraint-suggestions/part-00000-3a522aba-1bfb-447a-be63-9655ab82740e-c000.csv\n",
      "download: s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/success-metrics/part-00000-db46d024-c6cf-4ddd-9e00-b023824b79c7-c000.csv to amazon-reviews-spark-analyzer/success-metrics/part-00000-db46d024-c6cf-4ddd-9e00-b023824b79c7-c000.csv\n",
      "download: s3://sagemaker-us-west-2-250107111215/amazon-reviews-spark-analyzer-2020-08-22-18-18-50/output/constraint-checks/part-00000-3b85fc28-6313-41af-8623-0c1e24fcda39-c000.csv to amazon-reviews-spark-analyzer/constraint-checks/part-00000-3b85fc28-6313-41af-8623-0c1e24fcda39-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MinimumConstraint(Minimum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MaximumConstraint(Maximum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id)))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>ComplianceConstraint(Compliance(marketplace co...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check                                         constraint  \\\n",
       "0  Review Check                         SizeConstraint(Size(None))   \n",
       "1  Review Check       MinimumConstraint(Minimum(star_rating,None))   \n",
       "2  Review Check       MaximumConstraint(Maximum(star_rating,None))   \n",
       "3  Review Check  CompletenessConstraint(Completeness(review_id,...   \n",
       "4  Review Check  UniquenessConstraint(Uniqueness(List(review_id)))   \n",
       "5  Review Check  CompletenessConstraint(Completeness(marketplac...   \n",
       "6  Review Check  ComplianceConstraint(Compliance(marketplace co...   \n",
       "\n",
       "  constraint_status  constraint_message  \n",
       "0           Success                 NaN  \n",
       "1           Success                 NaN  \n",
       "2           Success                 NaN  \n",
       "3           Success                 NaN  \n",
       "4           Success                 NaN  \n",
       "5           Success                 NaN  \n",
       "6           Success                 NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks[['check', 'constraint', 'constraint_status', 'constraint_message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>238027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,star_rating</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>-0.080881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Mean</td>\n",
       "      <td>3.723706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>top star_rating</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.663338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,helpful_votes</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>0.980529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                   instance                 name          value\n",
       "0       Column                  review_id         Completeness       1.000000\n",
       "1       Column                  review_id  ApproxCountDistinct  238027.000000\n",
       "2  Mutlicolumn    total_votes,star_rating          Correlation      -0.080881\n",
       "3      Dataset                          *                 Size  247515.000000\n",
       "4       Column                star_rating                 Mean       3.723706\n",
       "5       Column            top star_rating           Compliance       0.663338\n",
       "6  Mutlicolumn  total_votes,helpful_votes          Correlation       0.980529"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Uniqueness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace contained in US,UK,DE,JP,FR</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                                 instance          name     value\n",
       "0   Column                                review_id  Completeness       1.0\n",
       "1   Column                                review_id    Uniqueness       1.0\n",
       "2  Dataset                                        *          Size  247515.0\n",
       "3   Column                              star_rating       Maximum       5.0\n",
       "4   Column                              star_rating       Minimum       1.0\n",
       "5   Column  marketplace contained in US,UK,DE,JP,FR    Compliance       1.0\n",
       "6   Column                              marketplace  Completeness       1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>description</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review_id</td>\n",
       "      <td>'review_id' is not null</td>\n",
       "      <td>.isComplete(\\review_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' is not null</td>\n",
       "      <td>.isComplete(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has type Integral</td>\n",
       "      <td>.hasDataType(\\customer_id\\\", ConstrainableData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has no negative values</td>\n",
       "      <td>.isNonNegative(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review_date</td>\n",
       "      <td>'review_date' is not null</td>\n",
       "      <td>.isComplete(\\review_date\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' is not null</td>\n",
       "      <td>.isComplete(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' is not null</td>\n",
       "      <td>.isComplete(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' has no negative values</td>\n",
       "      <td>.isNonNegative(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product_title</td>\n",
       "      <td>'product_title' is not null</td>\n",
       "      <td>.isComplete(\\product_title\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_headline</td>\n",
       "      <td>'review_headline' is not null</td>\n",
       "      <td>.isComplete(\\review_headline\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_id</td>\n",
       "      <td>'product_id' is not null</td>\n",
       "      <td>.isComplete(\\product_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' is not null</td>\n",
       "      <td>.isComplete(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' is not null</td>\n",
       "      <td>.isComplete(\\product_category\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' has value range 'Digital_Vi...</td>\n",
       "      <td>.isContainedIn(\\product_category\\\", Array(\\\"Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' is not null</td>\n",
       "      <td>.isComplete(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has type Integral</td>\n",
       "      <td>.hasDataType(\\product_parent\\\", ConstrainableD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has no negative values</td>\n",
       "      <td>.isNonNegative(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_body</td>\n",
       "      <td>'review_body' has less than 1% missing values</td>\n",
       "      <td>.hasCompleteness(\\review_body\\\", _ &gt;= 0.99, So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' is not null</td>\n",
       "      <td>.isComplete(\\vine\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' has value range 'N'</td>\n",
       "      <td>.isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' is not null</td>\n",
       "      <td>.isComplete(\\marketplace\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' has value range 'US'</td>\n",
       "      <td>.isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' is not null</td>\n",
       "      <td>.isComplete(\\verified_purchase\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' has value range 'Y', 'N'</td>\n",
       "      <td>.isContainedIn(\\verified_purchase\\\", Array(\\\"Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          column_name                                        description  \\\n",
       "0           review_id                            'review_id' is not null   \n",
       "1         customer_id                          'customer_id' is not null   \n",
       "2         customer_id                    'customer_id' has type Integral   \n",
       "3         customer_id               'customer_id' has no negative values   \n",
       "4         review_date                          'review_date' is not null   \n",
       "5       helpful_votes                        'helpful_votes' is not null   \n",
       "6       helpful_votes             'helpful_votes' has no negative values   \n",
       "7         star_rating                          'star_rating' is not null   \n",
       "8         star_rating               'star_rating' has no negative values   \n",
       "9       product_title                        'product_title' is not null   \n",
       "10    review_headline                      'review_headline' is not null   \n",
       "11         product_id                           'product_id' is not null   \n",
       "12        total_votes                          'total_votes' is not null   \n",
       "13        total_votes               'total_votes' has no negative values   \n",
       "14   product_category                     'product_category' is not null   \n",
       "15   product_category  'product_category' has value range 'Digital_Vi...   \n",
       "16     product_parent                       'product_parent' is not null   \n",
       "17     product_parent                 'product_parent' has type Integral   \n",
       "18     product_parent            'product_parent' has no negative values   \n",
       "19        review_body      'review_body' has less than 1% missing values   \n",
       "20               vine                                 'vine' is not null   \n",
       "21               vine                         'vine' has value range 'N'   \n",
       "22        marketplace                          'marketplace' is not null   \n",
       "23        marketplace                 'marketplace' has value range 'US'   \n",
       "24  verified_purchase                    'verified_purchase' is not null   \n",
       "25  verified_purchase       'verified_purchase' has value range 'Y', 'N'   \n",
       "\n",
       "                                                 code  \n",
       "0                          .isComplete(\\review_id\\\")\"  \n",
       "1                        .isComplete(\\customer_id\\\")\"  \n",
       "2   .hasDataType(\\customer_id\\\", ConstrainableData...  \n",
       "3                     .isNonNegative(\\customer_id\\\")\"  \n",
       "4                        .isComplete(\\review_date\\\")\"  \n",
       "5                      .isComplete(\\helpful_votes\\\")\"  \n",
       "6                   .isNonNegative(\\helpful_votes\\\")\"  \n",
       "7                        .isComplete(\\star_rating\\\")\"  \n",
       "8                     .isNonNegative(\\star_rating\\\")\"  \n",
       "9                      .isComplete(\\product_title\\\")\"  \n",
       "10                   .isComplete(\\review_headline\\\")\"  \n",
       "11                        .isComplete(\\product_id\\\")\"  \n",
       "12                       .isComplete(\\total_votes\\\")\"  \n",
       "13                    .isNonNegative(\\total_votes\\\")\"  \n",
       "14                  .isComplete(\\product_category\\\")\"  \n",
       "15  .isContainedIn(\\product_category\\\", Array(\\\"Di...  \n",
       "16                    .isComplete(\\product_parent\\\")\"  \n",
       "17  .hasDataType(\\product_parent\\\", ConstrainableD...  \n",
       "18                 .isNonNegative(\\product_parent\\\")\"  \n",
       "19  .hasCompleteness(\\review_body\\\", _ >= 0.99, So...  \n",
       "20                              .isComplete(\\vine\\\")\"  \n",
       "21             .isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"  \n",
       "22                       .isComplete(\\marketplace\\\")\"  \n",
       "23     .isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"  \n",
       "24                 .isComplete(\\verified_purchase\\\")\"  \n",
       "25  .isContainedIn(\\verified_purchase\\\", Array(\\\"Y...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions.columns=['column_name', 'description', 'code']\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_dataset_metrics' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df_dataset_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
